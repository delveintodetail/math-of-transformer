{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ***Part 1***"
      ],
      "metadata": {
        "id": "NrPdy0Q1-Srh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Demonstrate code of SVD\n"
      ],
      "metadata": {
        "id": "prcaPWQJ04Dz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1fFdd7miJF-",
        "outputId": "c6ca9f79-ff40-4951-fb2b-858d2af4c7b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2]) torch.Size([2]) torch.Size([3, 2])\n",
            "Original matrix:\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "\n",
            "Reconstructed matrix:\n",
            "tensor([[1.0000, 2.0000, 3.0000],\n",
            "        [4.0000, 5.0000, 6.0000]])\n",
            "\n",
            "Reconstruction error:\n",
            "tensor(1.8961e-06)\n"
          ]
        }
      ],
      "source": [
        "#Demonstrate code of SVD\n",
        "import torch\n",
        "\n",
        "# Create a simple matrix\n",
        "X = torch.tensor([[1., 2., 3.],\n",
        "                 [4., 5., 6.]])  # Try different values\n",
        "\n",
        "# X = torch.tensor([[1., 2.],\n",
        "#                   [4., 5.],\n",
        "#                   [5, 6]])\n",
        "\n",
        "# Perform SVD\n",
        "U, S, V = torch.svd(X)\n",
        "print(U.shape, S.shape, V.shape)\n",
        "\n",
        "# Reconstruct the matrix\n",
        "X_reconstructed = U @ torch.diag(S) @ V.T\n",
        "\n",
        "print(\"Original matrix:\")\n",
        "print(X)\n",
        "print(\"\\nReconstructed matrix:\")\n",
        "print(X_reconstructed)\n",
        "print(\"\\nReconstruction error:\")\n",
        "print(torch.norm(X - X_reconstructed))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Demonstrate code for power iteration"
      ],
      "metadata": {
        "id": "6J1q217b0_kr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uir_JDWQiNbb",
        "outputId": "c5145e1f-3ef2-41d3-e62e-fcba163f1e7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "λ=4.0000, v=tensor([ 1.0000, -0.0036])\n",
            "λ=3.0000, v=tensor([0.0036, 1.0000])\n",
            "tensor([4., 3.])\n"
          ]
        }
      ],
      "source": [
        "#Demonstrate code for power interation\n",
        "import torch\n",
        "\n",
        "# Test matrix\n",
        "X = torch.tensor([[4., 1.], [1., 3.]])\n",
        "X = torch.tensor([[4., 0.], [0., 3.]])\n",
        "\n",
        "# Power iteration\n",
        "v = torch.randn(2)\n",
        "v = v/torch.norm(v)\n",
        "for _ in range(20):\n",
        "    v = X @ v\n",
        "    v = v / torch.norm(v)\n",
        "\n",
        "eigenvalue = (v @ X @ v) / (v @ v)\n",
        "\n",
        "v1 = v\n",
        "\n",
        "print(f\"λ={eigenvalue:.4f}, v={v}\")\n",
        "\n",
        "v = torch.randn(2)\n",
        "v = v/torch.norm(v)\n",
        "for _ in range(20):\n",
        "    v = X @ v\n",
        "    v = v - v@v1*v1   # Try here, how to obtain the second-largest eigenvalue\n",
        "    v = v / torch.norm(v)\n",
        "eigenvalue2 = (v @ X @ v) / (v @ v)\n",
        "\n",
        "\n",
        "print(f\"λ={eigenvalue2:.4f}, v={v}\")\n",
        "\n",
        "U,S,V = torch.svd(X)\n",
        "print(S)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Two random high-dimensional vectors are orthogonal"
      ],
      "metadata": {
        "id": "Ji4h9BNm1HnF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xoj23DLXjtaY",
        "outputId": "015d570b-c0c1-4590-86a6-9f64546eece7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistics for 10000 trials with 4096-dimensional vectors:\n",
            "Mean norm: 4.999129\n",
            "Variance of norm: 0.001407\n",
            "Standard deviation of norm: 0.037515\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "import torch\n",
        "# Set parameters\n",
        "dim = 4096        # dimension of vectors\n",
        "n_trials = 10000   # number of trials\n",
        "# Store norms of results\n",
        "result_norms = torch.zeros(n_trials)\n",
        "# Perform trials\n",
        "for i in range(n_trials):\n",
        "    # Create two random vectors\n",
        "    v1 = torch.randn(dim)\n",
        "    v2 = torch.randn(dim)\n",
        "\n",
        "    # L2 normalize both vectors\n",
        "    v1_normalized = 3.*v1 / torch.norm(v1)  # Try 2.*v1 / torch.norm(v1)\n",
        "    v2_normalized = 4.*v2 / torch.norm(v2)  # Try 3.*v1 / torch.norm(v1)\n",
        "\n",
        "    # Add normalized vectors and compute norm\n",
        "    result = v1_normalized + v2_normalized\n",
        "    result_norms[i] = torch.norm(result)\n",
        "# Calculate statistics\n",
        "mean_norm = torch.mean(result_norms)\n",
        "var_norm = torch.var(result_norms)\n",
        "\n",
        "print(f\"Statistics for {n_trials} trials with {dim}-dimensional vectors:\")\n",
        "print(f\"Mean norm: {mean_norm:.6f}\")\n",
        "print(f\"Variance of norm: {var_norm:.6f}\")\n",
        "print(f\"Standard deviation of norm: {torch.sqrt(var_norm):.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Demonstrate code to Expected Norm with L2 Normalized Rows (Overdetermined)"
      ],
      "metadata": {
        "id": "3GSBOQGE1eE-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DT2ku-q_mPKO",
        "outputId": "f4c8d64c-7d1b-4793-893d-7962f0f05b12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected norm: 1.4143 ± 0.0221\n",
            "Expected norm:  tensor(1.4142)\n"
          ]
        }
      ],
      "source": [
        "#Demonstrate code to Expected Norm with L2 Normalized Rows (Overdetermined)\n",
        "import torch\n",
        "\n",
        "# Parameters\n",
        "m, n = 2048, 1024                   # Try different m, n\n",
        "trials = 1000\n",
        "norms = torch.zeros(trials)\n",
        "A = torch.randn(m, n)\n",
        "A = A / torch.norm(A, dim=1, keepdim=True)\n",
        "for i in range(trials):\n",
        "    # Create and normalize A and x\n",
        "    x = torch.randn(n)\n",
        "    x = x / torch.norm(x)\n",
        "\n",
        "    # Compute norm of y = Ax\n",
        "    norms[i] = torch.norm(A @ x)\n",
        "\n",
        "print(f\"Expected norm: {torch.mean(norms):.4f} ± {torch.std(norms):.4f}\")\n",
        "U,S,V = torch.svd(A)\n",
        "print('Expected norm: ', torch.sqrt(torch.pow(S, 2).mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Demonstrate code to Expected Norm with L2 Normalized Rows (Underdetermined)\n"
      ],
      "metadata": {
        "id": "9dDKfEUC1jEH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqtE7gqqsH5Z",
        "outputId": "2068a944-0eca-4938-a3f6-4dbd211809bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected norm: 0.4992 ± 0.0219\n",
            "Expected norm:  tensor(1.)\n"
          ]
        }
      ],
      "source": [
        "#Demonstrate code to Expected Norm with L2 Normalized Rows (Underdetermined)\n",
        "import torch\n",
        "\n",
        "# Parameters\n",
        "m, n = 256, 1024      # Try different m, n\n",
        "trials = 1000\n",
        "norms = torch.zeros(trials)\n",
        "A = torch.randn(m, n)\n",
        "A = A / torch.norm(A, dim=1, keepdim=True)\n",
        "for i in range(trials):\n",
        "    # Create and normalize A and x\n",
        "    x = torch.randn(n)\n",
        "    x = x / torch.norm(x)\n",
        "\n",
        "    # Compute norm of y = Ax\n",
        "    norms[i] = torch.norm(A @ x)\n",
        "\n",
        "print(f\"Expected norm: {torch.mean(norms):.4f} ± {torch.std(norms):.4f}\")\n",
        "U,S,V = torch.svd(A)\n",
        "print('Expected norm: ', torch.sqrt(torch.pow(S, 2).mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Demonstrate code to Expected Norm with L2 Normalized Rows (Rectangle)"
      ],
      "metadata": {
        "id": "4lgOYyiN1mz4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9Bhn5pxt9ZI",
        "outputId": "26a20336-4cde-423d-accf-5f51b475aa26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected norm: 2.0003 ± 0.0327\n",
            "Expected norm:  tensor(1.0000)\n"
          ]
        }
      ],
      "source": [
        "#Demonstrate code to Expected Norm with L2 Normalized Rows (Rectangle)\n",
        "import torch\n",
        "\n",
        "# Parameters\n",
        "m, n = 2048, 2048    # Try different m, n\n",
        "trials = 1000\n",
        "norms = torch.zeros(trials)\n",
        "A = torch.randn(m, n)\n",
        "A = A / torch.norm(A, dim=1, keepdim=True)\n",
        "for i in range(trials):\n",
        "    # Create and normalize A and x\n",
        "    x = torch.randn(n)\n",
        "    scale = 2.0\n",
        "    x = scale*x / torch.norm(x)\n",
        "\n",
        "    # Compute norm of y = Ax\n",
        "    norms[i] = torch.norm(A @ x)\n",
        "\n",
        "print(f\"Expected norm: {torch.mean(norms):.4f} ± {torch.std(norms):.4f}\")\n",
        "U,S,V = torch.svd(A)\n",
        "print('Expected norm: ', torch.sqrt(torch.pow(S, 2).mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Expected norm of orthogonal projection"
      ],
      "metadata": {
        "id": "arpiaHv41oZ1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GV4yKUSSu293",
        "outputId": "083211dd-8075-463f-9816-80fac873e051"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2048, 1024]) torch.Size([1024]) torch.Size([1024, 1024])\n",
            "Expected norm: 3.0000 ± 0.0000\n",
            "Expected norm:  tensor(1.)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Parameters\n",
        "m = 2048                                # Try different m and n\n",
        "n = 1024  # dimension of square matrix\n",
        "trials = 1000\n",
        "norms = torch.zeros(trials)\n",
        "# Create matrix A with L2 normalized columns\n",
        "A = torch.randn(m, n)\n",
        "A = A / torch.norm(A, dim=0, keepdim=True)  # normalize columns\n",
        "U, S, V = torch.svd(A)\n",
        "print(U.shape, S.shape, V.shape)   # see here, discuss the shape of U, S, V\n",
        "A = torch.mm(U, V.T)\n",
        "\n",
        "for i in range(trials):\n",
        "    # Random input vector\n",
        "    x = torch.randn(n)\n",
        "    x = 3.*x / torch.norm(x)\n",
        "\n",
        "    # Compute norm of y = Ax\n",
        "    norms[i] = torch.norm(A @ x)\n",
        "\n",
        "print(f\"Expected norm: {torch.mean(norms):.4f} ± {torch.std(norms):.4f}\")\n",
        "U,S,V = torch.svd(A)\n",
        "print('Expected norm: ', torch.sqrt(torch.pow(S, 2).mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. verify the norm of orthogonal projection\n",
        "\n"
      ],
      "metadata": {
        "id": "IegBvMU79VNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Parameters\n",
        "n = 100  # dimension\n",
        "trials = 1000\n",
        "norms = torch.zeros(trials)\n",
        "# Create orthogonal matrix using QR decomposition\n",
        "A = torch.randn(n, n)\n",
        "Q, _ = torch.linalg.qr(A)  # Q is orthogonal\n",
        "\n",
        "for i in range(trials):\n",
        "    # Random input vector\n",
        "    x = torch.randn(n)\n",
        "    x = 2.1*x / torch.norm(x)\n",
        "\n",
        "    # Compute norm of y = Ax\n",
        "    norms[i] = torch.norm(Q @ x)\n",
        "\n",
        "print(f\"Expected norm: {torch.mean(norms):.4f} ± {torch.std(norms):.4f}\")\n",
        "print(f\"Input vector norm (mean): {torch.mean(torch.tensor(norms[i])):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v8oUtHA9TgN",
        "outputId": "286cf053-c11a-462e-826b-5166fd640f9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected norm: 2.1000 ± 0.0000\n",
            "Input vector norm (mean): 2.1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-29-212264577.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  print(f\"Input vector norm (mean): {torch.mean(torch.tensor(norms[i])):.4f}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. verify automatic gradient and compute gradient according to equation\n"
      ],
      "metadata": {
        "id": "yYMhDk0mnV8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Define dimensions\n",
        "m = 3  # rows of A\n",
        "n = 4  # cols of A, rows of X\n",
        "b = 2  # cols of X\n",
        "\n",
        "# Initialize matrices\n",
        "A = torch.randn(m, n)\n",
        "X = torch.randn(n, b, requires_grad=True)\n",
        "\n",
        "print(\"Matrix A shape:\", A.shape)\n",
        "print(\"Matrix X shape:\", X.shape)\n",
        "\n",
        "# Forward pass\n",
        "Y = torch.mm(A, X)\n",
        "loss = torch.mean(Y)\n",
        "\n",
        "print(\"\\nComputed Y = AX shape:\", Y.shape)\n",
        "print(\"Loss value:\", loss.item())\n",
        "\n",
        "# Compute gradient using autograd\n",
        "loss.backward()\n",
        "autograd_gradient = X.grad.clone()\n",
        "\n",
        "# Manual gradient computation\n",
        "# ∂loss/∂X = (1/mb)A^T @ ones(m,b)\n",
        "ones_matrix = torch.ones(m, b)\n",
        "manual_gradient = (1/(m*b)) * (A.t() @ ones_matrix)\n",
        "\n",
        "print(\"\\nAutograd computed gradient:\")\n",
        "print(autograd_gradient)\n",
        "print(\"\\nManually computed gradient:\")\n",
        "print(manual_gradient)\n",
        "\n",
        "# Verify the gradients match\n",
        "is_close = torch.allclose(autograd_gradient, manual_gradient, rtol=1e-4)\n",
        "print(\"\\nGradients match:\", is_close)\n",
        "\n",
        "if not is_close:\n",
        "    rel_error = torch.norm(autograd_gradient - manual_gradient) / torch.norm(autograd_gradient)\n",
        "    print(f\"Relative error: {rel_error:.2e}\")\n",
        "\n",
        "# Verify with simpler example for clearer understanding\n",
        "print(\"\\n=== Simple Example for Verification ===\")\n",
        "A_simple = torch.tensor([[1., 2.],\n",
        "                        [3., 4.]], dtype=torch.float)\n",
        "X_simple = torch.tensor([[1., 0.],\n",
        "                        [0., 1.]], dtype=torch.float, requires_grad=True)\n",
        "\n",
        "print(\"Simple A:\")\n",
        "print(A_simple)\n",
        "print(\"\\nSimple X:\")\n",
        "print(X_simple)\n",
        "\n",
        "# Forward pass with simple matrices\n",
        "Y_simple = torch.mm(A_simple, X_simple)\n",
        "loss_simple = torch.mean(Y_simple)\n",
        "\n",
        "print(\"\\nY = AX:\")\n",
        "print(Y_simple)\n",
        "print(\"Loss = mean(Y):\", loss_simple.item())\n",
        "\n",
        "# Compute gradients for simple case\n",
        "loss_simple.backward()\n",
        "simple_autograd = X_simple.grad.clone()\n",
        "\n",
        "m_simple, b_simple = Y_simple.shape\n",
        "ones_simple = torch.ones(m_simple, b_simple)\n",
        "simple_manual = (1/(m_simple*b_simple)) * (A_simple.t() @ ones_simple)\n",
        "\n",
        "print(\"\\nSimple autograd gradient:\")\n",
        "print(simple_autograd)\n",
        "print(\"\\nSimple manual gradient:\")\n",
        "print(simple_manual)\n",
        "\n",
        "# Verify simple case\n",
        "is_close_simple = torch.allclose(simple_autograd, simple_manual, rtol=1e-4)\n",
        "print(\"\\nSimple gradients match:\", is_close_simple)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gff8w_ADOv76",
        "outputId": "15d1f012-5b00-4820-e52f-e6c88587c982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix A shape: torch.Size([3, 4])\n",
            "Matrix X shape: torch.Size([4, 2])\n",
            "\n",
            "Computed Y = AX shape: torch.Size([3, 2])\n",
            "Loss value: 0.9903979301452637\n",
            "\n",
            "Autograd computed gradient:\n",
            "tensor([[-0.0541, -0.0541],\n",
            "        [ 0.0350,  0.0350],\n",
            "        [ 0.4963,  0.4963],\n",
            "        [ 0.0669,  0.0669]])\n",
            "\n",
            "Manually computed gradient:\n",
            "tensor([[-0.0541, -0.0541],\n",
            "        [ 0.0350,  0.0350],\n",
            "        [ 0.4963,  0.4963],\n",
            "        [ 0.0669,  0.0669]])\n",
            "\n",
            "Gradients match: True\n",
            "\n",
            "=== Simple Example for Verification ===\n",
            "Simple A:\n",
            "tensor([[1., 2.],\n",
            "        [3., 4.]])\n",
            "\n",
            "Simple X:\n",
            "tensor([[1., 0.],\n",
            "        [0., 1.]], requires_grad=True)\n",
            "\n",
            "Y = AX:\n",
            "tensor([[1., 2.],\n",
            "        [3., 4.]], grad_fn=<MmBackward0>)\n",
            "Loss = mean(Y): 2.5\n",
            "\n",
            "Simple autograd gradient:\n",
            "tensor([[1.0000, 1.0000],\n",
            "        [1.5000, 1.5000]])\n",
            "\n",
            "Simple manual gradient:\n",
            "tensor([[1.0000, 1.0000],\n",
            "        [1.5000, 1.5000]])\n",
            "\n",
            "Simple gradients match: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Part 2***"
      ],
      "metadata": {
        "id": "oHfSGqDC9-4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. gradient of self-attention module"
      ],
      "metadata": {
        "id": "vhqGa3VA2BC3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGq6oWs_43Rn",
        "outputId": "8427b27f-6371-456e-d5e4-44cf64df6999"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention F norm tensor(1595.3085)\n",
            "Input grad_output norm: 27.7034\n",
            "Output Y=VA norm: 98.5762\n",
            "Gradient dL/dX norm: 231.0547\n",
            "\n",
            "Shapes:\n",
            "Input X: torch.Size([2, 1000, 768])\n",
            "Output Y: torch.Size([2, 1000, 768])\n",
            "Gradient dL/dX: torch.Size([2, 1000, 768])\n"
          ]
        }
      ],
      "source": [
        "# gradient of self-attention module\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "def self_attention_with_grad(X, d_model=512):\n",
        "    \"\"\"\n",
        "    Compute self-attention and its gradient with respect to input X\n",
        "    Args:\n",
        "        X: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "        d_model: Dimension of the model\n",
        "    Returns:\n",
        "        Y: Output tensor\n",
        "        dY_dX: Gradient of Y with respect to X\n",
        "    \"\"\"\n",
        "    # Ensure X requires gradient and create a fresh copy\n",
        "    X = X.clone().detach().requires_grad_(True)    #  Discuss here\n",
        "\n",
        "    batch_size, seq_len, _ = X.shape\n",
        "    ss = 2.0\n",
        "    ss1 = 2.0\n",
        "    ss2 = 2.0                                     # Try different ss and ss1\n",
        "\n",
        "\n",
        "    W_q = ss*torch.randn(d_model, d_model, device=X.device)*math.sqrt(2./(d_model + d_model))\n",
        "    W_k = ss1*torch.randn(d_model, d_model, device=X.device)*math.sqrt(2./(d_model + d_model))\n",
        "    W_v = ss2*torch.randn(d_model, d_model, device=X.device)*math.sqrt(2./(d_model + d_model))\n",
        "\n",
        "    # Compute Q, K, V\n",
        "    Q = torch.matmul(X, W_q)  # (batch_size, seq_len, d_model)\n",
        "    K = torch.matmul(X, W_k)  # (batch_size, seq_len, d_model)\n",
        "    V = torch.matmul(X, W_v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    # Scaled dot-product attention\n",
        "    scale = torch.sqrt(torch.tensor(d_model, dtype=torch.float32, device=X.device))\n",
        "    attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / scale  # (batch_size, seq_len, d_model)\n",
        "    attention_probs = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "\n",
        "    U, S, V1 = torch.svd(attention_probs.detach())\n",
        "    #print(S)\n",
        "    print(\"attention F norm\", S.pow(2).sum())\n",
        "\n",
        "    # Compute output\n",
        "    Y = torch.matmul(attention_probs, V)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    # Create a random gradient tensor to backpropagate\n",
        "    grad_output = torch.randn_like(Y)\n",
        "    print(f\"Input grad_output norm: {grad_output.norm(dim=-1).mean().item():.4f}\")         # Discuss here, B*L*D\n",
        "\n",
        "    # Compute gradient\n",
        "    Y.backward(grad_output)\n",
        "\n",
        "    # Get the gradient and make sure it exists\n",
        "    assert X.grad is not None, \"Gradient computation failed\"\n",
        "    grad = X.grad.clone()\n",
        "\n",
        "    return Y.detach(), grad\n",
        "\n",
        "def main():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Create sample input\n",
        "    batch_size = 2\n",
        "    seq_len = 1000\n",
        "    d_model = 768\n",
        "    X = torch.randn(batch_size, seq_len, d_model)*2.0\n",
        "\n",
        "    # Compute self-attention and its gradient\n",
        "    Y, X_gradient = self_attention_with_grad(X, d_model=d_model)\n",
        "\n",
        "    # Print norms for verification\n",
        "    print(f\"Output Y=VA norm: {Y.norm(dim=-1).mean().item():.4f}\")\n",
        "    print(f\"Gradient dL/dX norm: {X_gradient.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    # Print shapes for verification\n",
        "    print(f\"\\nShapes:\")\n",
        "    print(f\"Input X: {X.shape}\")\n",
        "    print(f\"Output Y: {Y.shape}\")\n",
        "    print(f\"Gradient dL/dX: {X_gradient.shape}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "# attention F norm tensor(5.4196)\n",
        "# Input grad_output norm: 27.7034\n",
        "# Output Y=VA norm: 3.4780\n",
        "# Gradient dL/dX norm: 5.3274\n",
        "\n",
        "# attention F norm tensor(58.2874)\n",
        "# Input grad_output norm: 27.7034\n",
        "# Output Y=VA norm: 9.5448\n",
        "# Gradient dL/dX norm: 23.5255\n",
        "\n",
        "# attention F norm tensor(550.2133)\n",
        "# Input grad_output norm: 27.7034\n",
        "# Output Y=VA norm: 27.7531\n",
        "# Gradient dL/dX norm: 88.2601\n",
        "\n",
        "\n",
        "# attention F norm tensor(1595.3085)\n",
        "# Input grad_output norm: 27.7034\n",
        "# Output Y=VA norm: 98.5762\n",
        "# Gradient dL/dX norm: 231.0547"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. How much is contributed by the first term?"
      ],
      "metadata": {
        "id": "XFYEbaQz-rMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient of self-attention module\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def self_attention_with_grad(X, d_model=512):\n",
        "    \"\"\"\n",
        "    Compute self-attention and its gradient with respect to input X\n",
        "    Args:\n",
        "        X: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "        d_model: Dimension of the model\n",
        "    Returns:\n",
        "        Y: Output tensor\n",
        "        dY_dX: Gradient of Y with respect to X\n",
        "    \"\"\"\n",
        "    # Ensure X requires gradient and create a fresh copy\n",
        "    X = X.clone().detach().requires_grad_(True)\n",
        "\n",
        "    batch_size, seq_len, _ = X.shape\n",
        "    ss = 1.0\n",
        "    ss1 = 1.0\n",
        "    ss2 = 1.0                                     # Try different ss and ss1\n",
        "\n",
        "    W_q = ss*torch.randn(d_model, d_model, device=X.device)*math.sqrt(2./(d_model + d_model))\n",
        "    W_k = ss1*torch.randn(d_model, d_model, device=X.device)*math.sqrt(2./(d_model + d_model))\n",
        "    W_v = ss2*torch.randn(d_model, d_model, device=X.device)*math.sqrt(2./(d_model + d_model))\n",
        "\n",
        "    # Compute Q, K, V\n",
        "    Q = torch.matmul(X, W_q)  # (batch_size, seq_len, d_model)\n",
        "    K = torch.matmul(X, W_k)  # (batch_size, seq_len, d_model)\n",
        "    V = torch.matmul(X, W_v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    # Scaled dot-product attention\n",
        "    scale = torch.sqrt(torch.tensor(d_model, dtype=torch.float32, device=X.device))\n",
        "    attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / scale  # (batch_size, seq_len, seq_len)\n",
        "    attention_probs = F.softmax(attention_scores, dim=-1)  #A\n",
        "\n",
        "    # U, S, V1 = torch.svd(attention_probs)\n",
        "    # #print(S)\n",
        "    # print(S.pow(2).mean().sqrt())\n",
        "\n",
        "    # Compute output\n",
        "    Y = torch.matmul(attention_probs, V)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    # Create a random gradient tensor to backpropagate\n",
        "    grad_output = torch.randn_like(Y)\n",
        "    print(f\"Input grad_output norm: {grad_output.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    dd = grad_output.detach()\n",
        "    aa = torch.matmul(dd, W_v.T)\n",
        "    cc = torch.bmm(aa.transpose(-2, -1), attention_probs)\n",
        "\n",
        "    first_order_part = cc.transpose(-2, -1)\n",
        "\n",
        "\n",
        "    # Compute gradient\n",
        "    Y.backward(grad_output)\n",
        "\n",
        "    # Get the gradient and make sure it exists\n",
        "    assert X.grad is not None, \"Gradient computation failed\"\n",
        "    grad = X.grad.clone()\n",
        "\n",
        "    return Y.detach(), grad, first_order_part\n",
        "\n",
        "def main():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Create sample input\n",
        "    batch_size = 2\n",
        "    seq_len = 1000\n",
        "    d_model = 768\n",
        "    X = torch.randn(batch_size, seq_len, d_model)*1000000000000. #0.0000000000001       # disucss and Try here\n",
        "\n",
        "    # Compute self-attention and its gradient\n",
        "    Y, X_gradient, first_order_part = self_attention_with_grad(X, d_model=d_model)\n",
        "\n",
        "    # Print norms for verification\n",
        "    print(f\"Output Y=VA norm: {Y.norm(dim=-1).mean().item():.4f}\")                     #discuss here\n",
        "    print(f\"Gradient dL/dX norm: {X_gradient.norm(dim=-1).mean().item():.4f}\")         #discuss here\n",
        "\n",
        "    print(f\"Gradient norm of first order part contributed by dL/dX W_v  A: {first_order_part.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    second = X_gradient - first_order_part\n",
        "    print(f\"Gradient norm of second order part: {second.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    # Print shapes for verification\n",
        "    print(f\"\\nShapes:\")\n",
        "    print(f\"Input X: {X.shape}\")\n",
        "    print(f\"Output Y: {Y.shape}\")\n",
        "    print(f\"Gradient dL/dX: {X_gradient.shape}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# Input grad_output norm: 27.7034\n",
        "# Output Y=VA norm: 1.7390\n",
        "# Gradient dL/dX norm: 2.6637\n",
        "# Gradient norm of first order part contributed by dL/dX W_v  A: 1.4345\n",
        "# Gradient norm of second order part: 2.2390\n",
        "\n",
        "\n",
        "# Input grad_output norm: 27.7034\n",
        "# Output Y=VA norm: 27.7531\n",
        "# Gradient dL/dX norm: 44.1301\n",
        "# Gradient norm of first order part contributed by dL/dX W_v  A: 11.8748\n",
        "# Gradient norm of second order part: 41.8361\n",
        "\n",
        "\n",
        "# Input grad_output norm: 27.7034\n",
        "# Output Y=VA norm: 27899189002240.0000\n",
        "# Gradient dL/dX norm: 20.8086\n",
        "# Gradient norm of first order part contributed by dL/dX W_v  A: 20.8086\n",
        "# Gradient norm of second order part: 0.0000\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpNltj9ussyJ",
        "outputId": "c993bd6c-ba83-4a88-8948-bddced59097d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input grad_output norm: 27.7034\n",
            "Output Y=VA norm: 27899189002240.0000\n",
            "Gradient dL/dX norm: 20.8086\n",
            "Gradient norm of first order part contributed by dL/dX W_v  A: 20.8086\n",
            "Gradient norm of second order part: 0.0000\n",
            "\n",
            "Shapes:\n",
            "Input X: torch.Size([2, 1000, 768])\n",
            "Output Y: torch.Size([2, 1000, 768])\n",
            "Gradient dL/dX: torch.Size([2, 1000, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "math.sqrt(2.2390*2.2390 + 1.4345*1.4345)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nyLS8eXpbP_",
        "outputId": "1054ffeb-4b8a-4d93-f360-39228db2a9cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.6591185099577643"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. impact of the scale of $W_q^{\\top} W_k$\n"
      ],
      "metadata": {
        "id": "Eg_88--9USGD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EEzLEDTApaK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient of self-attention module\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def self_attention_with_grad(X, d_model=512):\n",
        "    \"\"\"\n",
        "    Compute self-attention and its gradient with respect to input X\n",
        "    Args:\n",
        "        X: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "        d_model: Dimension of the model\n",
        "    Returns:\n",
        "        Y: Output tensor\n",
        "        dY_dX: Gradient of Y with respect to X\n",
        "    \"\"\"\n",
        "    # Ensure X requires gradient and create a fresh copy\n",
        "    X = X.clone().detach().requires_grad_(True)\n",
        "\n",
        "    batch_size, seq_len, _ = X.shape\n",
        "    ss = 160.0\n",
        "    ss1 = 160.0\n",
        "    ss2 = 1.0                                     # Try different ss and ss1\n",
        "\n",
        "    W_q = ss*torch.randn(d_model, d_model, device=X.device)*math.sqrt(2./(d_model + d_model))\n",
        "    W_k = ss1*torch.randn(d_model, d_model, device=X.device)*math.sqrt(2./(d_model + d_model))\n",
        "    W_v = ss2*torch.randn(d_model, d_model, device=X.device)*math.sqrt(2./(d_model + d_model))\n",
        "\n",
        "    # Compute Q, K, V\n",
        "    Q = torch.matmul(X, W_q)  # (batch_size, seq_len, d_model)\n",
        "    K = torch.matmul(X, W_k)  # (batch_size, seq_len, d_model)\n",
        "    V = torch.matmul(X, W_v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    # Scaled dot-product attention\n",
        "    scale = torch.sqrt(torch.tensor(d_model, dtype=torch.float32, device=X.device))\n",
        "    attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / scale  # (batch_size, seq_len, seq_len)\n",
        "    attention_probs = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "    # Compute output\n",
        "    Y = torch.matmul(attention_probs, V)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    # Create a random gradient tensor to backpropagate\n",
        "    grad_output = torch.randn_like(Y)\n",
        "    print(f\"Input grad_output norm: {grad_output.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    dd = grad_output.detach()\n",
        "    aa = torch.matmul(dd, W_v.T)\n",
        "    cc = torch.bmm(aa.transpose(-2, -1), attention_probs)\n",
        "\n",
        "    first_order_part = cc.transpose(-2, -1)\n",
        "\n",
        "    # Compute gradient\n",
        "    Y.backward(grad_output)\n",
        "\n",
        "\n",
        "    # Get the gradient and make sure it exists\n",
        "    assert X.grad is not None, \"Gradient computation failed\"\n",
        "    grad = X.grad.clone()\n",
        "\n",
        "    return Y.detach(), grad, first_order_part\n",
        "\n",
        "def main():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Create sample input\n",
        "    batch_size = 2\n",
        "    seq_len = 1000\n",
        "    d_model = 768\n",
        "    X = torch.randn(batch_size, seq_len, d_model)*1.0 #0.000000000000000000001  #1.0\n",
        "\n",
        "    # Compute self-attention and its gradient\n",
        "    Y, X_gradient,first_order_part = self_attention_with_grad(X, d_model=d_model)\n",
        "    second_order_part = X_gradient - first_order_part\n",
        "\n",
        "\n",
        "    # Print norms for verification\n",
        "    print(f\"Output Y=VA norm: {Y.norm(dim=-1).mean().item():.4f}\")\n",
        "    print(f\"Gradient dL/dX norm: {X_gradient.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    print(f\"Gradient norm of first order part contributed by dL/dX W_v  A: {first_order_part.norm(dim=-1).mean().item():.4f}\")\n",
        "    print(f\"Gradient norm of second order part contributed by second part: {second_order_part.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    # Print shapes for verification\n",
        "    print(f\"\\nShapes:\")\n",
        "    print(f\"Input X: {X.shape}\")\n",
        "    print(f\"Output Y: {Y.shape}\")\n",
        "    print(f\"Gradient dL/dX: {X_gradient.shape}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# Input grad_output norm: 27.7034\n",
        "# Output Y=VA norm: 1.7390\n",
        "# Gradient dL/dX norm: 2.6637\n",
        "# Gradient norm of first order part contributed by dL/dX W_v  A: 1.4345\n",
        "# # Gradient norm of second order part contributed by second part: 2.2390\n",
        "\n",
        "# Input grad_output norm: 27.7034\n",
        "# Output Y=VA norm: 24.6441\n",
        "# Gradient dL/dX norm: 115.5274\n",
        "# Gradient norm of first order part contributed by dL/dX W_v  A: 19.3340\n",
        "# Gradient norm of second order part contributed by second part: 111.2305\n",
        "\n",
        "# Input grad_output norm: 27.7034\n",
        "# Output Y=VA norm: 27.6932\n",
        "# Gradient dL/dX norm: 187.9635\n",
        "# Gradient norm of first order part contributed by dL/dX W_v  A: 20.7067\n",
        "# Gradient norm of second order part contributed by second part: 172.6898\n",
        "\n",
        "# Input grad_output norm: 27.7034\n",
        "# Output Y=VA norm: 27.8915\n",
        "# Gradient dL/dX norm: 864.0630\n",
        "# Gradient norm of first order part contributed by dL/dX W_v  A: 20.8018\n",
        "# Gradient norm of second order part contributed by second part: 843.4430\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1cq44MsM6dw",
        "outputId": "526478ac-2adc-43db-d3ff-3c1586f2b6d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input grad_output norm: 27.7034\n",
            "Output Y=VA norm: 27.8915\n",
            "Gradient dL/dX norm: 864.0630\n",
            "Gradient norm of first order part contributed by dL/dX W_v  A: 20.8018\n",
            "Gradient norm of second order part contributed by second part: 843.4430\n",
            "\n",
            "Shapes:\n",
            "Input X: torch.Size([2, 1000, 768])\n",
            "Output Y: torch.Size([2, 1000, 768])\n",
            "Gradient dL/dX: torch.Size([2, 1000, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13. impact of $W_v$"
      ],
      "metadata": {
        "id": "l8wN1ldnX6ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient of self-attention module\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def self_attention_with_grad(X, d_model=512):\n",
        "    \"\"\"\n",
        "    Compute self-attention and its gradient with respect to input X\n",
        "    Args:\n",
        "        X: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "        d_model: Dimension of the model\n",
        "    Returns:\n",
        "        Y: Output tensor\n",
        "        dY_dX: Gradient of Y with respect to X\n",
        "    \"\"\"\n",
        "    # Ensure X requires gradient and create a fresh copy\n",
        "    X = X.clone().detach().requires_grad_(True)\n",
        "\n",
        "    batch_size, seq_len, _ = X.shape\n",
        "    ss = 1.0\n",
        "    ss1 = 1.0\n",
        "    ss2 = 2.0                                     # Try different ss and ss1\n",
        "\n",
        "    W_q = ss*torch.randn(d_model, d_model, device=X.device)*math.sqrt(2./(d_model + d_model))\n",
        "    W_k = ss1*torch.randn(d_model, d_model, device=X.device)*math.sqrt(2./(d_model + d_model))\n",
        "    W_v = ss2*torch.randn(d_model, d_model, device=X.device)*math.sqrt(2./(d_model + d_model))\n",
        "\n",
        "    # Compute Q, K, V\n",
        "    Q = torch.matmul(X, W_q)  # (batch_size, seq_len, d_model)\n",
        "    K = torch.matmul(X, W_k)  # (batch_size, seq_len, d_model)\n",
        "    V = torch.matmul(X, W_v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    # Scaled dot-product attention\n",
        "    scale = torch.sqrt(torch.tensor(d_model, dtype=torch.float32, device=X.device))\n",
        "    attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / scale  # (batch_size, seq_len, seq_len)\n",
        "    attention_probs = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "    # Compute output\n",
        "    Y = torch.matmul(attention_probs, V)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    # Create a random gradient tensor to backpropagate\n",
        "    grad_output = torch.randn_like(Y)\n",
        "    print(f\"Input grad_output norm: {grad_output.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    dd = grad_output.detach()\n",
        "    aa = torch.matmul(dd, W_v.T)\n",
        "    cc = torch.bmm(aa.transpose(-2, -1), attention_probs)\n",
        "\n",
        "    first_order_part = cc.transpose(-2, -1)\n",
        "\n",
        "    # Compute gradient\n",
        "    Y.backward(grad_output)\n",
        "\n",
        "\n",
        "    # Get the gradient and make sure it exists\n",
        "    assert X.grad is not None, \"Gradient computation failed\"\n",
        "    grad = X.grad.clone()\n",
        "\n",
        "    return Y.detach(), grad, first_order_part\n",
        "\n",
        "def main():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Create sample input\n",
        "    batch_size = 2\n",
        "    seq_len = 1000\n",
        "    d_model = 768\n",
        "    X = torch.randn(batch_size, seq_len, d_model)*1.0 #0.000000000000000000001  #1.0\n",
        "\n",
        "    # Compute self-attention and its gradient\n",
        "    Y, X_gradient,first_order_part = self_attention_with_grad(X, d_model=d_model)\n",
        "    second_order_part = X_gradient - first_order_part\n",
        "\n",
        "\n",
        "    # Print norms for verification\n",
        "    print(f\"Output Y=VA norm: {Y.norm(dim=-1).mean().item():.4f}\")\n",
        "    print(f\"Gradient dL/dX norm: {X_gradient.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    print(f\"Gradient norm of first order part contributed by dL/dX W_v  A: {first_order_part.norm(dim=-1).mean().item():.4f}\")\n",
        "    print(f\"Gradient norm of second order part contributed by second part: {second_order_part.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    # Print shapes for verification\n",
        "    print(f\"\\nShapes:\")\n",
        "    print(f\"Input X: {X.shape}\")\n",
        "    print(f\"Output Y: {Y.shape}\")\n",
        "    print(f\"Gradient dL/dX: {X_gradient.shape}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# Input grad_output norm: 27.7034\n",
        "# Output Y=VA norm: 1.7390\n",
        "# Gradient dL/dX norm: 2.6637\n",
        "# Gradient norm of first order part contributed by dL/dX W_v  A: 1.4345\n",
        "# Gradient norm of second order part contributed by second part: 2.2390\n",
        "\n",
        "# Input grad_output norm: 27.7034\n",
        "# Output Y=VA norm: 3.4780\n",
        "# Gradient dL/dX norm: 5.3274\n",
        "# Gradient norm of first order part contributed by dL/dX W_v  A: 2.8691\n",
        "# Gradient norm of second order part contributed by second part: 4.4779"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG3YaCqsX-NO",
        "outputId": "835f8831-0da2-4729-bc18-f516ce7ed513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input grad_output norm: 27.7034\n",
            "Output Y=VA norm: 3.4780\n",
            "Gradient dL/dX norm: 5.3274\n",
            "Gradient norm of first order part contributed by dL/dX W_v  A: 2.8691\n",
            "Gradient norm of second order part contributed by second part: 4.4779\n",
            "\n",
            "Shapes:\n",
            "Input X: torch.Size([2, 1000, 768])\n",
            "Output Y: torch.Size([2, 1000, 768])\n",
            "Gradient dL/dX: torch.Size([2, 1000, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14. impact of X"
      ],
      "metadata": {
        "id": "XLRcS7u6UI_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient of self-attention module\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def self_attention_with_grad(X, d_model=512):\n",
        "    \"\"\"\n",
        "    Compute self-attention and its gradient with respect to input X\n",
        "    Args:\n",
        "        X: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "        d_model: Dimension of the model\n",
        "    Returns:\n",
        "        Y: Output tensor\n",
        "        dY_dX: Gradient of Y with respect to X\n",
        "    \"\"\"\n",
        "    # Ensure X requires gradient and create a fresh copy\n",
        "    X = X.clone().detach().requires_grad_(True)\n",
        "\n",
        "    batch_size, seq_len, _ = X.shape\n",
        "    ss = 1.0\n",
        "    ss1 = 1.0\n",
        "    ss2 = 2.0                                     # Try different ss and ss1\n",
        "\n",
        "    W_q = ss*torch.randn(d_model, d_model, device=X.device)*math.sqrt(2./(d_model + d_model))\n",
        "    W_k = ss1*torch.randn(d_model, d_model, device=X.device)*math.sqrt(2./(d_model + d_model))\n",
        "    W_v = ss2*torch.randn(d_model, d_model, device=X.device)*math.sqrt(2./(d_model + d_model))\n",
        "\n",
        "    # Compute Q, K, V\n",
        "    Q = torch.matmul(X, W_q)  # (batch_size, seq_len, d_model)\n",
        "    K = torch.matmul(X, W_k)  # (batch_size, seq_len, d_model)\n",
        "    V = torch.matmul(X, W_v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    # Scaled dot-product attention\n",
        "    scale = torch.sqrt(torch.tensor(d_model, dtype=torch.float32, device=X.device))\n",
        "    attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / scale  # (batch_size, seq_len, seq_len)\n",
        "    attention_probs = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "    # Compute output\n",
        "    Y = torch.matmul(attention_probs, V)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    # Create a random gradient tensor to backpropagate\n",
        "    grad_output = torch.randn_like(Y)\n",
        "    print(f\"Input grad_output norm: {grad_output.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    dd = grad_output.detach()\n",
        "    aa = torch.matmul(dd, W_v.T)\n",
        "    cc = torch.bmm(aa.transpose(-2, -1), attention_probs)\n",
        "\n",
        "    first_order_part = cc.transpose(-2, -1)\n",
        "\n",
        "    # Compute gradient\n",
        "    Y.backward(grad_output)\n",
        "\n",
        "\n",
        "    # Get the gradient and make sure it exists\n",
        "    assert X.grad is not None, \"Gradient computation failed\"\n",
        "    grad = X.grad.clone()\n",
        "\n",
        "    return Y.detach(), grad, first_order_part\n",
        "\n",
        "def main():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Create sample input\n",
        "    batch_size = 2\n",
        "    seq_len = 1000\n",
        "    d_model = 768\n",
        "    X = torch.randn(batch_size, seq_len, d_model)*100000000    #from 0.1 to 1000         # Try here\n",
        "\n",
        "    # Compute self-attention and its gradient\n",
        "    Y, X_gradient,first_order_part = self_attention_with_grad(X, d_model=d_model)\n",
        "    second_order_part = X_gradient - first_order_part\n",
        "\n",
        "\n",
        "    # Print norms for verification\n",
        "    print(f\"Output Y=VA norm: {Y.norm(dim=-1).mean().item():.4f}\")\n",
        "    print(f\"Gradient dL/dX norm: {X_gradient.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    print(f\"Gradient norm of first order part contributed by dL/dX W_v  A: {first_order_part.norm(dim=-1).mean().item():.4f}\")\n",
        "    print(f\"Gradient norm of second order part contributed by second part: {second_order_part.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    # Print shapes for verification\n",
        "    print(f\"\\nShapes:\")\n",
        "    print(f\"Input X: {X.shape}\")\n",
        "    print(f\"Output Y: {Y.shape}\")\n",
        "    print(f\"Gradient dL/dX: {X_gradient.shape}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# Input grad_output norm: 27.7034\n",
        "# Output Y=VA norm: 3.4780\n",
        "# Gradient dL/dX norm: 5.3274\n",
        "# Gradient norm of first order part contributed by dL/dX W_v  A: 2.8691\n",
        "# Gradient norm of second order part contributed by second part: 4.4779\n",
        "\n",
        "# Input grad_output norm: 27.7034\n",
        "# Output Y=VA norm: 0.1732\n",
        "# Gradient dL/dX norm: 1.7702\n",
        "# Gradient norm of first order part contributed by dL/dX W_v  A: 1.7699\n",
        "# Gradient norm of second order part contributed by second part: 0.0317\n",
        "\n",
        "# Input grad_output norm: 27.7034\n",
        "# Output Y=VA norm: 5577.1709\n",
        "# Gradient dL/dX norm: 1312.5988\n",
        "# Gradient norm of first order part contributed by dL/dX W_v  A: 41.5952\n",
        "# Gradient norm of second order part contributed by second part: 1271.6075\n",
        "\n",
        "# Input grad_output norm: 27.7034\n",
        "# Output Y=VA norm: 5579837952.0000\n",
        "# Gradient dL/dX norm: 41.6172\n",
        "# Gradient norm of first order part contributed by dL/dX W_v  A: 41.6172\n",
        "# Gradient norm of second order part contributed by second part: 0.0000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6wyy4TNwIR4",
        "outputId": "01484a4a-d939-48be-fc1d-3c61d020da7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input grad_output norm: 27.7034\n",
            "Output Y=VA norm: 5579837952.0000\n",
            "Gradient dL/dX norm: 41.6172\n",
            "Gradient norm of first order part contributed by dL/dX W_v  A: 41.6172\n",
            "Gradient norm of second order part contributed by second part: 0.0000\n",
            "\n",
            "Shapes:\n",
            "Input X: torch.Size([2, 1000, 768])\n",
            "Output Y: torch.Size([2, 1000, 768])\n",
            "Gradient dL/dX: torch.Size([2, 1000, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15. impact of hidden dimension"
      ],
      "metadata": {
        "id": "gldpb-K4UFA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient of self-attention module\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def self_attention_with_grad(X, d_model=512):\n",
        "    \"\"\"\n",
        "    Compute self-attention and its gradient with respect to input X\n",
        "    Args:\n",
        "        X: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "        d_model: Dimension of the model\n",
        "    Returns:\n",
        "        Y: Output tensor\n",
        "        dY_dX: Gradient of Y with respect to X\n",
        "    \"\"\"\n",
        "    # Ensure X requires gradient and create a fresh copy\n",
        "    X = X.clone().detach().requires_grad_(True)\n",
        "\n",
        "    batch_size, seq_len, _ = X.shape\n",
        "    ss = 2.0\n",
        "    ss1 = 2.0\n",
        "    ss2 = 1.0                                     # Try different ss and ss1\n",
        "\n",
        "    W_q = ss*torch.randn(d_model, d_model, device=X.device)*math.sqrt(2./(d_model + d_model))\n",
        "    W_k = ss1*torch.randn(d_model, d_model, device=X.device)*math.sqrt(2./(d_model + d_model))\n",
        "    W_v = ss2*torch.randn(d_model, d_model, device=X.device)*math.sqrt(2./(d_model + d_model))\n",
        "\n",
        "    # Compute Q, K, V\n",
        "    Q = torch.matmul(X, W_q)  # (batch_size, seq_len, d_model)\n",
        "    K = torch.matmul(X, W_k)  # (batch_size, seq_len, d_model)\n",
        "    V = torch.matmul(X, W_v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    # Scaled dot-product attention\n",
        "    scale = torch.sqrt(torch.tensor(d_model, dtype=torch.float32, device=X.device))\n",
        "    attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / scale  # (batch_size, seq_len, seq_len)\n",
        "    attention_probs = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "    # Compute output\n",
        "    Y = torch.matmul(attention_probs, V)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    # Create a random gradient tensor to backpropagate\n",
        "    grad_output = torch.randn_like(Y)\n",
        "    print(f\"Input grad_output norm: {grad_output.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    dd = grad_output.detach()\n",
        "    aa = torch.matmul(dd, W_v.T)\n",
        "    cc = torch.bmm(aa.transpose(-2, -1), attention_probs)\n",
        "\n",
        "    first_order_part = cc.transpose(-2, -1)\n",
        "\n",
        "    # Compute gradient\n",
        "    Y.backward(grad_output)\n",
        "\n",
        "\n",
        "    # Get the gradient and make sure it exists\n",
        "    assert X.grad is not None, \"Gradient computation failed\"\n",
        "    grad = X.grad.clone()\n",
        "\n",
        "    return Y.detach(), grad, first_order_part\n",
        "\n",
        "def main():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Create sample input\n",
        "    batch_size = 2\n",
        "    seq_len = 1000\n",
        "    d_model = 768*8\n",
        "    X = torch.randn(batch_size, seq_len, d_model)*1.0    #from 0.1 to 1000         # Try here\n",
        "\n",
        "    # Compute self-attention and its gradient\n",
        "    Y, X_gradient,first_order_part = self_attention_with_grad(X, d_model=d_model)\n",
        "    second_order_part = X_gradient - first_order_part\n",
        "\n",
        "\n",
        "    # Print norms for verification\n",
        "    print(f\"Output Y=VA norm: {Y.norm(dim=-1).mean().item():.4f}\")\n",
        "    print(f\"Gradient dL/dX norm: {X_gradient.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    print(f\"Gradient norm of first order part contributed by dL/dX W_v  A: {first_order_part.norm(dim=-1).mean().item():.4f}\")\n",
        "    print(f\"Gradient norm of second order part contributed by second part: {second_order_part.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    # Print shapes for verification\n",
        "    print(f\"\\nShapes:\")\n",
        "    print(f\"Input X: {X.shape}\")\n",
        "    print(f\"Output Y: {Y.shape}\")\n",
        "    print(f\"Gradient dL/dX: {X_gradient.shape}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "# Input grad_output norm: 27.7034\n",
        "# Output Y=VA norm: 1.7390\n",
        "# Gradient dL/dX norm: 2.6637\n",
        "# Gradient norm of first order part contributed by dL/dX W_v  A: 1.4345\n",
        "# Gradient norm of second order part contributed by second part: 2.2390\n",
        "\n",
        "\n",
        "# Input grad_output norm: 55.4204\n",
        "# Output Y=VA norm: 3.0210\n",
        "# Gradient dL/dX norm: 5.0497\n",
        "# Gradient norm of first order part contributed by dL/dX W_v  A: 2.8660\n",
        "# Gradient norm of second order part contributed by second part: 4.1475\n",
        "\n",
        "# Input grad_output norm: 78.3869\n",
        "# Output Y=VA norm: 4.1681\n",
        "# Gradient dL/dX norm: 7.0736\n",
        "# Gradient norm of first order part contributed by dL/dX W_v  A: 4.0444\n",
        "# # Gradient norm of second order part contributed by second part: 5.7925\n",
        "\n",
        "# Input grad_output norm: 78.3869\n",
        "# Output Y=VA norm: 37.9995\n",
        "# Gradient dL/dX norm: 126.7267\n",
        "# Gradient norm of first order part contributed by dL/dX W_v  A: 34.0442\n",
        "# Gradient norm of second order part contributed by second part: 120.2418\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDs-zbslxYJK",
        "outputId": "526759b4-49c0-457f-ecd9-1cca47afc120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input grad_output norm: 78.3869\n",
            "Output Y=VA norm: 37.9995\n",
            "Gradient dL/dX norm: 126.7267\n",
            "Gradient norm of first order part contributed by dL/dX W_v  A: 34.0442\n",
            "Gradient norm of second order part contributed by second part: 120.2418\n",
            "\n",
            "Shapes:\n",
            "Input X: torch.Size([2, 1000, 6144])\n",
            "Output Y: torch.Size([2, 1000, 6144])\n",
            "Gradient dL/dX: torch.Size([2, 1000, 6144])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 16. impact of seq length"
      ],
      "metadata": {
        "id": "dw_GX2jiUh8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient of self-attention module\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def self_attention_with_grad(X, d_model=512):\n",
        "    \"\"\"\n",
        "    Compute self-attention and its gradient with respect to input X\n",
        "    Args:\n",
        "        X: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "        d_model: Dimension of the model\n",
        "    Returns:\n",
        "        Y: Output tensor\n",
        "        dY_dX: Gradient of Y with respect to X\n",
        "    \"\"\"\n",
        "    # Ensure X requires gradient and create a fresh copy\n",
        "    X = X.clone().detach().requires_grad_(True)\n",
        "\n",
        "    batch_size, seq_len, _ = X.shape\n",
        "    ss = 4.0\n",
        "    ss1 = 4.0\n",
        "    ss2 = 1.0                                     # Try different ss and ss1\n",
        "\n",
        "    W_q = ss*torch.randn(d_model, d_model, device=X.device)*math.sqrt(2./(d_model + d_model))\n",
        "    W_k = ss1*torch.randn(d_model, d_model, device=X.device)*math.sqrt(2./(d_model + d_model))\n",
        "    W_v = ss2*torch.randn(d_model, d_model, device=X.device)*math.sqrt(2./(d_model + d_model))\n",
        "\n",
        "    # Compute Q, K, V\n",
        "    Q = torch.matmul(X, W_q)  # (batch_size, seq_len, d_model)\n",
        "    K = torch.matmul(X, W_k)  # (batch_size, seq_len, d_model)\n",
        "    V = torch.matmul(X, W_v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    # Scaled dot-product attention\n",
        "    scale = torch.sqrt(torch.tensor(d_model, dtype=torch.float32, device=X.device))\n",
        "    attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / scale  # (batch_size, seq_len, seq_len)\n",
        "    attention_probs = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "    # Compute output\n",
        "    Y = torch.matmul(attention_probs, V)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    # Create a random gradient tensor to backpropagate\n",
        "    grad_output = torch.randn_like(Y)\n",
        "    print(f\"Input grad_output norm: {grad_output.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    dd = grad_output.detach()\n",
        "    aa = torch.matmul(dd, W_v.T)\n",
        "    cc = torch.bmm(aa.transpose(-2, -1), attention_probs)\n",
        "\n",
        "    first_order_part = cc.transpose(-2, -1)\n",
        "\n",
        "    # Compute gradient\n",
        "    Y.backward(grad_output)\n",
        "\n",
        "\n",
        "    # Get the gradient and make sure it exists\n",
        "    assert X.grad is not None, \"Gradient computation failed\"\n",
        "    grad = X.grad.clone()\n",
        "\n",
        "    return Y.detach(), grad, first_order_part\n",
        "\n",
        "def main():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Create sample input\n",
        "    batch_size = 2\n",
        "    seq_len = 2000\n",
        "    d_model = 768\n",
        "    X = torch.randn(batch_size, seq_len, d_model)*1.0    #from 0.1 to 1000         # Try here\n",
        "\n",
        "    # Compute self-attention and its gradient\n",
        "    Y, X_gradient,first_order_part = self_attention_with_grad(X, d_model=d_model)\n",
        "    second_order_part = X_gradient - first_order_part\n",
        "\n",
        "\n",
        "    # Print norms for verification\n",
        "    print(f\"Output Y=VA norm: {Y.norm(dim=-1).mean().item():.4f}\")\n",
        "    print(f\"Gradient dL/dX norm: {X_gradient.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    print(f\"Gradient norm of first order part contributed by dL/dX W_v  A: {first_order_part.norm(dim=-1).mean().item():.4f}\")\n",
        "    print(f\"Gradient norm of second order part contributed by second part: {second_order_part.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    # Print shapes for verification\n",
        "    print(f\"\\nShapes:\")\n",
        "    print(f\"Input X: {X.shape}\")\n",
        "    print(f\"Output Y: {Y.shape}\")\n",
        "    print(f\"Gradient dL/dX: {X_gradient.shape}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Input grad_output norm: 27.7034\n",
        "# Output Y=VA norm: 1.7390\n",
        "# Gradient dL/dX norm: 2.6637\n",
        "# Gradient norm of first order part contributed by dL/dX W_v  A: 1.4345\n",
        "# Gradient norm of second order part contributed by second part: 2.2390\n",
        "\n",
        "\n",
        "\n",
        "# Input grad_output norm: 27.6983\n",
        "# Output Y=VA norm: 1.4247\n",
        "# Gradient dL/dX norm: 2.0213\n",
        "# Gradient norm of first order part contributed by dL/dX W_v  A: 1.0074\n",
        "# Gradient norm of second order part contributed by second part: 1.7483\n",
        "\n",
        "# Input grad_output norm: 27.6983\n",
        "# Output Y=VA norm: 24.3520\n",
        "# Gradient dL/dX norm: 121.1854\n",
        "# Gradient norm of first order part contributed by dL/dX W_v  A: 18.8972\n",
        "# Gradient norm of second order part contributed by second part: 117.4591"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dZxU3EbzB5s",
        "outputId": "00c64431-8223-4c86-819c-5fa37a37903b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input grad_output norm: 27.6983\n",
            "Output Y=VA norm: 24.3520\n",
            "Gradient dL/dX norm: 121.1854\n",
            "Gradient norm of first order part contributed by dL/dX W_v  A: 18.8972\n",
            "Gradient norm of second order part contributed by second part: 117.4591\n",
            "\n",
            "Shapes:\n",
            "Input X: torch.Size([2, 2000, 768])\n",
            "Output Y: torch.Size([2, 2000, 768])\n",
            "Gradient dL/dX: torch.Size([2, 2000, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 17. what is the norm of $A^{\\top} \\otimes W_v$, approximate not extactly solution."
      ],
      "metadata": {
        "id": "mGSaZ_EuZBVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient of self-attention module\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def self_attention_with_grad(X, d_model=512):\n",
        "    \"\"\"\n",
        "    Compute self-attention and its gradient with respect to input X\n",
        "    Args:\n",
        "        X: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "        d_model: Dimension of the model\n",
        "    Returns:\n",
        "        Y: Output tensor\n",
        "        dY_dX: Gradient of Y with respect to X\n",
        "    \"\"\"\n",
        "    # Ensure X requires gradient and create a fresh copy\n",
        "    X = X.clone().detach().requires_grad_(True)\n",
        "\n",
        "    batch_size, seq_len, _ = X.shape\n",
        "    ss = 1.0\n",
        "    ss1 = 1.0\n",
        "    ss2 = 1.0                                     # Try different ss and ss1\n",
        "\n",
        "    W_q = ss*torch.randn(d_model, d_model, device=X.device)*math.sqrt(2./(d_model + d_model))\n",
        "    W_k = ss1*torch.randn(d_model, d_model, device=X.device)*math.sqrt(2./(d_model + d_model))\n",
        "    W_v = ss2*torch.randn(d_model, d_model, device=X.device)*math.sqrt(2./(d_model + d_model))\n",
        "\n",
        "    # Compute Q, K, V\n",
        "    Q = torch.matmul(X, W_q)  # (batch_size, seq_len, d_model)\n",
        "    K = torch.matmul(X, W_k)  # (batch_size, seq_len, d_model)\n",
        "    V = torch.matmul(X, W_v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    # Scaled dot-product attention\n",
        "    scale = torch.sqrt(torch.tensor(d_model, dtype=torch.float32, device=X.device))\n",
        "    attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / scale  # (batch_size, seq_len, seq_len)\n",
        "    attention_probs = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "    # Compute output\n",
        "    Y = torch.matmul(attention_probs, V)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    # Create a random gradient tensor to backpropagate\n",
        "    grad_output = torch.randn_like(Y)\n",
        "    print(f\"Input grad_output norm: {grad_output.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    dd = grad_output.detach()\n",
        "    aa = torch.matmul(dd, W_v.T)\n",
        "    cc = torch.bmm(aa.transpose(-2, -1), attention_probs)\n",
        "\n",
        "    first_order_part = cc.transpose(-2, -1)\n",
        "\n",
        "    # Compute gradient\n",
        "    Y.backward(grad_output)\n",
        "\n",
        "\n",
        "    # Get the gradient and make sure it exists\n",
        "    assert X.grad is not None, \"Gradient computation failed\"\n",
        "    grad = X.grad.clone()\n",
        "\n",
        "    return Y.detach(), grad, first_order_part\n",
        "\n",
        "def main():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Create sample input\n",
        "    batch_size = 2\n",
        "    seq_len = 1000\n",
        "    d_model = 768\n",
        "    X = torch.randn(batch_size, seq_len, d_model)*0.0000000000001    #from 0.1 to 1000         # Try here\n",
        "\n",
        "    # Compute self-attention and its gradient\n",
        "    Y, X_gradient,first_order_part = self_attention_with_grad(X, d_model=d_model)\n",
        "    second_order_part = X_gradient - first_order_part\n",
        "\n",
        "\n",
        "    # Print norms for verification\n",
        "    print(f\"Output Y=VA norm: {Y.norm(dim=-1).mean().item():.4f}\")\n",
        "    print(f\"Gradient dL/dX norm: {X_gradient.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    print(f\"Gradient norm of first order part contributed by dL/dX W_v  A: {first_order_part.norm(dim=-1).mean().item():.4f}\")\n",
        "    print(f\"Gradient norm of second order part contributed by second part: {second_order_part.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    # Print shapes for verification\n",
        "    print(f\"\\nShapes:\")\n",
        "    print(f\"Input X: {X.shape}\")\n",
        "    print(f\"Output Y: {Y.shape}\")\n",
        "    print(f\"Gradient dL/dX: {X_gradient.shape}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OATk865HZPxe",
        "outputId": "df8d732b-83f7-4b58-d4df-6ef9da34cd40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input grad_output norm: 27.7034\n",
            "Output Y=VA norm: 0.0000\n",
            "Gradient dL/dX norm: 0.8849\n",
            "Gradient norm of first order part contributed by dL/dX W_v  A: 0.8849\n",
            "Gradient norm of second order part contributed by second part: 0.0000\n",
            "\n",
            "Shapes:\n",
            "Input X: torch.Size([2, 1000, 768])\n",
            "Output Y: torch.Size([2, 1000, 768])\n",
            "Gradient dL/dX: torch.Size([2, 1000, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 18. gradient of FFN module"
      ],
      "metadata": {
        "id": "wKCe7ufR2GmI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TL_Vl3rM8TUA",
        "outputId": "df5575b3-2bee-4d1f-d95e-a34b85a69da2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight 1 F norm tensor(70.1137)\n",
            "Input grad_output norm: 27.7076\n",
            "Input X norm: 27.7075\n",
            "Output Y norm: 62.4894\n",
            "Gradient dL/dX norm: 62.6505\n"
          ]
        }
      ],
      "source": [
        "# gradient of FFN module\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def ffn_with_grad(X, d_model=512, d_ff=2048):\n",
        "    \"\"\"\n",
        "    Compute FFN (Feed-Forward Network) and its gradient with respect to input X\n",
        "    Args:\n",
        "        X: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "        d_model: Model dimension\n",
        "        d_ff: Feed-forward dimension (usually 4*d_model)\n",
        "    Returns:\n",
        "        Y: Output tensor\n",
        "        dL_dX: Gradient of L with respect to X\n",
        "    \"\"\"\n",
        "    # Ensure X requires gradient and create a fresh copy\n",
        "    X = X.clone().detach().requires_grad_(True)\n",
        "\n",
        "\n",
        "    ss = 2.0\n",
        "    ss1 = 2.0\n",
        "    # Initialize weights for the two linear transformations\n",
        "    W1 = ss*torch.randn(d_model, d_ff, device=X.device)* np.sqrt(2./(d_model+d_ff))              # Try different values\n",
        "    b1 = ss*torch.zeros(d_ff, device=X.device)\n",
        "\n",
        "    U,S,V1 = torch.svd(W1)\n",
        "    print(\"Weight 1 F norm\", S.pow(2).sum().sqrt())\n",
        "\n",
        "    W2 = ss1*torch.randn(d_ff, d_model, device=X.device) * np.sqrt(2./(d_model+d_ff))             # Try different values\n",
        "    b2 = ss1*torch.zeros(d_model, device=X.device)\n",
        "\n",
        "    # First linear transformation and ReLU\n",
        "    hidden = torch.matmul(X, W1) + b1  # (batch_size, seq_len, d_ff)\n",
        "    hidden = F.relu(hidden)\n",
        "\n",
        "    # Second linear transformation\n",
        "    Y = torch.matmul(hidden, W2) + b2  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    # Create a random gradient tensor to backpropagate\n",
        "    grad_output = torch.randn_like(Y)\n",
        "    print(f\"Input grad_output norm: {grad_output.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "\n",
        "    # Compute gradient\n",
        "    Y.backward(grad_output)\n",
        "\n",
        "    # Get the gradient and make sure it exists\n",
        "    assert X.grad is not None, \"Gradient computation failed\"\n",
        "    grad = X.grad.clone()\n",
        "\n",
        "    return Y.detach(), grad\n",
        "\n",
        "def main():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Create sample input\n",
        "    batch_size = 2\n",
        "    seq_len = 1000\n",
        "    d_model = 768\n",
        "    d_ff = 768*4  # Usually 4*d_model\n",
        "\n",
        "    X = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "    # Compute FFN and its gradient\n",
        "    Y, X_gradient = ffn_with_grad(X, d_model=d_model, d_ff=d_ff)\n",
        "\n",
        "    # Print norms for verification\n",
        "    print(f\"Input X norm: {X.norm(dim=-1).mean().item():.4f}\")\n",
        "    print(f\"Output Y norm: {Y.norm(dim=-1).mean().item():.4f}\")\n",
        "    print(f\"Gradient dL/dX norm: {X_gradient.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    # Verify gradient shape matches input shape\n",
        "    assert X.shape == X_gradient.shape, \"Gradient shape mismatch!\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# Weight 1 F norm tensor(35.0569)\n",
        "# Input grad_output norm: 27.7076\n",
        "# Input X norm: 27.7075\n",
        "# Output Y norm: 15.6224\n",
        "# Gradient dL/dX norm: 15.6626\n",
        "\n",
        "\n",
        "# Weight 1 F norm tensor(35.0569)\n",
        "# Input grad_output norm: 27.7076\n",
        "# Input X norm: 27.7075\n",
        "# Output Y norm: 31.2447\n",
        "# Gradient dL/dX norm: 31.3253\n",
        "\n",
        "# Weight 1 F norm tensor(70.1137)\n",
        "# Input grad_output norm: 27.7076\n",
        "# Input X norm: 27.7075\n",
        "# Output Y norm: 62.4894\n",
        "# Gradient dL/dX norm: 62.6505"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 19. gradient of SwishGLU"
      ],
      "metadata": {
        "id": "ciK8Fyhm2KRS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KzB3WQXROMf",
        "outputId": "a877ebc3-89f2-494e-a521-ed36d9a2ffb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input grad_output norm: 123.9427\n",
            "Input X norm: 4472.0405\n",
            "Output Y norm: 620946.4375\n",
            "Gradient dL/dX norm: 24601.3379\n",
            "\n",
            "Shapes:\n",
            "Input X: torch.Size([2, 1000, 768])\n",
            "Output Y: torch.Size([2, 1000, 768])\n",
            "Gradient dL/dX: torch.Size([2, 1000, 768])\n"
          ]
        }
      ],
      "source": [
        "# gradient of SwishGLU\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def ffn_with_grad(X, d_model=512, d_ff=2048):\n",
        "    \"\"\"\n",
        "    Compute FFN (Feed-Forward Network) and its gradient with respect to input X\n",
        "    Args:\n",
        "        X: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "        d_model: Model dimension\n",
        "        d_ff: Feed-forward dimension (usually 4*d_model)\n",
        "    Returns:\n",
        "        Y: Output tensor\n",
        "        dY_dX: Gradient of Y with respect to X\n",
        "    \"\"\"\n",
        "    # Ensure X requires gradient and create a fresh copy\n",
        "    X = X.clone().detach().requires_grad_(True)\n",
        "\n",
        "    # Initialize weights for the two linear transformations\n",
        "    W1 = torch.randn(d_model, d_ff, device=X.device) * np.sqrt(2./(d_model+d_ff))\n",
        "    W1 = W1/torch.norm(W1, dim=0, keepdim=True)                                   # Discuss here, remove this norm\n",
        "    #b1 = torch.zeros(d_ff, device=X.device)\n",
        "\n",
        "    W2 = torch.randn(d_model, d_ff, device=X.device) * np.sqrt(2./(d_model+d_ff))\n",
        "    W2 = W2/torch.norm(W2, dim=0, keepdim=True)                                   # Discuss here, remove this norm\n",
        "\n",
        "\n",
        "    W3 = torch.randn(d_ff, d_model, device=X.device) * np.sqrt(2./(d_model+d_ff))\n",
        "    W3 = W3/torch.norm(W3, dim=1, keepdim=True)                                   # Discuss here, remove this norm\n",
        "\n",
        "\n",
        "\n",
        "    #b2 = torch.zeros(d_model, device=X.device)\n",
        "\n",
        "    # First linear transformation and ReLU\n",
        "    u = torch.matmul(X, W1)   # (batch_size, seq_len, d_ff)\n",
        "\n",
        "    v = torch.matmul(X, W2)\n",
        "\n",
        "    s = u*F.silu(v)*torch.sqrt(torch.tensor(768.))\n",
        "\n",
        "    # Second linear transformation\n",
        "    Y = torch.matmul(s, W3)   # (batch_size, seq_len, d_model)\n",
        "\n",
        "    #Y = Y/torch.norm(Y, dim=2, keepdim=True)\n",
        "    #Y = Y\n",
        "\n",
        "    # Create a random gradient tensor to backpropagate\n",
        "    grad_output = 0.1*torch.randn_like(Y)\n",
        "    #grad_output = grad_output/torch.norm(grad_output, dim=2, keepdim=True)\n",
        "    print(f\"Input grad_output norm: {grad_output.norm().item():.4f}\")\n",
        "\n",
        "\n",
        "    # Compute gradient\n",
        "    Y.backward(grad_output)\n",
        "\n",
        "    # Get the gradient and make sure it exists\n",
        "    assert X.grad is not None, \"Gradient computation failed\"\n",
        "    grad = X.grad.clone()\n",
        "\n",
        "    return Y.detach(), grad\n",
        "\n",
        "def main():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Create sample input\n",
        "    batch_size = 2\n",
        "    seq_len = 1000\n",
        "    d_model = 768\n",
        "    d_ff = 768*4  # Usually 4*d_model\n",
        "\n",
        "    X = torch.randn(batch_size, seq_len, d_model)\n",
        "    X = 100*X/torch.norm(X, dim=2, keepdim=True)\n",
        "\n",
        "    # Compute FFN and its gradient\n",
        "    Y, X_gradient = ffn_with_grad(X, d_model=d_model, d_ff=d_ff)\n",
        "\n",
        "    # Print norms for verification\n",
        "    print(f\"Input X norm: {X.norm().item():.4f}\")\n",
        "    print(f\"Output Y norm: {Y.norm().item():.4f}\")\n",
        "    print(f\"Gradient dL/dX norm: {X_gradient.norm().item():.4f}\")\n",
        "\n",
        "    # Print shapes for verification\n",
        "    print(f\"\\nShapes:\")\n",
        "    print(f\"Input X: {X.shape}\")\n",
        "    print(f\"Output Y: {Y.shape}\")\n",
        "    print(f\"Gradient dL/dX: {X_gradient.shape}\")\n",
        "\n",
        "    # Verify gradient shape matches input shape\n",
        "    assert X.shape == X_gradient.shape, \"Gradient shape mismatch!\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  20. play with Q, K, V singular values"
      ],
      "metadata": {
        "id": "CALJ0QAa2NEL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snDDEaKkSnA-",
        "outputId": "391d331f-1374-44d5-9ff3-8ed1b7ba64aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input grad_output norm: 123.8593\n",
            "Output Y norm: 44.7206\n",
            "Gradient dL/dX norm: 390.0540\n",
            "\n",
            "Shapes:\n",
            "Input X: torch.Size([2, 1000, 768])\n",
            "Output Y: torch.Size([2, 1000, 768])\n",
            "Gradient dL/dX: torch.Size([2, 1000, 768])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def self_attention_with_grad(X, d_model=512):\n",
        "    \"\"\"\n",
        "    Compute self-attention and its gradient with respect to input X\n",
        "    Args:\n",
        "        X: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "        d_model: Dimension of the model\n",
        "    Returns:\n",
        "        Y: Output tensor\n",
        "        dY_dX: Gradient of Y with respect to X\n",
        "    \"\"\"\n",
        "    # Ensure X requires gradient and create a fresh copy\n",
        "    X = X.clone().detach().requires_grad_(True)\n",
        "\n",
        "    batch_size, seq_len, _ = X.shape\n",
        "\n",
        "    # Initialize learnable matrices\n",
        "    W_q = torch.randn(d_model, d_model, device=X.device)* np.sqrt(2./(d_model+d_model))       #discuss here\n",
        "    W_k = torch.randn(d_model, d_model, device=X.device)* np.sqrt(2./(d_model+d_model))\n",
        "    W_v = torch.randn(d_model, d_model, device=X.device)* np.sqrt(2./(d_model+d_model))\n",
        "\n",
        "    W_o = torch.randn(d_model, d_model, device=X.device)* np.sqrt(2./(d_model+d_model))\n",
        "\n",
        "\n",
        "    W_q = 4*W_q/torch.norm(W_q, dim=0, keepdim=True)          # Try here\n",
        "    W_k = 4*W_k/torch.norm(W_k, dim=0, keepdim=True)          # Try here\n",
        "    W_v = 4*W_v/torch.norm(W_v, dim=0, keepdim=True)          # Try here\n",
        "    W_o = W_o/torch.norm(W_o, dim=0, keepdim=True)            # Try here\n",
        "\n",
        "\n",
        "    # Compute Q, K, V\n",
        "    Q = torch.matmul(X, W_q)  # (batch_size, seq_len, d_model)\n",
        "    K = torch.matmul(X, W_k)  # (batch_size, seq_len, d_model)\n",
        "    V = torch.matmul(X, W_v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    # Scaled dot-product attention\n",
        "    scale = torch.sqrt(torch.tensor(d_model, dtype=torch.float32, device=X.device))\n",
        "    attention_scores = torch.sqrt(torch.tensor(768.))*torch.matmul(Q, K.transpose(-2, -1)) / scale  # (batch_size, seq_len, seq_len)\n",
        "    attention_probs = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "    # Compute output\n",
        "    Y = torch.matmul(attention_probs, V)  # (batch_size, seq_len, d_model)\n",
        "    Y = torch.matmul(Y, W_o)\n",
        "\n",
        "    Y = Y/torch.norm(Y, dim=2, keepdim=True)\n",
        "\n",
        "    # Create a random gradient tensor to backpropagate\n",
        "    grad_output = 0.1*torch.randn_like(Y)\n",
        "    print(f\"Input grad_output norm: {grad_output.norm().item():.4f}\")\n",
        "\n",
        "    # Compute gradient\n",
        "    Y.backward(grad_output)\n",
        "\n",
        "    # Get the gradient and make sure it exists\n",
        "    assert X.grad is not None, \"Gradient computation failed\"\n",
        "    grad = X.grad.clone()\n",
        "\n",
        "    return Y.detach(), grad\n",
        "\n",
        "def main():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Create sample input\n",
        "    batch_size = 2\n",
        "    seq_len = 1000\n",
        "    d_model = 768\n",
        "    X = torch.randn(batch_size, seq_len, d_model)\n",
        "    X = 100*X/torch.norm(X, dim=2, keepdim=True)\n",
        "\n",
        "    # Compute self-attention and its gradient\n",
        "    Y, X_gradient = self_attention_with_grad(X, d_model=d_model)\n",
        "\n",
        "    # Print norms for verification\n",
        "    print(f\"Output Y norm: {Y.norm().item():.4f}\")\n",
        "    print(f\"Gradient dL/dX norm: {X_gradient.norm().item():.4f}\")\n",
        "\n",
        "    # Print shapes for verification\n",
        "    print(f\"\\nShapes:\")\n",
        "    print(f\"Input X: {X.shape}\")\n",
        "    print(f\"Output Y: {Y.shape}\")\n",
        "    print(f\"Gradient dL/dX: {X_gradient.shape}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 21. see the activation of self-attention and FFN"
      ],
      "metadata": {
        "id": "3y89Qgwg2Ytk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pqOwor5bw35",
        "outputId": "f54025f8-dd0b-4c12-f05f-b928c8aa55f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Norm after block 1: 304.4572\n",
            "Norm after block 2: 443.7474\n",
            "Norm after block 3: 544.0354\n",
            "Norm after block 4: 624.8737\n",
            "Norm after block 5: 701.4705\n",
            "Norm after block 6: 770.0895\n",
            "Norm after block 7: 840.2938\n",
            "Norm after block 8: 891.9997\n",
            "Norm after block 9: 956.7276\n",
            "Norm after block 10: 1005.1816\n",
            "Norm after block 11: 1062.8512\n",
            "Norm after block 12: 1108.5682\n",
            "Norm after block 13: 1153.2292\n",
            "Norm after block 14: 1193.0669\n",
            "Norm after block 15: 1233.2219\n",
            "Norm after block 16: 1269.5864\n",
            "Norm after block 17: 1315.0914\n",
            "Norm after block 18: 1350.2504\n",
            "Norm after block 19: 1388.7863\n",
            "Norm after block 20: 1432.7998\n",
            "Norm after block 21: 1479.0663\n",
            "Norm after block 22: 1526.9390\n",
            "Norm after block 23: 1560.1458\n",
            "Norm after block 24: 1597.6620\n",
            "Norms of activations after each block: [304.4571533203125, 443.7474365234375, 544.035400390625, 624.8736572265625, 701.470458984375, 770.0894775390625, 840.2938232421875, 891.9996948242188, 956.7276000976562, 1005.1815795898438, 1062.8511962890625, 1108.5682373046875, 1153.229248046875, 1193.06689453125, 1233.221923828125, 1269.58642578125, 1315.0914306640625, 1350.2503662109375, 1388.7862548828125, 1432.7998046875, 1479.0662841796875, 1526.93896484375, 1560.145751953125, 1597.6619873046875]\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PreNormTransformerBlock(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super(PreNormTransformerBlock, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_dim // num_heads\n",
        "\n",
        "        # Weight matrices for attention\n",
        "        ss = 1.0\n",
        "        ss1 = 1.0\n",
        "        ss2 = 2.0\n",
        "        ss3 = 2.0\n",
        "\n",
        "        self.Wq = ss*torch.randn(hidden_dim, hidden_dim)*math.sqrt(2./(hidden_dim + hidden_dim))\n",
        "        self.Wk = ss1*torch.randn(hidden_dim, hidden_dim)*math.sqrt(2./(hidden_dim + hidden_dim))\n",
        "        self.Wv = ss2*torch.randn(hidden_dim, hidden_dim)*math.sqrt(2./(hidden_dim + hidden_dim))\n",
        "        self.Wo = ss3*torch.randn(hidden_dim, hidden_dim)*math.sqrt(2./(hidden_dim + hidden_dim))\n",
        "\n",
        "        # Layer norm\n",
        "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        ss4 = 4.0\n",
        "        ss5 = 4.0\n",
        "        # Parameters for feed-forward network\n",
        "        self.W1 = ss4*torch.randn(hidden_dim, ff_dim)*math.sqrt(2./(hidden_dim + ff_dim))\n",
        "        self.b1 = torch.zeros(ff_dim)\n",
        "        self.W2 = ss5*torch.randn(ff_dim, hidden_dim)*math.sqrt(2./(hidden_dim + ff_dim))\n",
        "        self.b2 = torch.zeros(hidden_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V):\n",
        "        \"\"\"\n",
        "        Compute scaled dot-product attention.\n",
        "        \"\"\"\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_output = torch.matmul(attn_weights, V)\n",
        "        return attn_output\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pre-norm for attention\n",
        "        x_norm = self.norm1(x)\n",
        "\n",
        "        # Compute Q, K, V\n",
        "        Q = torch.matmul(x_norm, self.Wq)\n",
        "        K = torch.matmul(x_norm, self.Wk)\n",
        "        V = torch.matmul(x_norm, self.Wv)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        Q = Q.view(Q.shape[0], Q.shape[1], self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(K.shape[0], K.shape[1], self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(V.shape[0], V.shape[1], self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V)\n",
        "        attn_output = attn_output.permute(0, 2, 1, 3).contiguous()\n",
        "        attn_output = attn_output.view(attn_output.shape[0], attn_output.shape[1], -1)\n",
        "\n",
        "        # Apply output projection\n",
        "        attn_output = torch.matmul(attn_output, self.Wo)\n",
        "        x = x + self.dropout(attn_output)\n",
        "\n",
        "        # Pre-norm for feed-forward network\n",
        "        x_norm = self.norm2(x)\n",
        "\n",
        "        # Feed-forward network\n",
        "        ffn_output = torch.matmul(x_norm, self.W1) + self.b1\n",
        "        ffn_output = F.relu(ffn_output)\n",
        "        ffn_output = torch.matmul(ffn_output, self.W2) + self.b2\n",
        "\n",
        "        x = x + self.dropout(ffn_output)\n",
        "\n",
        "        return x\n",
        "\n",
        "class PreNormTransformerModel(nn.Module):\n",
        "    def __init__(self, num_layers, hidden_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super(PreNormTransformerModel, self).__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            PreNormTransformerBlock(hidden_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        norms = []\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x)\n",
        "            norm = torch.norm(x, dim=-1).mean()  # Calculate the mean norm of activations\n",
        "            norms.append(norm.item())                                                        #discuss here\n",
        "            print(f\"Norm after block {i + 1}: {norm.item():.4f}\")\n",
        "        return x, norms\n",
        "\n",
        "# Define input tensor and model parameters\n",
        "batch_size = 2\n",
        "seq_len = 100\n",
        "hidden_dim = 1024\n",
        "num_layers = 24\n",
        "num_heads = 8\n",
        "ff_dim = 4096\n",
        "\n",
        "# Instantiate model\n",
        "model = PreNormTransformerModel(num_layers, hidden_dim, num_heads, ff_dim, dropout=0.1)\n",
        "\n",
        "# Random input tensor\n",
        "X = torch.randn(batch_size, seq_len, hidden_dim)*1.0                                                 #discuss here, try 10 or 100\n",
        "\n",
        "# Forward pass\n",
        "output, norms = model(X)\n",
        "\n",
        "# Output the final norms\n",
        "print(\"Norms of activations after each block:\", norms)\n",
        "\n",
        "\n",
        "#Norms of activations after each block: [37.687137603759766, 43.631919860839844, 49.15379333496094, 55.43330001831055, 62.62900924682617, 69.6657485961914]\n",
        "#Norms of activations after each block: [38.710323333740234, 49.41816329956055, 65.0183334350586, 83.65380859375, 102.41839599609375, 120.15369415283203]\n",
        "#Norms of activations after each block: [43.27378845214844, 73.94953155517578, 129.1836700439453, 176.1245880126953, 214.35826110839844, 250.1253662109375]\n",
        "#Norms of activations after each block: [85.71643829345703, 137.7996063232422, 185.8983917236328, 231.1069793701172, 268.2578430175781, 302.7626037597656]\n",
        "#Norms of activations after each block: [309.7471618652344, 443.662841796875, 546.740234375, 631.20947265625, 703.5995483398438, 775.96240234375]\n",
        "\n",
        "\n",
        "#Norms of activations after each block: [304.4571533203125, 443.7474365234375, 544.035400390625, 624.8736572265625, 701.470458984375, 770.0894775390625, 840.2938232421875, 891.9996948242188, 956.7276000976562, 1005.1815795898438, 1062.8511962890625, 1108.5682373046875, 1153.229248046875, 1193.06689453125, 1233.221923828125, 1269.58642578125, 1315.0914306640625, 1350.2503662109375, 1388.7862548828125, 1432.7998046875, 1479.0662841796875, 1526.93896484375, 1560.145751953125, 1597.6619873046875]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ujjA0pMw8GJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Part 3***"
      ],
      "metadata": {
        "id": "ihgI5D5i_LkJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 22. play the forward and backward of Transformer"
      ],
      "metadata": {
        "id": "BAX6skyL2ogx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nurxGRckQTPw",
        "outputId": "48a2e6a3-20c1-4826-ea07-b43f99343a80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Norm after block 1: 38.5175\n",
            "Norm after block 2: 45.2748\n",
            "Norm after block 3: 52.1532\n",
            "Norm after block 4: 58.6773\n",
            "Norm after block 5: 65.6663\n",
            "Norm after block 6: 72.3147\n",
            "the following is the backward gradient\n",
            "Gradient of L w.r.t. X^1: 0.0006\n",
            "Gradient of L w.r.t. X^2: 0.0004\n",
            "Gradient of L w.r.t. X^3: 0.0003\n",
            "Gradient of L w.r.t. X^4: 0.0002\n",
            "Gradient of L w.r.t. X^5: 0.0002\n",
            "Gradient of L w.r.t. X^6: 0.0002\n"
          ]
        }
      ],
      "source": [
        "#play the forward and backward of Transformer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PreNormTransformerBlock(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super(PreNormTransformerBlock, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_dim // num_heads\n",
        "        # Weight matrices for attention\n",
        "        ss = 1.0\n",
        "        ss1 = 1.0\n",
        "        ss2 = 1.0\n",
        "        ss3 = 1.0\n",
        "\n",
        "        self.Wq = ss*torch.randn(hidden_dim, hidden_dim)*math.sqrt(2./(hidden_dim + hidden_dim))\n",
        "        self.Wk = ss1*torch.randn(hidden_dim, hidden_dim)*math.sqrt(2./(hidden_dim + hidden_dim))\n",
        "        self.Wv = ss2*torch.randn(hidden_dim, hidden_dim)*math.sqrt(2./(hidden_dim + hidden_dim))\n",
        "        self.Wo = ss3*torch.randn(hidden_dim, hidden_dim)*math.sqrt(2./(hidden_dim + hidden_dim))\n",
        "\n",
        "        # Layer norm\n",
        "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        ss4 = 1.0\n",
        "        ss5 = 1.0\n",
        "        # Parameters for feed-forward network\n",
        "        self.W1 = ss4*torch.randn(hidden_dim, ff_dim)*math.sqrt(2./(hidden_dim + ff_dim))\n",
        "        self.b1 = torch.zeros(ff_dim)\n",
        "        self.W2 = ss5*torch.randn(ff_dim, hidden_dim)*math.sqrt(2./(hidden_dim + ff_dim))\n",
        "        self.b2 = torch.zeros(hidden_dim)\n",
        "\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V):\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_output = torch.matmul(attn_weights, V)\n",
        "        return attn_output\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_norm = self.norm1(x)\n",
        "\n",
        "        Q = torch.matmul(x_norm, self.Wq)\n",
        "        K = torch.matmul(x_norm, self.Wk)\n",
        "        V = torch.matmul(x_norm, self.Wv)\n",
        "\n",
        "        Q = Q.view(Q.shape[0], Q.shape[1], self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(K.shape[0], K.shape[1], self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(V.shape[0], V.shape[1], self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V)\n",
        "        attn_output = attn_output.permute(0, 2, 1, 3).contiguous()\n",
        "        attn_output = attn_output.view(attn_output.shape[0], attn_output.shape[1], -1)\n",
        "\n",
        "        attn_output = torch.matmul(attn_output, self.Wo)\n",
        "        x = x + self.dropout(attn_output)\n",
        "\n",
        "        x_norm = self.norm2(x)\n",
        "\n",
        "        ffn_output = torch.matmul(x_norm, self.W1) + self.b1\n",
        "        ffn_output = F.relu(ffn_output)\n",
        "        ffn_output = torch.matmul(ffn_output, self.W2) + self.b2\n",
        "\n",
        "        x = x + self.dropout(ffn_output)\n",
        "\n",
        "        return x\n",
        "\n",
        "class PreNormTransformerModel(nn.Module):\n",
        "    def __init__(self, num_layers, hidden_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super(PreNormTransformerModel, self).__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            PreNormTransformerBlock(hidden_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.activations = []  # To store activations for each layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.activations = []  # Reset activations\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x)\n",
        "\n",
        "            norm = torch.norm(x, dim=-1).mean()  # Calculate the mean norm of activations\n",
        "            norms.append(norm.item())\n",
        "            print(f\"Norm after block {i + 1}: {norm.item():.4f}\")\n",
        "\n",
        "            x.retain_grad()  # Retain gradients for backward pass\n",
        "            self.activations.append(x)\n",
        "        return x\n",
        "\n",
        "# Define input tensor and model parameters\n",
        "batch_size = 2\n",
        "seq_len = 500         # Try 10, 100, 500, 1000\n",
        "hidden_dim = 1024     # Try 256, 512, 1024, 4096\n",
        "num_layers = 6\n",
        "num_heads = 8\n",
        "ff_dim = 2048\n",
        "\n",
        "# Instantiate model\n",
        "model = PreNormTransformerModel(num_layers, hidden_dim, num_heads, ff_dim, dropout=0.0)\n",
        "\n",
        "# Random input tensor\n",
        "X = torch.randn(batch_size, seq_len, hidden_dim, requires_grad=True)\n",
        "\n",
        "# Define a dummy loss function\n",
        "dummy_target = torch.ones(batch_size, seq_len, hidden_dim)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Forward pass\n",
        "output = model(X)\n",
        "\n",
        "# Compute loss\n",
        "loss = criterion(output, dummy_target)\n",
        "\n",
        "# Backward pass\n",
        "loss.backward()\n",
        "\n",
        "print(\"the following is the backward gradient\")\n",
        "# Output gradients for each layer's input\n",
        "for l, activation in enumerate(model.activations):\n",
        "    print(f\"Gradient of L w.r.t. X^{l+1}: {torch.mean(activation.grad.norm(dim=2, keepdim=True)).item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 23. forward and backward process"
      ],
      "metadata": {
        "id": "BRQgFR7E2tcr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd7pR_48ghPr",
        "outputId": "f56c4758-df12-4d59-fc91-6fd11ef39355"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Norm input into self-attention block 1: 31.9634\n",
            "Norm input into FFN block 1: 1549.2820\n",
            "Norm input into self-attention block 2: 1801.9089\n",
            "Norm input into FFN block 2: 2366.6768\n",
            "Norm input into self-attention block 3: 2544.0872\n",
            "Norm input into FFN block 3: 2978.7090\n",
            "Norm input into self-attention block 4: 3123.0508\n",
            "Norm input into FFN block 4: 3471.8562\n",
            "Norm input into self-attention block 5: 3583.3672\n",
            "Norm input into FFN block 5: 3903.2988\n",
            "Norm input into self-attention block 6: 4024.1147\n",
            "Norm input into FFN block 6: 4308.9507\n",
            "Norm input into self-attention block 7: 4414.2729\n",
            "Norm input into FFN block 7: 4677.9761\n",
            "Norm input into self-attention block 8: 4755.2119\n",
            "Norm input into FFN block 8: 5004.3569\n",
            "Norm input into self-attention block 9: 5096.2939\n",
            "Norm input into FFN block 9: 5304.2944\n",
            "Norm input into self-attention block 10: 5391.0479\n",
            "Norm input into FFN block 10: 5610.8535\n",
            "Norm input into self-attention block 11: 5690.1011\n",
            "Norm input into FFN block 11: 5914.3145\n",
            "Norm input into self-attention block 12: 5977.3770\n",
            "Norm input into FFN block 12: 6176.5493\n",
            "Gradient of L w.r.t. input to attention block 1: 20755092.0000\n",
            "Gradient of L w.r.t. input to FFN block 1: 42261.1836\n",
            "Gradient of L w.r.t. input to attention block 2: 36448.1719\n",
            "Gradient of L w.r.t. input to FFN block 2: 4282.4258\n",
            "Gradient of L w.r.t. input to attention block 3: 4002.4036\n",
            "Gradient of L w.r.t. input to FFN block 3: 678.7224\n",
            "Gradient of L w.r.t. input to attention block 4: 649.8843\n",
            "Gradient of L w.r.t. input to FFN block 4: 137.3969\n",
            "Gradient of L w.r.t. input to attention block 5: 132.9523\n",
            "Gradient of L w.r.t. input to FFN block 5: 31.6923\n",
            "Gradient of L w.r.t. input to attention block 6: 30.8812\n",
            "Gradient of L w.r.t. input to FFN block 6: 8.8877\n",
            "Gradient of L w.r.t. input to attention block 7: 8.7026\n",
            "Gradient of L w.r.t. input to FFN block 7: 2.5138\n",
            "Gradient of L w.r.t. input to attention block 8: 2.4685\n",
            "Gradient of L w.r.t. input to FFN block 8: 0.8035\n",
            "Gradient of L w.r.t. input to attention block 9: 0.7908\n",
            "Gradient of L w.r.t. input to FFN block 9: 0.2764\n",
            "Gradient of L w.r.t. input to attention block 10: 0.2725\n",
            "Gradient of L w.r.t. input to FFN block 10: 0.0984\n",
            "Gradient of L w.r.t. input to attention block 11: 0.0972\n",
            "Gradient of L w.r.t. input to FFN block 11: 0.0381\n",
            "Gradient of L w.r.t. input to attention block 12: 0.0377\n",
            "Gradient of L w.r.t. input to FFN block 12: 0.0123\n"
          ]
        }
      ],
      "source": [
        "# forward and backward process\n",
        "# activations and gradients with respect to activations\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads, dropout=0.0):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_dim // num_heads\n",
        "        scale = 10.0\n",
        "\n",
        "        # Weight matrices for attention\n",
        "        self.Wq = nn.Parameter(scale * torch.randn(hidden_dim, hidden_dim) / torch.sqrt(torch.tensor(hidden_dim + hidden_dim, dtype=torch.float32)))\n",
        "        self.Wk = nn.Parameter(scale * torch.randn(hidden_dim, hidden_dim) / torch.sqrt(torch.tensor(hidden_dim + hidden_dim, dtype=torch.float32)))\n",
        "        self.Wv = nn.Parameter(scale * torch.randn(hidden_dim, hidden_dim) / torch.sqrt(torch.tensor(hidden_dim + hidden_dim, dtype=torch.float32)))\n",
        "        self.Wo = nn.Parameter(scale * torch.randn(hidden_dim, hidden_dim) / torch.sqrt(torch.tensor(hidden_dim + hidden_dim, dtype=torch.float32)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V):\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_output = torch.matmul(attn_weights, V)\n",
        "        return attn_output\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Compute Q, K, V\n",
        "        Q = torch.matmul(x, self.Wq)\n",
        "        K = torch.matmul(x, self.Wk)\n",
        "        V = torch.matmul(x, self.Wv)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        Q = Q.view(Q.shape[0], Q.shape[1], self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(K.shape[0], K.shape[1], self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(V.shape[0], V.shape[1], self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V)\n",
        "        attn_output = attn_output.permute(0, 2, 1, 3).contiguous()\n",
        "        attn_output = attn_output.view(attn_output.shape[0], attn_output.shape[1], -1)\n",
        "\n",
        "        # Output projection\n",
        "        attn_output = torch.matmul(attn_output, self.Wo)\n",
        "        return self.dropout(attn_output)\n",
        "\n",
        "class FeedForwardNetwork(nn.Module):\n",
        "    def __init__(self, hidden_dim, ff_dim, dropout=0.0):\n",
        "        super(FeedForwardNetwork, self).__init__()\n",
        "        scale = 10.\n",
        "        self.W1 = nn.Parameter(scale*torch.randn(hidden_dim, ff_dim) / torch.sqrt(torch.tensor(hidden_dim + ff_dim, dtype=torch.float32)))\n",
        "        self.b1 = nn.Parameter(torch.zeros(ff_dim))\n",
        "        self.W2 = nn.Parameter(scale*torch.randn(ff_dim, hidden_dim) / torch.sqrt(torch.tensor(hidden_dim + ff_dim, dtype=torch.float32)))\n",
        "        self.b2 = nn.Parameter(torch.zeros(hidden_dim))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ffn_output = torch.matmul(x, self.W1) + self.b1\n",
        "        ffn_output = F.relu(ffn_output)\n",
        "        ffn_output = torch.matmul(ffn_output, self.W2) + self.b2\n",
        "        return self.dropout(ffn_output)\n",
        "\n",
        "class PreNormTransformerBlock(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super(PreNormTransformerBlock, self).__init__()\n",
        "        self.attention = MultiHeadSelfAttention(hidden_dim, num_heads, dropout)\n",
        "        self.ffn = FeedForwardNetwork(hidden_dim, ff_dim, dropout)\n",
        "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pre-norm and attention\n",
        "        x1 = x\n",
        "        x1.retain_grad() # Retain gradients for input to attention\n",
        "        x_norm1 = self.norm1(x1)\n",
        "\n",
        "        attn_output = self.attention(x_norm1)\n",
        "        x_mid = x1 + attn_output\n",
        "\n",
        "        # Pre-norm and feed-forward network\n",
        "        x2 = x_mid\n",
        "        x2.retain_grad()  # Retain gradients for input to FFN\n",
        "\n",
        "        x_norm2 = self.norm2(x2)\n",
        "        ffn_output = self.ffn(x_norm2)\n",
        "        x3 = x2 + ffn_output\n",
        "        return x3, x1, x2\n",
        "\n",
        "class PreNormTransformerModel(nn.Module):\n",
        "    def __init__(self, num_layers, hidden_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super(PreNormTransformerModel, self).__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            PreNormTransformerBlock(hidden_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.attention_inputs = []  # To store inputs to attention blocks\n",
        "        self.ffn_inputs = []  # To store inputs to FFN blocks\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.attention_inputs = []\n",
        "        self.ffn_inputs = []\n",
        "        norms = []\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x, x1, x2 = layer(x)\n",
        "\n",
        "            norm = torch.norm(x1, dim=-1).mean()  # Calculate the mean norm of activations\n",
        "            norms.append(norm.item())\n",
        "            print(f\"Norm input into self-attention block {i + 1}: {norm.item():.4f}\")\n",
        "\n",
        "            norm = torch.norm(x2, dim=-1).mean()  # Calculate the mean norm of activations\n",
        "            norms.append(norm.item())\n",
        "            print(f\"Norm input into FFN block {i + 1}: {norm.item():.4f}\")\n",
        "\n",
        "\n",
        "            self.attention_inputs.append(x1)\n",
        "            self.ffn_inputs.append(x2)\n",
        "        return x\n",
        "\n",
        "# Define input tensor and model parameters\n",
        "batch_size = 1\n",
        "seq_len = 1000\n",
        "hidden_dim = 1024\n",
        "num_layers = 12\n",
        "num_heads = 8\n",
        "ff_dim = 4*hidden_dim\n",
        "\n",
        "# Instantiate model\n",
        "model = PreNormTransformerModel(num_layers, hidden_dim, num_heads, ff_dim, dropout=0.0)\n",
        "\n",
        "# Random input tensor\n",
        "X = 1.0*torch.randn(batch_size, seq_len, hidden_dim, requires_grad=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "X = X.to(device)\n",
        "\n",
        "# Define a dummy loss function\n",
        "dummy_target = torch.ones(batch_size, seq_len, hidden_dim).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Forward pass\n",
        "output = model(X)\n",
        "\n",
        "# Compute loss\n",
        "loss = criterion(output, dummy_target)\n",
        "\n",
        "# Backward pass\n",
        "loss.backward()\n",
        "\n",
        "# Output gradients for inputs to attention and FFN blocks\n",
        "for l, (attn_input, ffn_input) in enumerate(zip(model.attention_inputs, model.ffn_inputs)):\n",
        "    print(f\"Gradient of L w.r.t. input to attention block {l+1}: {torch.mean(attn_input.grad.norm(dim=2)).item():.4f}\")\n",
        "    print(f\"Gradient of L w.r.t. input to FFN block {l+1}: {torch.mean(ffn_input.grad.norm(dim=2)).item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 24. Reproduce the NaN problem"
      ],
      "metadata": {
        "id": "jrPSrut-2wfI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87X5WT9qSf9t",
        "outputId": "c10ca641-ef22-4a8b-d04a-93bcca60e20e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Norm input into self-attention block 1: 32.0183\n",
            "Norm input into FFN block 1: 862394624.0000\n",
            "Norm input into self-attention block 2: 862394624.0000\n",
            "Norm input into FFN block 2: 1565009664.0000\n",
            "Norm input into self-attention block 3: 1565009664.0000\n",
            "Norm input into FFN block 3: 1740689664.0000\n",
            "Norm input into self-attention block 4: 1740689664.0000\n",
            "Norm input into FFN block 4: 1847820032.0000\n",
            "Norm input into self-attention block 5: 1847820032.0000\n",
            "Norm input into FFN block 5: 2012462464.0000\n",
            "Norm input into self-attention block 6: 2012462464.0000\n",
            "Norm input into FFN block 6: 2380709120.0000\n",
            "Norm input into self-attention block 7: 2380709120.0000\n",
            "Norm input into FFN block 7: 2484409344.0000\n",
            "Norm input into self-attention block 8: 2484409344.0000\n",
            "Norm input into FFN block 8: 2578425856.0000\n",
            "Norm input into self-attention block 9: 2578425856.0000\n",
            "Norm input into FFN block 9: 2794188032.0000\n",
            "Norm input into self-attention block 10: 2794188032.0000\n",
            "Norm input into FFN block 10: 2846813440.0000\n",
            "Norm input into self-attention block 11: 2846813440.0000\n",
            "Norm input into FFN block 11: 2964209408.0000\n",
            "Norm input into self-attention block 12: 2964209408.0000\n",
            "Norm input into FFN block 12: 3037898240.0000\n",
            "Gradient of L w.r.t. input to attention block 1: nan\n",
            "Gradient of L w.r.t. input to FFN block 1: nan\n",
            "Gradient of L w.r.t. input to attention block 2: nan\n",
            "Gradient of L w.r.t. input to FFN block 2: inf\n",
            "Gradient of L w.r.t. input to attention block 3: inf\n",
            "Gradient of L w.r.t. input to FFN block 3: inf\n",
            "Gradient of L w.r.t. input to attention block 4: inf\n",
            "Gradient of L w.r.t. input to FFN block 4: 9490327240966144.0000\n",
            "Gradient of L w.r.t. input to attention block 5: 9490327240966144.0000\n",
            "Gradient of L w.r.t. input to FFN block 5: 246694130221056.0000\n",
            "Gradient of L w.r.t. input to attention block 6: 246694130221056.0000\n",
            "Gradient of L w.r.t. input to FFN block 6: 5674334945280.0000\n",
            "Gradient of L w.r.t. input to attention block 7: 5674334945280.0000\n",
            "Gradient of L w.r.t. input to FFN block 7: 85261688832.0000\n",
            "Gradient of L w.r.t. input to attention block 8: 85261688832.0000\n",
            "Gradient of L w.r.t. input to FFN block 8: 15549315072.0000\n",
            "Gradient of L w.r.t. input to attention block 9: 15549315072.0000\n",
            "Gradient of L w.r.t. input to FFN block 9: 460519936.0000\n",
            "Gradient of L w.r.t. input to attention block 10: 460519936.0000\n",
            "Gradient of L w.r.t. input to FFN block 10: 11790918.0000\n",
            "Gradient of L w.r.t. input to attention block 11: 11790918.0000\n",
            "Gradient of L w.r.t. input to FFN block 11: 294885.5625\n",
            "Gradient of L w.r.t. input to attention block 12: 294885.5625\n",
            "Gradient of L w.r.t. input to FFN block 12: 5933.3960\n"
          ]
        }
      ],
      "source": [
        "#reproduce the nan problem\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads, dropout=0.0):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_dim // num_heads\n",
        "        scale = 1000\n",
        "\n",
        "        rank = 2   #2, 4, 8\n",
        "        self.rank = rank                #Try here\n",
        "\n",
        "        # Low-rank decomposition for attention weights\n",
        "        self.Aq = nn.Parameter(scale * torch.randn(hidden_dim, rank) / torch.sqrt(torch.tensor(hidden_dim + rank, dtype=torch.float32)))\n",
        "        self.Bq = nn.Parameter(scale * torch.randn(rank, hidden_dim) / torch.sqrt(torch.tensor(rank + hidden_dim, dtype=torch.float32)))\n",
        "        self.Ak = nn.Parameter(scale * torch.randn(hidden_dim, rank) / torch.sqrt(torch.tensor(hidden_dim + rank, dtype=torch.float32)))\n",
        "        self.Bk = nn.Parameter(scale * torch.randn(rank, hidden_dim) / torch.sqrt(torch.tensor(rank + hidden_dim, dtype=torch.float32)))\n",
        "        self.Av = nn.Parameter(scale * torch.randn(hidden_dim, rank) / torch.sqrt(torch.tensor(hidden_dim + rank, dtype=torch.float32)))\n",
        "        self.Bv = nn.Parameter(scale * torch.randn(rank, hidden_dim) / torch.sqrt(torch.tensor(rank + hidden_dim, dtype=torch.float32)))\n",
        "        self.Ao = nn.Parameter(scale * torch.randn(hidden_dim, rank) / torch.sqrt(torch.tensor(hidden_dim + rank, dtype=torch.float32)))\n",
        "        self.Bo = nn.Parameter(scale * torch.randn(rank, hidden_dim) / torch.sqrt(torch.tensor(rank + hidden_dim, dtype=torch.float32)))\n",
        "        self.Wo = nn.Parameter(scale * torch.randn(hidden_dim, hidden_dim) / torch.sqrt(torch.tensor(hidden_dim + hidden_dim, dtype=torch.float32)))\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V):\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_output = torch.matmul(attn_weights, V)\n",
        "        return attn_output\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Compute low-rank Q, K, V\n",
        "        Q = torch.matmul(torch.matmul(x, self.Aq), self.Bq)  # Low-rank Wq, #Discuss here\n",
        "        K = torch.matmul(torch.matmul(x, self.Ak), self.Bk)  # Low-rank Wk  #Try here\n",
        "        V = torch.matmul(torch.matmul(x, self.Av), self.Bv)  # Low-rank Wv\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        Q = Q.view(Q.shape[0], Q.shape[1], self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(K.shape[0], K.shape[1], self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(V.shape[0], V.shape[1], self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V)\n",
        "        attn_output = attn_output.permute(0, 2, 1, 3).contiguous()\n",
        "        attn_output = attn_output.view(attn_output.shape[0], attn_output.shape[1], -1)\n",
        "\n",
        "        # Output projection\n",
        "        attn_output = torch.matmul(attn_output, self.Wo)\n",
        "        return attn_output\n",
        "\n",
        "class FeedForwardNetwork(nn.Module):\n",
        "    def __init__(self, hidden_dim, ff_dim):\n",
        "        super(FeedForwardNetwork, self).__init__()\n",
        "        scale = 0.1\n",
        "        self.W1 = nn.Parameter(scale*torch.randn(hidden_dim, ff_dim) / torch.sqrt(torch.tensor(hidden_dim + ff_dim, dtype=torch.float32)))\n",
        "        self.W2 = nn.Parameter(scale*torch.randn(ff_dim, hidden_dim) / torch.sqrt(torch.tensor(hidden_dim + ff_dim, dtype=torch.float32)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        ffn_output = torch.matmul(x, self.W1)\n",
        "        ffn_output = F.relu(ffn_output)\n",
        "        ffn_output = torch.matmul(ffn_output, self.W2)\n",
        "        return ffn_output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class PreNormTransformerBlock(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super(PreNormTransformerBlock, self).__init__()\n",
        "        self.attention = MultiHeadSelfAttention(hidden_dim, num_heads, dropout)\n",
        "        self.ffn = FeedForwardNetwork(hidden_dim, ff_dim)\n",
        "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pre-norm and attention\n",
        "        x1 = x\n",
        "        x1.retain_grad() # Retain gradients for input to attention\n",
        "        x_norm1 = self.norm1(x1)\n",
        "\n",
        "        attn_output = self.attention(x_norm1)\n",
        "        x_mid = x1 + attn_output\n",
        "\n",
        "        # Pre-norm and feed-forward network\n",
        "        x2 = x_mid\n",
        "        x2.retain_grad()  # Retain gradients for input to FFN\n",
        "\n",
        "        x_norm2 = self.norm2(x2)\n",
        "        ffn_output = self.ffn(x_norm2)\n",
        "        x3 = x2 + ffn_output\n",
        "        return x3, x1, x2\n",
        "\n",
        "\n",
        "class PreNormTransformerModel(nn.Module):\n",
        "    def __init__(self, num_layers, hidden_dim, num_heads, ff_dim):\n",
        "        super(PreNormTransformerModel, self).__init__()\n",
        "        self.layers = nn.ModuleList([PreNormTransformerBlock(hidden_dim, num_heads, ff_dim) for _ in range(num_layers)])\n",
        "        self.attention_inputs = []  # To store inputs to attention blocks\n",
        "        self.ffn_inputs = []  # To store inputs to FFN blocks\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.attention_inputs = []\n",
        "        self.ffn_inputs = []\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x, x1, x2 = layer(x)\n",
        "\n",
        "            norm = torch.norm(x1, dim=-1).mean()  # Calculate the mean norm of activations\n",
        "            print(f\"Norm input into self-attention block {i + 1}: {norm.item():.4f}\")\n",
        "\n",
        "            norm = torch.norm(x2, dim=-1).mean()  # Calculate the mean norm of activations\n",
        "            print(f\"Norm input into FFN block {i + 1}: {norm.item():.4f}\")\n",
        "\n",
        "            self.attention_inputs.append(x1)\n",
        "            self.ffn_inputs.append(x2)\n",
        "        return x\n",
        "\n",
        "# Define input tensor and model parameters\n",
        "batch_size = 1\n",
        "seq_len = 1000\n",
        "hidden_dim = 1024\n",
        "num_layers = 12\n",
        "num_heads = 8\n",
        "ff_dim = 4096\n",
        "\n",
        "# Instantiate model\n",
        "model = PreNormTransformerModel(num_layers, hidden_dim, num_heads, ff_dim)\n",
        "\n",
        "# Random input tensor\n",
        "X = 1*torch.randn(batch_size, seq_len, hidden_dim, requires_grad=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "X = X.to(device)\n",
        "\n",
        "# Define a dummy loss function\n",
        "dummy_target = torch.ones(batch_size, seq_len, hidden_dim).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Forward pass\n",
        "output = model(X)\n",
        "\n",
        "# Compute loss\n",
        "loss = criterion(output, dummy_target)\n",
        "\n",
        "# Backward pass\n",
        "loss.backward()\n",
        "\n",
        "# Output gradients for inputs to attention and FFN blocks\n",
        "for l, (attn_input, ffn_input) in enumerate(zip(model.attention_inputs, model.ffn_inputs)):\n",
        "    print(f\"Gradient of L w.r.t. input to attention block {l+1}: {torch.mean(attn_input.grad.norm(dim=2)).item():.4f}\")\n",
        "    print(f\"Gradient of L w.r.t. input to FFN block {l+1}: {torch.mean(ffn_input.grad.norm(dim=2)).item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 25. Deeper layer"
      ],
      "metadata": {
        "id": "DGQUxIgW8a-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#reproduce the nan problem\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_dim // num_heads\n",
        "        scale = 500\n",
        "\n",
        "        rank = 2   #2, 4, 8\n",
        "        self.rank = rank       # Try here\n",
        "\n",
        "        # Low-rank decomposition for attention weights\n",
        "        self.Aq = nn.Parameter(scale * torch.randn(hidden_dim, rank) / torch.sqrt(torch.tensor(hidden_dim + rank, dtype=torch.float32)))\n",
        "        self.Bq = nn.Parameter(scale * torch.randn(rank, hidden_dim) / torch.sqrt(torch.tensor(rank + hidden_dim, dtype=torch.float32)))\n",
        "        self.Ak = nn.Parameter(scale * torch.randn(hidden_dim, rank) / torch.sqrt(torch.tensor(hidden_dim + rank, dtype=torch.float32)))\n",
        "        self.Bk = nn.Parameter(scale * torch.randn(rank, hidden_dim) / torch.sqrt(torch.tensor(rank + hidden_dim, dtype=torch.float32)))\n",
        "        self.Av = nn.Parameter(scale * torch.randn(hidden_dim, rank) / torch.sqrt(torch.tensor(hidden_dim + rank, dtype=torch.float32)))\n",
        "        self.Bv = nn.Parameter(scale * torch.randn(rank, hidden_dim) / torch.sqrt(torch.tensor(rank + hidden_dim, dtype=torch.float32)))\n",
        "        self.Ao = nn.Parameter(scale * torch.randn(hidden_dim, rank) / torch.sqrt(torch.tensor(hidden_dim + rank, dtype=torch.float32)))\n",
        "        self.Bo = nn.Parameter(scale * torch.randn(rank, hidden_dim) / torch.sqrt(torch.tensor(rank + hidden_dim, dtype=torch.float32)))\n",
        "        self.Wo = nn.Parameter(scale * torch.randn(hidden_dim, hidden_dim) / torch.sqrt(torch.tensor(hidden_dim + hidden_dim, dtype=torch.float32)))\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V):\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_output = torch.matmul(attn_weights, V)\n",
        "        return attn_output\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Compute low-rank Q, K, V\n",
        "        Q = torch.matmul(torch.matmul(x, self.Aq), self.Bq)  # Low-rank Wq\n",
        "        K = torch.matmul(torch.matmul(x, self.Ak), self.Bk)  # Low-rank Wk\n",
        "        V = torch.matmul(torch.matmul(x, self.Av), self.Bv)  # Low-rank Wv\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        Q = Q.view(Q.shape[0], Q.shape[1], self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(K.shape[0], K.shape[1], self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(V.shape[0], V.shape[1], self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V)\n",
        "        attn_output = attn_output.permute(0, 2, 1, 3).contiguous()\n",
        "        attn_output = attn_output.view(attn_output.shape[0], attn_output.shape[1], -1)\n",
        "\n",
        "        # Output projection\n",
        "        attn_output = torch.matmul(attn_output, self.Wo)\n",
        "        return attn_output\n",
        "\n",
        "class FeedForwardNetwork(nn.Module):\n",
        "    def __init__(self, hidden_dim, ff_dim):\n",
        "        super(FeedForwardNetwork, self).__init__()\n",
        "        scale = 10\n",
        "        self.W1 = nn.Parameter(scale*torch.randn(hidden_dim, ff_dim) / torch.sqrt(torch.tensor(hidden_dim + ff_dim, dtype=torch.float32)))\n",
        "        self.W2 = nn.Parameter(scale*torch.randn(ff_dim, hidden_dim) / torch.sqrt(torch.tensor(hidden_dim + ff_dim, dtype=torch.float32)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        ffn_output = torch.matmul(x, self.W1)\n",
        "        ffn_output = F.relu(ffn_output)\n",
        "        ffn_output = torch.matmul(ffn_output, self.W2)\n",
        "        return ffn_output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class PreNormTransformerBlock(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads, ff_dim):\n",
        "        super(PreNormTransformerBlock, self).__init__()\n",
        "        self.attention = MultiHeadSelfAttention(hidden_dim, num_heads)\n",
        "        self.ffn = FeedForwardNetwork(hidden_dim, ff_dim)\n",
        "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pre-norm and attention\n",
        "        x1 = x\n",
        "        x1.retain_grad() # Retain gradients for input to attention\n",
        "        x_norm1 = self.norm1(x1)\n",
        "\n",
        "        attn_output = self.attention(x_norm1)\n",
        "        x_mid = x1 + attn_output\n",
        "\n",
        "        # Pre-norm and feed-forward network\n",
        "        x2 = x_mid\n",
        "        x2.retain_grad()  # Retain gradients for input to FFN\n",
        "\n",
        "        x_norm2 = self.norm2(x2)\n",
        "        ffn_output = self.ffn(x_norm2)\n",
        "        x3 = x2 + ffn_output\n",
        "        return x3, x1, x2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class PreNormTransformerModel(nn.Module):\n",
        "    def __init__(self, num_layers, hidden_dim, num_heads, ff_dim):\n",
        "        super(PreNormTransformerModel, self).__init__()\n",
        "        self.layers = nn.ModuleList([PreNormTransformerBlock(hidden_dim, num_heads, ff_dim) for _ in range(num_layers)])\n",
        "        self.attention_inputs = []  # To store inputs to attention blocks\n",
        "        self.ffn_inputs = []  # To store inputs to FFN blocks\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.attention_inputs = []\n",
        "        self.ffn_inputs = []\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x, x1, x2 = layer(x)\n",
        "\n",
        "            norm = torch.norm(x1, dim=-1).mean()  # Calculate the mean norm of activations\n",
        "            print(f\"Norm input into self-attention block {i + 1}: {norm.item():.4f}\")\n",
        "\n",
        "            norm = torch.norm(x2, dim=-1).mean()  # Calculate the mean norm of activations\n",
        "            print(f\"Norm input into FFN block {i + 1}: {norm.item():.4f}\")\n",
        "\n",
        "            self.attention_inputs.append(x1)\n",
        "            self.ffn_inputs.append(x2)\n",
        "        return x\n",
        "\n",
        "# Define input tensor and model parameters\n",
        "batch_size = 1\n",
        "seq_len = 1000\n",
        "hidden_dim = 1024\n",
        "num_layers = 36\n",
        "num_heads = 8\n",
        "ff_dim = 4096\n",
        "\n",
        "# Instantiate model\n",
        "model = PreNormTransformerModel(num_layers, hidden_dim, num_heads, ff_dim)\n",
        "\n",
        "# Random input tensor\n",
        "X = 1*torch.randn(batch_size, seq_len, hidden_dim, requires_grad=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "X = X.to(device)\n",
        "\n",
        "# Define a dummy loss function\n",
        "dummy_target = torch.ones(batch_size, seq_len, hidden_dim).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Forward pass\n",
        "output = model(X)\n",
        "\n",
        "# Compute loss\n",
        "loss = criterion(output, dummy_target)\n",
        "\n",
        "# Backward pass\n",
        "loss.backward()\n",
        "\n",
        "# Output gradients for inputs to attention and FFN blocks\n",
        "for l, (attn_input, ffn_input) in enumerate(zip(model.attention_inputs, model.ffn_inputs)):\n",
        "    print(f\"Gradient of L w.r.t. input to attention block {l+1}: {torch.mean(attn_input.grad.norm(dim=2)).item():.4f}\")\n",
        "    print(f\"Gradient of L w.r.t. input to FFN block {l+1}: {torch.mean(ffn_input.grad.norm(dim=2)).item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJKi8X3D7ItJ",
        "outputId": "c426ca4b-a091-47af-a725-d6e04037c95a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Norm input into self-attention block 1: 31.9828\n",
            "Norm input into FFN block 1: 107826232.0000\n",
            "Norm input into self-attention block 2: 107826224.0000\n",
            "Norm input into FFN block 2: 182624944.0000\n",
            "Norm input into self-attention block 3: 182624928.0000\n",
            "Norm input into FFN block 3: 221420288.0000\n",
            "Norm input into self-attention block 4: 221420288.0000\n",
            "Norm input into FFN block 4: 237398944.0000\n",
            "Norm input into self-attention block 5: 237398944.0000\n",
            "Norm input into FFN block 5: 249151264.0000\n",
            "Norm input into self-attention block 6: 249151264.0000\n",
            "Norm input into FFN block 6: 289732928.0000\n",
            "Norm input into self-attention block 7: 289732928.0000\n",
            "Norm input into FFN block 7: 305392832.0000\n",
            "Norm input into self-attention block 8: 305392864.0000\n",
            "Norm input into FFN block 8: 333821056.0000\n",
            "Norm input into self-attention block 9: 333821056.0000\n",
            "Norm input into FFN block 9: 350212544.0000\n",
            "Norm input into self-attention block 10: 350212544.0000\n",
            "Norm input into FFN block 10: 372145248.0000\n",
            "Norm input into self-attention block 11: 372145216.0000\n",
            "Norm input into FFN block 11: 393537056.0000\n",
            "Norm input into self-attention block 12: 393537056.0000\n",
            "Norm input into FFN block 12: 422641952.0000\n",
            "Norm input into self-attention block 13: 422641984.0000\n",
            "Norm input into FFN block 13: 431645536.0000\n",
            "Norm input into self-attention block 14: 431645504.0000\n",
            "Norm input into FFN block 14: 441084320.0000\n",
            "Norm input into self-attention block 15: 441084320.0000\n",
            "Norm input into FFN block 15: 459639552.0000\n",
            "Norm input into self-attention block 16: 459639552.0000\n",
            "Norm input into FFN block 16: 469471232.0000\n",
            "Norm input into self-attention block 17: 469471232.0000\n",
            "Norm input into FFN block 17: 493801280.0000\n",
            "Norm input into self-attention block 18: 493801280.0000\n",
            "Norm input into FFN block 18: 498671904.0000\n",
            "Norm input into self-attention block 19: 498671904.0000\n",
            "Norm input into FFN block 19: 509267840.0000\n",
            "Norm input into self-attention block 20: 509267840.0000\n",
            "Norm input into FFN block 20: 512653472.0000\n",
            "Norm input into self-attention block 21: 512653472.0000\n",
            "Norm input into FFN block 21: 524808800.0000\n",
            "Norm input into self-attention block 22: 524808832.0000\n",
            "Norm input into FFN block 22: 536036992.0000\n",
            "Norm input into self-attention block 23: 536037024.0000\n",
            "Norm input into FFN block 23: 557175552.0000\n",
            "Norm input into self-attention block 24: 557175552.0000\n",
            "Norm input into FFN block 24: 564398784.0000\n",
            "Norm input into self-attention block 25: 564398784.0000\n",
            "Norm input into FFN block 25: 574981568.0000\n",
            "Norm input into self-attention block 26: 574981504.0000\n",
            "Norm input into FFN block 26: 593616192.0000\n",
            "Norm input into self-attention block 27: 593616192.0000\n",
            "Norm input into FFN block 27: 599867648.0000\n",
            "Norm input into self-attention block 28: 599867648.0000\n",
            "Norm input into FFN block 28: 610308032.0000\n",
            "Norm input into self-attention block 29: 610308032.0000\n",
            "Norm input into FFN block 29: 612674688.0000\n",
            "Norm input into self-attention block 30: 612674752.0000\n",
            "Norm input into FFN block 30: 622295232.0000\n",
            "Norm input into self-attention block 31: 622295232.0000\n",
            "Norm input into FFN block 31: 626503232.0000\n",
            "Norm input into self-attention block 32: 626503232.0000\n",
            "Norm input into FFN block 32: 648439296.0000\n",
            "Norm input into self-attention block 33: 648439296.0000\n",
            "Norm input into FFN block 33: 658990720.0000\n",
            "Norm input into self-attention block 34: 658990720.0000\n",
            "Norm input into FFN block 34: 690014976.0000\n",
            "Norm input into self-attention block 35: 690014976.0000\n",
            "Norm input into FFN block 35: 697715776.0000\n",
            "Norm input into self-attention block 36: 697715712.0000\n",
            "Norm input into FFN block 36: 713774016.0000\n",
            "Gradient of L w.r.t. input to attention block 1: inf\n",
            "Gradient of L w.r.t. input to FFN block 1: inf\n",
            "Gradient of L w.r.t. input to attention block 2: inf\n",
            "Gradient of L w.r.t. input to FFN block 2: inf\n",
            "Gradient of L w.r.t. input to attention block 3: inf\n",
            "Gradient of L w.r.t. input to FFN block 3: inf\n",
            "Gradient of L w.r.t. input to attention block 4: inf\n",
            "Gradient of L w.r.t. input to FFN block 4: inf\n",
            "Gradient of L w.r.t. input to attention block 5: inf\n",
            "Gradient of L w.r.t. input to FFN block 5: inf\n",
            "Gradient of L w.r.t. input to attention block 6: inf\n",
            "Gradient of L w.r.t. input to FFN block 6: 33737234398052352.0000\n",
            "Gradient of L w.r.t. input to attention block 7: 33737238693019648.0000\n",
            "Gradient of L w.r.t. input to FFN block 7: 11525553328750592.0000\n",
            "Gradient of L w.r.t. input to attention block 8: 11525553328750592.0000\n",
            "Gradient of L w.r.t. input to FFN block 8: 3262620903669760.0000\n",
            "Gradient of L w.r.t. input to attention block 9: 3262620903669760.0000\n",
            "Gradient of L w.r.t. input to FFN block 9: 937756784066560.0000\n",
            "Gradient of L w.r.t. input to attention block 10: 937756716957696.0000\n",
            "Gradient of L w.r.t. input to FFN block 10: 211684224401408.0000\n",
            "Gradient of L w.r.t. input to attention block 11: 211684224401408.0000\n",
            "Gradient of L w.r.t. input to FFN block 11: 66189199933440.0000\n",
            "Gradient of L w.r.t. input to attention block 12: 66189199933440.0000\n",
            "Gradient of L w.r.t. input to FFN block 12: 36179235831808.0000\n",
            "Gradient of L w.r.t. input to attention block 13: 36179235831808.0000\n",
            "Gradient of L w.r.t. input to FFN block 13: 15100419244032.0000\n",
            "Gradient of L w.r.t. input to attention block 14: 15100420292608.0000\n",
            "Gradient of L w.r.t. input to FFN block 14: 3410843402240.0000\n",
            "Gradient of L w.r.t. input to attention block 15: 3410843402240.0000\n",
            "Gradient of L w.r.t. input to FFN block 15: 1880992907264.0000\n",
            "Gradient of L w.r.t. input to attention block 16: 1880992776192.0000\n",
            "Gradient of L w.r.t. input to FFN block 16: 478863261696.0000\n",
            "Gradient of L w.r.t. input to attention block 17: 478863261696.0000\n",
            "Gradient of L w.r.t. input to FFN block 17: 183232004096.0000\n",
            "Gradient of L w.r.t. input to attention block 18: 183231987712.0000\n",
            "Gradient of L w.r.t. input to FFN block 18: 66663559168.0000\n",
            "Gradient of L w.r.t. input to attention block 19: 66663559168.0000\n",
            "Gradient of L w.r.t. input to FFN block 19: 32503883776.0000\n",
            "Gradient of L w.r.t. input to attention block 20: 32503883776.0000\n",
            "Gradient of L w.r.t. input to FFN block 20: 24105945088.0000\n",
            "Gradient of L w.r.t. input to attention block 21: 24105945088.0000\n",
            "Gradient of L w.r.t. input to FFN block 21: 7242863616.0000\n",
            "Gradient of L w.r.t. input to attention block 22: 7242864640.0000\n",
            "Gradient of L w.r.t. input to FFN block 22: 2297149184.0000\n",
            "Gradient of L w.r.t. input to attention block 23: 2297149184.0000\n",
            "Gradient of L w.r.t. input to FFN block 23: 590474048.0000\n",
            "Gradient of L w.r.t. input to attention block 24: 590474048.0000\n",
            "Gradient of L w.r.t. input to FFN block 24: 264189488.0000\n",
            "Gradient of L w.r.t. input to attention block 25: 264189472.0000\n",
            "Gradient of L w.r.t. input to FFN block 25: 98578152.0000\n",
            "Gradient of L w.r.t. input to attention block 26: 98578152.0000\n",
            "Gradient of L w.r.t. input to FFN block 26: 30040150.0000\n",
            "Gradient of L w.r.t. input to attention block 27: 30040150.0000\n",
            "Gradient of L w.r.t. input to FFN block 27: 16649878.0000\n",
            "Gradient of L w.r.t. input to attention block 28: 16649878.0000\n",
            "Gradient of L w.r.t. input to FFN block 28: 8039377.5000\n",
            "Gradient of L w.r.t. input to attention block 29: 8039377.5000\n",
            "Gradient of L w.r.t. input to FFN block 29: 3697337.2500\n",
            "Gradient of L w.r.t. input to attention block 30: 3697337.2500\n",
            "Gradient of L w.r.t. input to FFN block 30: 1487258.2500\n",
            "Gradient of L w.r.t. input to attention block 31: 1487258.1250\n",
            "Gradient of L w.r.t. input to FFN block 31: 355721.6562\n",
            "Gradient of L w.r.t. input to attention block 32: 355721.6562\n",
            "Gradient of L w.r.t. input to FFN block 32: 146853.9219\n",
            "Gradient of L w.r.t. input to attention block 33: 146853.9062\n",
            "Gradient of L w.r.t. input to FFN block 33: 48541.7812\n",
            "Gradient of L w.r.t. input to attention block 34: 48541.7773\n",
            "Gradient of L w.r.t. input to FFN block 34: 12606.4512\n",
            "Gradient of L w.r.t. input to attention block 35: 12606.4512\n",
            "Gradient of L w.r.t. input to FFN block 35: 6898.1602\n",
            "Gradient of L w.r.t. input to attention block 36: 6898.1602\n",
            "Gradient of L w.r.t. input to FFN block 36: 1394.0900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 26. Study $W_q$, $W_k$ and $W_q^{\\top} W_k$"
      ],
      "metadata": {
        "id": "l4WgRDHjuTIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "Wq = torch.randn(128, 128) #16*1.41\n",
        "Wk = torch.randn(128, 128)\n",
        "\n",
        "Wqk = torch.mm(Wq, Wk.T)\n",
        "u1,s1,v1 = torch.svd(Wq)\n",
        "print(s1[0:10])\n",
        "\n",
        "u2,s2,v2 = torch.svd(Wk)\n",
        "print(s2[0:10])\n",
        "\n",
        "u,s,v = torch.svd(Wqk)\n",
        "print(s[0:10])\n",
        "\n"
      ],
      "metadata": {
        "id": "Dbci4t7grH-K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd9e9f50-50e1-43e3-f132-abf177fd3a88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([22.0232, 21.8135, 21.2377, 20.6127, 20.3597, 20.2243, 19.9166, 19.5745,\n",
            "        19.4448, 19.0802])\n",
            "tensor([22.7231, 22.1647, 21.0667, 20.8144, 20.7236, 20.1788, 19.9988, 19.8202,\n",
            "        19.4834, 19.1639])\n",
            "tensor([324.4269, 312.2340, 308.3246, 295.8634, 275.6356, 273.9460, 264.7079,\n",
            "        254.9568, 249.5356, 242.9580])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 27. SVD($X X^{\\top} W_q^{\\top} W_k$)"
      ],
      "metadata": {
        "id": "v-1K3NLNvkSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "X = 10.*torch.randn(128, 100)\n",
        "X_2 = torch.mm(X, X.T)\n",
        "\n",
        "Wq = torch.randn(128, 64)\n",
        "Wk = torch.randn(128, 64)\n",
        "\n",
        "Wqk = torch.mm(Wq, Wk.T)\n",
        "\n",
        "out = torch.mm(X_2, Wqk)\n",
        "u,s,v = torch.svd(out)\n",
        "print(s[0:10])\n",
        "\n"
      ],
      "metadata": {
        "id": "szmWwjF3rH4T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1aee9ea-0586-4ac4-87e7-4deb8dfab0b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([5419413.0000, 4857707.0000, 4323589.0000, 3960228.2500, 3658190.5000,\n",
            "        3574974.7500, 3411739.2500, 3257293.2500, 3087529.2500, 3027628.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 28. LN(X) in SVD($X X^{\\top} W_q^{\\top} W_k$)"
      ],
      "metadata": {
        "id": "wDelnpHXwj6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "d = 2048\n",
        "# Create a LayerNorm layer\n",
        "layer_norm = nn.LayerNorm([128]) # 4 is the normalized_shape, which corresponds to the feature dimension\n",
        "\n",
        "X = torch.randn(100,128)\n",
        "\n",
        "# Normalize the tensor\n",
        "X = layer_norm(X)\n",
        "\n",
        "X_2 = torch.mm(X.T, X)\n",
        "\n",
        "Wq = torch.randn(128, 64)\n",
        "Wk = torch.randn(128, 64)\n",
        "\n",
        "Wqk = torch.mm(Wq, Wk.T)\n",
        "\n",
        "out = torch.mm(X_2, Wqk)\n",
        "u,s,v = torch.svd(out)\n",
        "print(s[0:10])"
      ],
      "metadata": {
        "id": "6eofR5DMrHtl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6a159bd-d454-4992-cfb7-9a232d9bc74d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([51477.5117, 47185.5352, 44340.9609, 42584.2461, 37479.5859, 34200.7695,\n",
            "        33563.8789, 31600.7363, 30768.7559, 27521.9688],\n",
            "       grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "toRVGsEuBTyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 29. RMSN(X) in SVD($X X^{\\top} W_q^{\\top} W_k$)"
      ],
      "metadata": {
        "id": "UDNFr-_9BTVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "d = 2048\n",
        "# Create a LayerNorm layer\n",
        "layer_norm = nn.RMSNorm([128]) # 4 is the normalized_shape, which corresponds to the feature dimension\n",
        "\n",
        "X = torch.randn(100,128)\n",
        "\n",
        "# Normalize the tensor\n",
        "X = layer_norm(X)\n",
        "\n",
        "X_2 = torch.mm(X.T, X)\n",
        "\n",
        "Wq = torch.randn(128, 64)\n",
        "Wk = torch.randn(128, 64)\n",
        "\n",
        "Wqk = torch.mm(Wq, Wk.T)\n",
        "\n",
        "out = torch.mm(X_2, Wqk)\n",
        "u,s,v = torch.svd(out)\n",
        "print(s[0:10])"
      ],
      "metadata": {
        "id": "g_6_LJkWBVjb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c958c7df-4f0b-460d-e14c-b9a4e300005c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([51774.5352, 49899.9688, 45108.9375, 42439.7500, 39172.0469, 36968.2305,\n",
            "        34599.0156, 33056.3281, 30647.6250, 29361.5273],\n",
            "       grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 30. $W_o W_v X X^{\\top} W_q^{\\top} W_k$"
      ],
      "metadata": {
        "id": "3TrtycH5XCA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "d = 2048\n",
        "# Create a LayerNorm layer\n",
        "layer_norm = nn.RMSNorm([128]) # 4 is the normalized_shape, which corresponds to the feature dimension\n",
        "\n",
        "X = torch.randn(200,128)\n",
        "# Normalize the tensor\n",
        "X = layer_norm(X)\n",
        "\n",
        "Wo = torch.randn(128, 128)\n",
        "Wv = torch.randn(128, 128)\n",
        "\n",
        "\n",
        "X = torch.mm(X, Wv)\n",
        "\n",
        "X_2 = torch.mm(X.T, X)\n",
        "\n",
        "Wq = torch.randn(128, 64)\n",
        "Wk = torch.randn(128, 64)\n",
        "\n",
        "Wqk = torch.mm(Wq, Wk.T)\n",
        "\n",
        "out = torch.mm(X_2, Wqk)\n",
        "\n",
        "out = torch.mm(Wo, out)\n",
        "\n",
        "u,s,v = torch.svd(out)\n",
        "print(s[0:10])"
      ],
      "metadata": {
        "id": "lqizpE5ErHq0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "469a950e-23d7-40bb-a58b-ae110480da27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.7488e+08, 1.5047e+08, 1.4183e+08, 1.3364e+08, 1.2630e+08, 1.1991e+08,\n",
            "        1.1129e+08, 1.0268e+08, 8.9498e+07, 8.6902e+07],\n",
            "       grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Part 4***"
      ],
      "metadata": {
        "id": "FOnWTavf_det"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 31. Sandwich LN"
      ],
      "metadata": {
        "id": "pS4V59hUD2IN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient of self-attention module\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "def self_attention_with_grad(X, d_model=512):\n",
        "    \"\"\"\n",
        "    Compute self-attention and its gradient with respect to input X\n",
        "    Args:\n",
        "        X: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "        d_model: Dimension of the model\n",
        "    Returns:\n",
        "        Y: Output tensor\n",
        "        dY_dX: Gradient of Y with respect to X\n",
        "    \"\"\"\n",
        "    # Ensure X requires gradient and create a fresh copy\n",
        "    X = X.clone().detach().requires_grad_(True)\n",
        "\n",
        "    batch_size, seq_len, _ = X.shape\n",
        "    ss = 10.0\n",
        "    ss1 = 10.0\n",
        "    ss2 = 10.0\n",
        "    ss3 = 10.0\n",
        "\n",
        "    LN = nn.LayerNorm([d_model])\n",
        "    LN1 = nn.LayerNorm([d_model])   # Discuss here, sandwich LN\n",
        "\n",
        "    X1 = LN(X)\n",
        "\n",
        "    #X1 = X\n",
        "\n",
        "    # Initialize learnable matrices\n",
        "    W_q = torch.randn(d_model, d_model, device=X.device)*math.sqrt(2./(d_model + d_model))*ss\n",
        "    W_k = torch.randn(d_model, d_model, device=X.device)*math.sqrt(2./(d_model + d_model))*ss1\n",
        "    W_v = torch.randn(d_model, d_model, device=X.device)*math.sqrt(2./(d_model + d_model))*ss2\n",
        "    W_o = torch.randn(d_model, d_model, device=X.device)*math.sqrt(2./(d_model + d_model))*ss3\n",
        "\n",
        "    # Compute Q, K, V\n",
        "    Q = torch.matmul(X1, W_q)  # (batch_size, seq_len, d_model)\n",
        "    K = torch.matmul(X1, W_k)  # (batch_size, seq_len, d_model)\n",
        "    V = torch.matmul(X1, W_v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    # Scaled dot-product attention\n",
        "    scale = torch.sqrt(torch.tensor(d_model, dtype=torch.float32, device=X.device))\n",
        "    attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / scale  # (batch_size, seq_len, seq_len)\n",
        "    attention_probs = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "    # Compute output\n",
        "    Y = torch.matmul(attention_probs, V)  # (batch_size, seq_len, d_model)\n",
        "    Y = torch.matmul(Y, W_o)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    Y = LN1(Y)\n",
        "\n",
        "    # Create a random gradient tensor to backpropagate\n",
        "    grad_output = torch.randn_like(Y)\n",
        "    print(f\"Input grad_output norm: {grad_output.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    # Compute gradient\n",
        "    Y.backward(grad_output)\n",
        "\n",
        "    # Get the gradient and make sure it exists\n",
        "    assert X.grad is not None, \"Gradient computation failed\"\n",
        "    grad = X.grad.clone()\n",
        "\n",
        "    return Y.detach(), grad\n",
        "\n",
        "def main():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Create sample input\n",
        "    batch_size = 2\n",
        "    seq_len = 1000\n",
        "    d_model = 768\n",
        "    X = torch.randn(batch_size, seq_len, d_model)*20.0   # Try here\n",
        "\n",
        "\n",
        "    # Compute self-attention and its gradient\n",
        "    Y, X_gradient = self_attention_with_grad(X, d_model=d_model)\n",
        "\n",
        "    # Print norms for verification\n",
        "    print(f\"Output Y=VA norm: {Y.norm(dim=-1).mean().item():.4f}\")\n",
        "    print(f\"Gradient dl/dX norm: {X_gradient.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    # Print shapes for verification\n",
        "    print(f\"\\nShapes:\")\n",
        "    print(f\"Input X: {X.shape}\")\n",
        "    print(f\"Output Y: {Y.shape}\")\n",
        "    print(f\"Gradient dl/dX: {X_gradient.shape}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# Input grad_output norm: 27.6875\n",
        "# Output Y=VA norm: 27.6771\n",
        "# Gradient dl/dX norm: 41.7768\n",
        "\n",
        "\n",
        "# Input grad_output norm: 27.6875\n",
        "# Output Y=VA norm: 27.7125\n",
        "# Gradient dl/dX norm: 41.8306\n"
      ],
      "metadata": {
        "id": "jkXLMAn4VaFS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "800539db-0962-4d63-b4d5-de46255db245"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input grad_output norm: 27.6875\n",
            "Output Y=VA norm: 27.7128\n",
            "Gradient dl/dX norm: 8.4898\n",
            "\n",
            "Shapes:\n",
            "Input X: torch.Size([2, 1000, 768])\n",
            "Output Y: torch.Size([2, 1000, 768])\n",
            "Gradient dl/dX: torch.Size([2, 1000, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 32. LayerNorm + FFN"
      ],
      "metadata": {
        "id": "MzIs2aBl2Qf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient of self-attention module\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def self_attention_with_grad(X, d_model=512):\n",
        "    \"\"\"\n",
        "    Compute self-attention and its gradient with respect to input X\n",
        "    Args:\n",
        "        X: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "        d_model: Dimension of the model\n",
        "    Returns:\n",
        "        Y: Output tensor\n",
        "        dY_dX: Gradient of Y with respect to X\n",
        "    \"\"\"\n",
        "    # Ensure X requires gradient and create a fresh copy\n",
        "    X = X.clone().detach().requires_grad_(True)\n",
        "\n",
        "    batch_size, seq_len, _ = X.shape\n",
        "    ss = 2.0\n",
        "    ss1 = 2.0\n",
        "    ss2 = 1.0\n",
        "\n",
        "    LN = nn.LayerNorm([d_model])\n",
        "    LN1 = nn.LayerNorm([d_model])\n",
        "\n",
        "    X1 = LN(X)\n",
        "\n",
        "    #X1 = X\n",
        "\n",
        "    # Initialize learnable matrices\n",
        "    W1 = torch.randn(d_model, 4*d_model, device=X.device)*math.sqrt(2./(d_model + 4*d_model))*ss\n",
        "    W2 = torch.randn(d_model*4, d_model, device=X.device)*math.sqrt(2./(d_model + 4*d_model))*ss1\n",
        "\n",
        "    Y = torch.matmul(F.relu(torch.matmul(X1, W1)), W2)\n",
        "\n",
        "    #Y = LN1(Y)\n",
        "\n",
        "    # Create a random gradient tensor to backpropagate\n",
        "    grad_output = torch.randn_like(Y)\n",
        "    print(f\"Input grad_output norm: {grad_output.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    # Compute gradient\n",
        "    Y.backward(grad_output)\n",
        "\n",
        "    # Get the gradient and make sure it exists\n",
        "    assert X.grad is not None, \"Gradient computation failed\"\n",
        "    grad = X.grad.clone()\n",
        "\n",
        "    return Y.detach(), grad\n",
        "\n",
        "def main():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Create sample input\n",
        "    batch_size = 2\n",
        "    seq_len = 1000\n",
        "    d_model = 768\n",
        "    X = torch.randn(batch_size, seq_len, d_model)*1.\n",
        "\n",
        "\n",
        "    # Compute self-attention and its gradient\n",
        "    Y, X_gradient = self_attention_with_grad(X, d_model=d_model)\n",
        "\n",
        "    # Print norms for verification\n",
        "    print(f\"Output Y=FFN(LN(X)) norm: {Y.norm(dim=-1).mean().item():.4f}\")\n",
        "    print(f\"Gradient dL/dX norm: {X_gradient.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    # Print shapes for verification\n",
        "    print(f\"\\nShapes:\")\n",
        "    print(f\"Input X: {X.shape}\")\n",
        "    print(f\"Output Y: {Y.shape}\")\n",
        "    print(f\"Gradient dL/dX: {X_gradient.shape}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# Input grad_output norm: 27.7076\n",
        "# Output Y=FFN(LN(X)) norm: 15.6245\n",
        "# Gradient dL/dX norm: 15.6656\n",
        "\n",
        "\n",
        "# Input grad_output norm: 27.7076\n",
        "# Output Y=FFN(LN(X)) norm: 15.6246\n",
        "# Gradient dL/dX norm: 7.8328\n",
        "\n",
        "\n",
        "# Input grad_output norm: 27.7076\n",
        "# Output Y=FFN(LN(X)) norm: 31.2490\n",
        "# Gradient dL/dX norm: 31.3312\n",
        "\n",
        "# Input grad_output norm: 27.7076\n",
        "# Output Y=FFN(LN(X)) norm: 62.4980\n",
        "# Gradient dL/dX norm: 62.6624"
      ],
      "metadata": {
        "id": "iDdNXB9p9CP8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64448c8b-4fe8-4eea-f5ae-0f9e692e974f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input grad_output norm: 27.7076\n",
            "Output Y=FFN(LN(X)) norm: 62.4980\n",
            "Gradient dL/dX norm: 62.6624\n",
            "\n",
            "Shapes:\n",
            "Input X: torch.Size([2, 1000, 768])\n",
            "Output Y: torch.Size([2, 1000, 768])\n",
            "Gradient dL/dX: torch.Size([2, 1000, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 33. LayerNorm + FFN for large $\\|x\\|$\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "7UaUggWn2j63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient of self-attention module\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def self_attention_with_grad(X, d_model=512):\n",
        "    \"\"\"\n",
        "    Compute self-attention and its gradient with respect to input X\n",
        "    Args:\n",
        "        X: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "        d_model: Dimension of the model\n",
        "    Returns:\n",
        "        Y: Output tensor\n",
        "        dY_dX: Gradient of Y with respect to X\n",
        "    \"\"\"\n",
        "    # Ensure X requires gradient and create a fresh copy\n",
        "    X = X.clone().detach().requires_grad_(True)\n",
        "\n",
        "    batch_size, seq_len, _ = X.shape\n",
        "    ss = 1.0\n",
        "    ss1 = 1.0\n",
        "    ss2 = 1.0\n",
        "\n",
        "    LN = nn.LayerNorm([d_model])\n",
        "    LN1 = nn.LayerNorm([d_model])\n",
        "\n",
        "    X1 = LN(X)\n",
        "\n",
        "    #X1 = X\n",
        "\n",
        "    # Initialize learnable matrices\n",
        "    W1 = torch.randn(d_model, 4*d_model, device=X.device)*math.sqrt(2./(d_model + 4*d_model))*ss    # Try here\n",
        "    W2 = torch.randn(d_model*4, d_model, device=X.device)*math.sqrt(2./(d_model + 4*d_model))*ss\n",
        "\n",
        "    Y = torch.matmul(F.relu(torch.matmul(X1, W1)), W2)\n",
        "\n",
        "    #Y = LN1(Y)\n",
        "\n",
        "    # Create a random gradient tensor to backpropagate\n",
        "    grad_output = torch.randn_like(Y)\n",
        "    print(f\"Input grad_output norm: {grad_output.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    # Compute gradient\n",
        "    Y.backward(grad_output)\n",
        "\n",
        "    # Get the gradient and make sure it exists\n",
        "    assert X.grad is not None, \"Gradient computation failed\"\n",
        "    grad = X.grad.clone()\n",
        "\n",
        "    return Y.detach(), grad\n",
        "\n",
        "def main():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Create sample input\n",
        "    batch_size = 2\n",
        "    seq_len = 1000\n",
        "    d_model = 768\n",
        "    X = torch.randn(batch_size, seq_len, d_model)*10000.   # Try here\n",
        "\n",
        "\n",
        "    # Compute self-attention and its gradient\n",
        "    Y, X_gradient = self_attention_with_grad(X, d_model=d_model)\n",
        "\n",
        "    # Print norms for verification\n",
        "    print(f\"Output Y=FFN(LN(X)) norm: {Y.norm(dim=-1).mean().item():.4f}\")\n",
        "    print(f\"Gradient dL/dX norm: {X_gradient.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    # Print shapes for verification\n",
        "    print(f\"\\nShapes:\")\n",
        "    print(f\"Input X: {X.shape}\")\n",
        "    print(f\"Output Y: {Y.shape}\")\n",
        "    print(f\"Gradient dL/dX: {X_gradient.shape}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "# Input grad_output norm: 27.7076\n",
        "# Output Y=FFN(LN(X)) norm: 15.6245\n",
        "# Gradient dL/dX norm: 15.6656\n",
        "\n",
        "# Input grad_output norm: 27.7076\n",
        "# Output Y=FFN(LN(X)) norm: 15.6246\n",
        "# Gradient dL/dX norm: 0.0016"
      ],
      "metadata": {
        "id": "ea0Z8d7e9sjy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f8612e1-cd25-4dfb-90eb-9c88e7da3ba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input grad_output norm: 27.7076\n",
            "Output Y=FFN(LN(X)) norm: 15.6246\n",
            "Gradient dL/dX norm: 0.0016\n",
            "\n",
            "Shapes:\n",
            "Input X: torch.Size([2, 1000, 768])\n",
            "Output Y: torch.Size([2, 1000, 768])\n",
            "Gradient dL/dX: torch.Size([2, 1000, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 34. LayerNorm + FFN for large $W_2$ and $W_1$\n"
      ],
      "metadata": {
        "id": "wBc9rMmk21yC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient of self-attention module\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def self_attention_with_grad(X, d_model=512):\n",
        "    \"\"\"\n",
        "    Compute self-attention and its gradient with respect to input X\n",
        "    Args:\n",
        "        X: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "        d_model: Dimension of the model\n",
        "    Returns:\n",
        "        Y: Output tensor\n",
        "        dY_dX: Gradient of Y with respect to X\n",
        "    \"\"\"\n",
        "    # Ensure X requires gradient and create a fresh copy\n",
        "    X = X.clone().detach().requires_grad_(True)\n",
        "\n",
        "    batch_size, seq_len, _ = X.shape\n",
        "    ss = 1000.0\n",
        "    ss1 = 10000.0\n",
        "    ss2 = 1.0\n",
        "\n",
        "    LN = nn.LayerNorm([d_model])\n",
        "    LN1 = nn.LayerNorm([d_model])\n",
        "\n",
        "    X1 = LN(X)\n",
        "\n",
        "    #X1 = X\n",
        "\n",
        "    # Initialize learnable matrices\n",
        "    W1 = torch.randn(d_model, 4*d_model, device=X.device)*math.sqrt(2./(d_model + 4*d_model))*ss\n",
        "    W2 = torch.randn(d_model*4, d_model, device=X.device)*math.sqrt(2./(d_model + 4*d_model))*ss1        # Discuss and Try here\n",
        "\n",
        "    Y = torch.matmul(F.relu(torch.matmul(X1, W1)), W2)\n",
        "\n",
        "    #Y = LN1(Y)\n",
        "\n",
        "    # Create a random gradient tensor to backpropagate\n",
        "    grad_output = torch.randn_like(Y)\n",
        "    print(f\"Input grad_output norm: {grad_output.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    # Compute gradient\n",
        "    Y.backward(grad_output)\n",
        "\n",
        "    # Get the gradient and make sure it exists\n",
        "    assert X.grad is not None, \"Gradient computation failed\"\n",
        "    grad = X.grad.clone()\n",
        "\n",
        "    return Y.detach(), grad\n",
        "\n",
        "def main():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Create sample input\n",
        "    batch_size = 2\n",
        "    seq_len = 1000\n",
        "    d_model = 768\n",
        "    X = torch.randn(batch_size, seq_len, d_model)*1.\n",
        "\n",
        "\n",
        "    # Compute self-attention and its gradient\n",
        "    Y, X_gradient = self_attention_with_grad(X, d_model=d_model)\n",
        "\n",
        "    # Print norms for verification\n",
        "    print(f\"Output Y=FFN(LN(X)) norm: {Y.norm(dim=-1).mean().item():.4f}\")\n",
        "    print(f\"Gradient dL/dX norm: {X_gradient.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    # Print shapes for verification\n",
        "    print(f\"\\nShapes:\")\n",
        "    print(f\"Input X: {X.shape}\")\n",
        "    print(f\"Output Y: {Y.shape}\")\n",
        "    print(f\"Gradient dL/dX: {X_gradient.shape}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "# Input grad_output norm: 27.7076\n",
        "# Output Y=FFN(LN(X)) norm: 15.6245\n",
        "# Gradient dL/dX norm: 15.6656\n",
        "\n",
        "# Input grad_output norm: 27.7076\n",
        "# Output Y=FFN(LN(X)) norm: 156244976.0000\n",
        "# Gradient dL/dX norm: 156655920.0000"
      ],
      "metadata": {
        "id": "-r0NazHx2tSK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4032dfc-0028-47ce-c54d-12a5bb4b0795"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input grad_output norm: 27.7076\n",
            "Output Y=FFN(LN(X)) norm: 156244976.0000\n",
            "Gradient dL/dX norm: 156655920.0000\n",
            "\n",
            "Shapes:\n",
            "Input X: torch.Size([2, 1000, 768])\n",
            "Output Y: torch.Size([2, 1000, 768])\n",
            "Gradient dL/dX: torch.Size([2, 1000, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 35. FFN + LayerNorm"
      ],
      "metadata": {
        "id": "FAfYnhPK4TGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient of self-attention module\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def self_attention_with_grad(X, d_model=512):\n",
        "    \"\"\"\n",
        "    Compute self-attention and its gradient with respect to input X\n",
        "    Args:\n",
        "        X: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "        d_model: Dimension of the model\n",
        "    Returns:\n",
        "        Y: Output tensor\n",
        "        dY_dX: Gradient of Y with respect to X\n",
        "    \"\"\"\n",
        "    # Ensure X requires gradient and create a fresh copy\n",
        "    X = X.clone().detach().requires_grad_(True)\n",
        "\n",
        "    batch_size, seq_len, _ = X.shape\n",
        "    ss = 2.0\n",
        "    ss1 = 2.0\n",
        "    ss2 = 1.0\n",
        "\n",
        "    LN = nn.LayerNorm([d_model])\n",
        "    LN1 = nn.LayerNorm([d_model])\n",
        "\n",
        "    #X1 = LN(X)\n",
        "\n",
        "    X1 = X\n",
        "\n",
        "    # Initialize learnable matrices\n",
        "    W1 = torch.randn(d_model, 4*d_model, device=X.device)*math.sqrt(2./(d_model + 4*d_model))*ss\n",
        "    W2 = torch.randn(d_model*4, d_model, device=X.device)*math.sqrt(2./(d_model + 4*d_model))*ss1\n",
        "\n",
        "    Y = torch.matmul(F.relu(torch.matmul(X1, W1)), W2)\n",
        "\n",
        "    Y = LN1(Y)\n",
        "\n",
        "    # Create a random gradient tensor to backpropagate\n",
        "    grad_output = torch.randn_like(Y)\n",
        "    print(f\"Input grad_output norm: {grad_output.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    # Compute gradient\n",
        "    Y.backward(grad_output)\n",
        "\n",
        "    # Get the gradient and make sure it exists\n",
        "    assert X.grad is not None, \"Gradient computation failed\"\n",
        "    grad = X.grad.clone()\n",
        "\n",
        "    return Y.detach(), grad\n",
        "\n",
        "def main():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Create sample input\n",
        "    batch_size = 2\n",
        "    seq_len = 1000\n",
        "    d_model = 768\n",
        "    X = torch.randn(batch_size, seq_len, d_model)*10.\n",
        "\n",
        "\n",
        "    # Compute self-attention and its gradient\n",
        "    Y, X_gradient = self_attention_with_grad(X, d_model=d_model)\n",
        "\n",
        "    # Print norms for verification\n",
        "    print(f\"Output Y=LN(FFN(X)) norm: {Y.norm(dim=-1).mean().item():.4f}\")\n",
        "    print(f\"Gradient dL/dX norm: {X_gradient.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    # Print shapes for verification\n",
        "    print(f\"\\nShapes:\")\n",
        "    print(f\"Input X: {X.shape}\")\n",
        "    print(f\"Output Y: {Y.shape}\")\n",
        "    print(f\"Gradient dL/dX: {X_gradient.shape}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "# Input grad_output norm: 27.7076\n",
        "# Output Y=LN(FFN(X)) norm: 27.7124\n",
        "# # Gradient dL/dX norm: 27.7840\n",
        "\n",
        "# Input grad_output norm: 27.7076\n",
        "# Output Y=LN(FFN(X)) norm: 27.7127\n",
        "# Gradient dL/dX norm: 27.7843\n",
        "\n",
        "# Input grad_output norm: 27.7076\n",
        "# Output Y=LN(FFN(X)) norm: 27.7128\n",
        "# Gradient dL/dX norm: 27.7844\n",
        "\n",
        "\n",
        "# Input grad_output norm: 27.7076\n",
        "# Output Y=LN(FFN(X)) norm: 27.7128\n",
        "# Gradient dL/dX norm: 2.7784"
      ],
      "metadata": {
        "id": "SextHqYs2_RM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25902d58-7f82-4007-c521-740ee41183ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input grad_output norm: 27.7076\n",
            "Output Y=LN(FFN(X)) norm: 27.7128\n",
            "Gradient dL/dX norm: 2.7784\n",
            "\n",
            "Shapes:\n",
            "Input X: torch.Size([2, 1000, 768])\n",
            "Output Y: torch.Size([2, 1000, 768])\n",
            "Gradient dL/dX: torch.Size([2, 1000, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 36. FFN + LayerNorm for large $W_2$  and $W_1$"
      ],
      "metadata": {
        "id": "ZUyW4VF65Hz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient of self-attention module\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "def self_attention_with_grad(X, d_model=512):\n",
        "    \"\"\"\n",
        "    Compute self-attention and its gradient with respect to input X\n",
        "    Args:\n",
        "        X: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "        d_model: Dimension of the model\n",
        "    Returns:\n",
        "        Y: Output tensor\n",
        "        dY_dX: Gradient of Y with respect to X\n",
        "    \"\"\"\n",
        "    # Ensure X requires gradient and create a fresh copy\n",
        "    X = X.clone().detach().requires_grad_(True)\n",
        "\n",
        "    batch_size, seq_len, _ = X.shape\n",
        "    ss = 10000.0\n",
        "    ss1 = 1000.0\n",
        "    ss2 = 1.0\n",
        "\n",
        "    LN = nn.LayerNorm([d_model])\n",
        "    LN1 = nn.LayerNorm([d_model])\n",
        "\n",
        "    #X1 = LN(X)\n",
        "\n",
        "    X1 = X\n",
        "\n",
        "    # Initialize learnable matrices\n",
        "    W1 = torch.randn(d_model, 4*d_model, device=X.device)*math.sqrt(2./(d_model + 4*d_model))*ss             # Discuss and Try here\n",
        "    W2 = torch.randn(d_model*4, d_model, device=X.device)*math.sqrt(2./(d_model + 4*d_model))*ss1             # Discuss and Try here\n",
        "\n",
        "    Y = torch.matmul(F.relu(torch.matmul(X1, W1)), W2)\n",
        "\n",
        "    Y = LN1(Y)\n",
        "\n",
        "    # Create a random gradient tensor to backpropagate\n",
        "    grad_output = torch.randn_like(Y)\n",
        "    print(f\"Input grad_output norm: {grad_output.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    # Compute gradient\n",
        "    Y.backward(grad_output)\n",
        "\n",
        "    # Get the gradient and make sure it exists\n",
        "    assert X.grad is not None, \"Gradient computation failed\"\n",
        "    grad = X.grad.clone()\n",
        "\n",
        "    return Y.detach(), grad\n",
        "\n",
        "def main():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Create sample input\n",
        "    batch_size = 2\n",
        "    seq_len = 1000\n",
        "    d_model = 768\n",
        "    X = torch.randn(batch_size, seq_len, d_model)*1.\n",
        "\n",
        "\n",
        "    # Compute self-attention and its gradient\n",
        "    Y, X_gradient = self_attention_with_grad(X, d_model=d_model)\n",
        "\n",
        "    # Print norms for verification\n",
        "    print(f\"Output Y=LN(FFN(X)) norm: {Y.norm(dim=-1).mean().item():.4f}\")\n",
        "    print(f\"Gradient dL/dX norm: {X_gradient.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    # Print shapes for verification\n",
        "    print(f\"\\nShapes:\")\n",
        "    print(f\"Input X: {X.shape}\")\n",
        "    print(f\"Output Y: {Y.shape}\")\n",
        "    print(f\"Gradient dL/dX: {X_gradient.shape}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Lvyr2yQD4c1n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee167f84-a71a-425e-dd88-f8a4778c556b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input grad_output norm: 27.7076\n",
            "Output Y=LN(FFN(X)) norm: 27.7128\n",
            "Gradient dL/dX norm: 27.7844\n",
            "\n",
            "Shapes:\n",
            "Input X: torch.Size([2, 1000, 768])\n",
            "Output Y: torch.Size([2, 1000, 768])\n",
            "Gradient dL/dX: torch.Size([2, 1000, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 37. FFN + LayerNorm for large $\\|x \\|$\n"
      ],
      "metadata": {
        "id": "1kyZLZ555cwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient of self-attention module\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def self_attention_with_grad(X, d_model=512):\n",
        "    \"\"\"\n",
        "    Compute self-attention and its gradient with respect to input X\n",
        "    Args:\n",
        "        X: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "        d_model: Dimension of the model\n",
        "    Returns:\n",
        "        Y: Output tensor\n",
        "        dY_dX: Gradient of Y with respect to X\n",
        "    \"\"\"\n",
        "    # Ensure X requires gradient and create a fresh copy\n",
        "    X = X.clone().detach().requires_grad_(True)\n",
        "\n",
        "    batch_size, seq_len, _ = X.shape\n",
        "    ss = 1.0\n",
        "    ss1 = 1.0\n",
        "    ss2 = 1.0\n",
        "\n",
        "    LN = nn.LayerNorm([d_model])\n",
        "    LN1 = nn.LayerNorm([d_model])\n",
        "\n",
        "    #X1 = LN(X)\n",
        "\n",
        "    X1 = X\n",
        "\n",
        "    # Initialize learnable matrices\n",
        "    W1 = torch.randn(d_model, 4*d_model, device=X.device)*math.sqrt(2./(d_model + 4*d_model))*ss\n",
        "    W2 = torch.randn(d_model*4, d_model, device=X.device)*math.sqrt(2./(d_model + 4*d_model))*ss\n",
        "\n",
        "    Y = torch.matmul(F.relu(torch.matmul(X1, W1)), W2)\n",
        "\n",
        "    Y = LN1(Y)\n",
        "\n",
        "    # Create a random gradient tensor to backpropagate\n",
        "    grad_output = torch.randn_like(Y)\n",
        "    print(f\"Input grad_output norm: {grad_output.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    # Compute gradient\n",
        "    Y.backward(grad_output)\n",
        "\n",
        "    # Get the gradient and make sure it exists\n",
        "    assert X.grad is not None, \"Gradient computation failed\"\n",
        "    grad = X.grad.clone()\n",
        "\n",
        "    return Y.detach(), grad\n",
        "\n",
        "def main():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Create sample input\n",
        "    batch_size = 2\n",
        "    seq_len = 1000\n",
        "    d_model = 768\n",
        "    X = torch.randn(batch_size, seq_len, d_model)*10000.0    # Try and Discuss here,\n",
        "\n",
        "\n",
        "    # Compute self-attention and its gradient\n",
        "    Y, X_gradient = self_attention_with_grad(X, d_model=d_model)\n",
        "\n",
        "    # Print norms for verification\n",
        "    print(f\"Output Y=LN(FFN(X)) norm: {Y.norm(dim=-1).mean().item():.4f}\")\n",
        "    print(f\"Gradient dL/dX norm: {X_gradient.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    # Print shapes for verification\n",
        "    print(f\"\\nShapes:\")\n",
        "    print(f\"Input X: {X.shape}\")\n",
        "    print(f\"Output Y: {Y.shape}\")\n",
        "    print(f\"Gradient dL/dX: {X_gradient.shape}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "e82LK9OW5aPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b6d1d61-ddde-4990-e97c-5d2fa6370b71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input grad_output norm: 27.7076\n",
            "Output Y=LN(FFN(X)) norm: 27.7128\n",
            "Gradient dL/dX norm: 0.0028\n",
            "\n",
            "Shapes:\n",
            "Input X: torch.Size([2, 1000, 768])\n",
            "Output Y: torch.Size([2, 1000, 768])\n",
            "Gradient dL/dX: torch.Size([2, 1000, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 38. LayerNorm + FFN + LayerNorm"
      ],
      "metadata": {
        "id": "g6WgrPs95tYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient of self-attention module\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "def self_attention_with_grad(X, d_model=512):\n",
        "    \"\"\"\n",
        "    Compute self-attention and its gradient with respect to input X\n",
        "    Args:\n",
        "        X: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "        d_model: Dimension of the model\n",
        "    Returns:\n",
        "        Y: Output tensor\n",
        "        dY_dX: Gradient of Y with respect to X\n",
        "    \"\"\"\n",
        "    # Ensure X requires gradient and create a fresh copy\n",
        "    X = X.clone().detach().requires_grad_(True)\n",
        "\n",
        "    batch_size, seq_len, _ = X.shape\n",
        "    ss = 10.0\n",
        "    ss1 = 100.0\n",
        "    ss2 = 1.0\n",
        "\n",
        "    LN = nn.LayerNorm([d_model])\n",
        "    LN1 = nn.LayerNorm([d_model])\n",
        "\n",
        "    X1 = LN(X)\n",
        "\n",
        "    # Initialize learnable matrices\n",
        "    W1 = torch.randn(d_model, 4*d_model, device=X.device)*math.sqrt(2./(d_model + 4*d_model))*ss\n",
        "    W2 = torch.randn(d_model*4, d_model, device=X.device)*math.sqrt(2./(d_model + 4*d_model))*ss1\n",
        "\n",
        "    Y = torch.matmul(F.relu(torch.matmul(X1, W1)), W2)\n",
        "\n",
        "    Y = LN1(Y)\n",
        "\n",
        "    # Create a random gradient tensor to backpropagate\n",
        "    grad_output = torch.randn_like(Y)\n",
        "    print(f\"Input grad_output norm: {grad_output.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    # Compute gradient\n",
        "    Y.backward(grad_output)\n",
        "\n",
        "    # Get the gradient and make sure it exists\n",
        "    assert X.grad is not None, \"Gradient computation failed\"\n",
        "    grad = X.grad.clone()\n",
        "\n",
        "    return Y.detach(), grad\n",
        "\n",
        "def main():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Create sample input\n",
        "    batch_size = 2\n",
        "    seq_len = 1000\n",
        "    d_model = 768\n",
        "    X = torch.randn(batch_size, seq_len, d_model)*100.\n",
        "\n",
        "\n",
        "    # Compute self-attention and its gradient\n",
        "    Y, X_gradient = self_attention_with_grad(X, d_model=d_model)\n",
        "\n",
        "    # Print norms for verification\n",
        "    print(f\"Output Y=LN(FFN(LN(X))) norm: {Y.norm(dim=-1).mean().item():.4f}\")\n",
        "    print(f\"Gradient dL/dX norm: {X_gradient.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    # Print shapes for verification\n",
        "    print(f\"\\nShapes:\")\n",
        "    print(f\"Input X: {X.shape}\")\n",
        "    print(f\"Output Y: {Y.shape}\")\n",
        "    print(f\"Gradient dL/dX: {X_gradient.shape}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Input grad_output norm: 27.7076\n",
        "# Output Y=LN(FFN(LN(X))) norm: 27.7124\n",
        "# Gradient dL/dX norm: 27.7840\n",
        "\n",
        "\n",
        "# Input grad_output norm: 27.7076\n",
        "# Output Y=LN(FFN(LN(X))) norm: 27.7128\n",
        "# Gradient dL/dX norm: 27.7844\n",
        "\n",
        "\n",
        "\n",
        "# Input grad_output norm: 27.7076\n",
        "# Output Y=LN(FFN(LN(X))) norm: 27.7128\n",
        "# Gradient dL/dX norm: 27.7844\n",
        "\n",
        "\n",
        "# Input grad_output norm: 27.7076\n",
        "# Output Y=LN(FFN(LN(X))) norm: 27.7128\n",
        "# Gradient dL/dX norm: 0.2778"
      ],
      "metadata": {
        "id": "ofDtuog45RNR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7beb8a1e-32a9-43c1-c198-33c40a66df3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input grad_output norm: 27.7076\n",
            "Output Y=LN(FFN(LN(X))) norm: 27.7128\n",
            "Gradient dL/dX norm: 0.2778\n",
            "\n",
            "Shapes:\n",
            "Input X: torch.Size([2, 1000, 768])\n",
            "Output Y: torch.Size([2, 1000, 768])\n",
            "Gradient dL/dX: torch.Size([2, 1000, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 39. LayerNorm + FFN + LayerNorm for large $\\|x\\|$"
      ],
      "metadata": {
        "id": "HAT09AsrE3O5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient of self-attention module\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "def self_attention_with_grad(X, d_model=512):\n",
        "    \"\"\"\n",
        "    Compute self-attention and its gradient with respect to input X\n",
        "    Args:\n",
        "        X: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "        d_model: Dimension of the model\n",
        "    Returns:\n",
        "        Y: Output tensor\n",
        "        dY_dX: Gradient of Y with respect to X\n",
        "    \"\"\"\n",
        "    # Ensure X requires gradient and create a fresh copy\n",
        "    X = X.clone().detach().requires_grad_(True)\n",
        "\n",
        "    batch_size, seq_len, _ = X.shape\n",
        "    ss = 1.0\n",
        "    ss1 = 1.0\n",
        "    ss2 = 1.0\n",
        "\n",
        "    LN = nn.LayerNorm([d_model])\n",
        "    LN1 = nn.LayerNorm([d_model])\n",
        "\n",
        "    X1 = LN(X)\n",
        "\n",
        "    # Initialize learnable matrices\n",
        "    W1 = torch.randn(d_model, 4*d_model, device=X.device)*math.sqrt(2./(d_model + 4*d_model))*ss\n",
        "    W2 = torch.randn(d_model*4, d_model, device=X.device)*math.sqrt(2./(d_model + 4*d_model))*ss\n",
        "\n",
        "    Y = torch.matmul(F.relu(torch.matmul(X1, W1)), W2)\n",
        "\n",
        "    Y = LN1(Y)\n",
        "\n",
        "    # Create a random gradient tensor to backpropagate\n",
        "    grad_output = torch.randn_like(Y)\n",
        "    print(f\"Input grad_output norm: {grad_output.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    # Compute gradient\n",
        "    Y.backward(grad_output)\n",
        "\n",
        "    # Get the gradient and make sure it exists\n",
        "    assert X.grad is not None, \"Gradient computation failed\"\n",
        "    grad = X.grad.clone()\n",
        "\n",
        "    return Y.detach(), grad\n",
        "\n",
        "def main():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Create sample input\n",
        "    batch_size = 2\n",
        "    seq_len = 1000\n",
        "    d_model = 768\n",
        "    X = torch.randn(batch_size, seq_len, d_model)*1000000000.    # Discuss here, Try here\n",
        "\n",
        "\n",
        "    # Compute self-attention and its gradient\n",
        "    Y, X_gradient = self_attention_with_grad(X, d_model=d_model)\n",
        "\n",
        "    # Print norms for verification\n",
        "    print(f\"Output Y=LN(FFN(LN(X))) norm: {Y.norm(dim=-1).mean().item():.4f}\")\n",
        "    print(f\"Gradient dL/dX norm: {X_gradient.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    # Print shapes for verification\n",
        "    print(f\"\\nShapes:\")\n",
        "    print(f\"Input X: {X.shape}\")\n",
        "    print(f\"Output Y: {Y.shape}\")\n",
        "    print(f\"Gradient dL/dX: {X_gradient.shape}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "e54Wns7T511E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33001d8c-fc50-4cc3-d98f-eceadd41efd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input grad_output norm: 27.7076\n",
            "Output Y=LN(FFN(LN(X))) norm: 27.7124\n",
            "Gradient dL/dX norm: 0.0000\n",
            "\n",
            "Shapes:\n",
            "Input X: torch.Size([2, 1000, 768])\n",
            "Output Y: torch.Size([2, 1000, 768])\n",
            "Gradient dL/dX: torch.Size([2, 1000, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 40. LayerNorm + FFN + LayerNorm for large $W_2$  and $W_1$"
      ],
      "metadata": {
        "id": "F2cBwHXUFVbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient of self-attention module\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "def self_attention_with_grad(X, d_model=512):\n",
        "    \"\"\"\n",
        "    Compute self-attention and its gradient with respect to input X\n",
        "    Args:\n",
        "        X: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "        d_model: Dimension of the model\n",
        "    Returns:\n",
        "        Y: Output tensor\n",
        "        dY_dX: Gradient of Y with respect to X\n",
        "    \"\"\"\n",
        "    # Ensure X requires gradient and create a fresh copy\n",
        "    X = X.clone().detach().requires_grad_(True)\n",
        "\n",
        "    batch_size, seq_len, _ = X.shape\n",
        "\n",
        "    ss = 1000000.0\n",
        "    ss1 = 100000.0\n",
        "    ss2 = 1.0\n",
        "\n",
        "    LN = nn.LayerNorm([d_model])\n",
        "    LN1 = nn.LayerNorm([d_model])\n",
        "\n",
        "    X1 = LN(X)\n",
        "\n",
        "\n",
        "    # Initialize learnable matrices\n",
        "    W1 = torch.randn(d_model, 4*d_model, device=X.device)*math.sqrt(2./(d_model + 4*d_model))*ss\n",
        "    W2 = torch.randn(d_model*4, d_model, device=X.device)*math.sqrt(2./(d_model + 4*d_model))*ss1\n",
        "\n",
        "    Y = torch.matmul(F.relu(torch.matmul(X1, W1)), W2)\n",
        "\n",
        "    Y = LN1(X)\n",
        "\n",
        "    # Create a random gradient tensor to backpropagate\n",
        "    grad_output = torch.randn_like(Y)\n",
        "    print(f\"Input grad_output norm: {grad_output.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    # Compute gradient\n",
        "    Y.backward(grad_output)\n",
        "\n",
        "    # Get the gradient and make sure it exists\n",
        "    assert X.grad is not None, \"Gradient computation failed\"\n",
        "    grad = X.grad.clone()\n",
        "\n",
        "    return Y.detach(), grad\n",
        "\n",
        "def main():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Create sample input\n",
        "    batch_size = 2\n",
        "    seq_len = 1000\n",
        "    d_model = 768\n",
        "    X = torch.randn(batch_size, seq_len, d_model)*1.\n",
        "\n",
        "\n",
        "    # Compute self-attention and its gradient\n",
        "    Y, X_gradient = self_attention_with_grad(X, d_model=d_model)\n",
        "\n",
        "    # Print norms for verification\n",
        "    print(f\"Output Y=LN(FFN(LN(X))) norm: {Y.norm(dim=-1).mean().item():.4f}\")\n",
        "    print(f\"Gradient dL/dX norm: {X_gradient.norm(dim=-1).mean().item():.4f}\")\n",
        "\n",
        "    # Print shapes for verification\n",
        "    print(f\"\\nShapes:\")\n",
        "    print(f\"Input X: {X.shape}\")\n",
        "    print(f\"Output Y: {Y.shape}\")\n",
        "    print(f\"Gradient dL/dX: {X_gradient.shape}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "HrUPZexOFB2l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4652be7d-9aec-4085-8d3f-08c985f6eec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input grad_output norm: 27.7076\n",
            "Output Y=LN(FFN(LN(X))) norm: 27.7127\n",
            "Gradient dL/dX norm: 27.7139\n",
            "\n",
            "Shapes:\n",
            "Input X: torch.Size([2, 1000, 768])\n",
            "Output Y: torch.Size([2, 1000, 768])\n",
            "Gradient dL/dX: torch.Size([2, 1000, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 41. Y = W/|Wx| and compute the singular value of Y in pytorch"
      ],
      "metadata": {
        "id": "LyJEx-CVElIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write a code to implement Y = W/|Wx| and compute the singular value of Y in pytorch\n",
        "\n",
        "import torch\n",
        "\n",
        "# Assuming 'W' is a PyTorch tensor\n",
        "\n",
        "scale = 1000.0\n",
        "W = scale*torch.randn(1024, 1024)  # Example: 3x4 matrix\n",
        "x = torch.randn(1024, 1)\n",
        "x = x/torch.norm(x, dim=0, keepdim=True)\n",
        "\n",
        "Wx = torch.mm(W, x)\n",
        "Wx_norm = torch.norm(Wx, dim=0, keepdim=True)\n",
        "\n",
        "\n",
        "Y = W / Wx_norm\n",
        "\n",
        "# Compute the singular values of Y\n",
        "U, S, V = torch.linalg.svd(Y)\n",
        "\n",
        "print(\"Singular Values of Y:\\n\",S)\n"
      ],
      "metadata": {
        "id": "lUf-BdWJ9OJ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b3e729b-19a5-4b3e-a82e-eff2df9ddb8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Singular Values of Y:\n",
            " tensor([1.9903e+00, 1.9856e+00, 1.9742e+00,  ..., 4.5740e-03, 2.5592e-03,\n",
            "        7.7754e-04])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Others***"
      ],
      "metadata": {
        "id": "OBGBT9m2Euob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# verify the following equation\n",
        "# $\\text{d}\\boldsymbol{A^{-1}} = \\boldsymbol{A^{-1}} (\\text{d} \\boldsymbol{A})  \\boldsymbol{A^{-1}}$"
      ],
      "metadata": {
        "id": "0v2wnIJVINN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "A = torch.tensor([[1.87, 2.26],[3.7, 4.12]])\n",
        "dA = torch.randn(2, 2)*0.0001\n",
        "\n",
        "B = A + dA\n",
        "dA_inv = torch.inverse(B) - torch.inverse(A)\n",
        "\n",
        "\n",
        "dA_inv_approx = torch.inverse(A).mm(dA).mm(torch.inverse(A))\n",
        "print(dA_inv)\n",
        "print(dA_inv_approx)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXSv1wm_GVMO",
        "outputId": "0162218a-ff44-4748-ec2c-b49f053d16de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0092, -0.0050],\n",
            "        [-0.0080,  0.0044]])\n",
            "tensor([[-0.0092,  0.0050],\n",
            "        [ 0.0080, -0.0044]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# verify the following equation\n",
        "# $\\text{d}(\\text{ln} (\\text{det}(A))) = \\text{Tr}(A^{-1}) \\text{d}(A)$"
      ],
      "metadata": {
        "id": "X78qMj8uBBGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "A = torch.tensor([[1.87, 2.26],[2.26, 4.12]], requires_grad=True)\n",
        "dA = torch.randn(2, 2)*0.0001\n",
        "\n",
        "# Calculate det(A)\n",
        "det_A = torch.det(A)\n",
        "\n",
        "# Calculate ln(det(A))\n",
        "ln_det_A = torch.log(det_A)\n",
        "ln_det_A_plus = torch.log(torch.det(A + dA))\n",
        "\n",
        "d_ln_det_A = ln_det_A_plus - ln_det_A\n",
        "\n",
        "\n",
        "# Calculate the trace of the inverse of A multiplied by dA\n",
        "A_inv = torch.inverse(A)\n",
        "tr_A_inv_dA = torch.trace(torch.matmul(A_inv, dA))\n",
        "\n",
        "\n",
        "#Compare the results\n",
        "print(\"d(ln(det(A))): \", d_ln_det_A)\n",
        "print(\"Tr(A^{-1}dA): \", tr_A_inv_dA)\n",
        "\n",
        "#Calculate the difference between ln(det(A)) and Tr(A^{-1}dA)\n",
        "difference = tr_A_inv_dA - d_ln_det_A\n",
        "print(\"Difference: \", difference)"
      ],
      "metadata": {
        "id": "J5RuqUgIGXZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# verify the following equation\n",
        "# $d(X\\cdot Y) = X\\cdot dY + dX\\cdot Y$"
      ],
      "metadata": {
        "id": "2rnvPrMMDiyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "# Define two tensors X and Y\n",
        "X = torch.randn(3, 3, requires_grad=True)\n",
        "Y = torch.randn(3, 3, requires_grad=True)\n",
        "\n",
        "dX = torch.randn(3, 3)*0.000001\n",
        "dY = torch.randn(3, 3)*0.00001\n",
        "\n",
        "# Calculate the product X * Y\n",
        "Z = torch.matmul(X, Y)\n",
        "\n",
        "Z_plus = torch.matmul(X + dX, Y + dY)\n",
        "\n",
        "# Calculate the differential of Z\n",
        "dZ = Z_plus - Z\n",
        "\n",
        "\n",
        "# Calculate X * dY + dX * Y\n",
        "LHS = torch.matmul(X, dY) + torch.matmul(dX, Y)\n",
        "\n",
        "# Calculate the difference between the two sides of the equation\n",
        "difference = LHS - dZ\n",
        "\n",
        "# Print the results\n",
        "print(\"Difference between LHS and dZ:\", difference)\n",
        "\n",
        "# The difference should be very close to zero, validating the equation"
      ],
      "metadata": {
        "id": "feBVQCrgBlnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# verify the following equation\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "CCEOWqtwmpZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "\n",
        "def gradient_ln_det(X):\n",
        "  \"\"\"\n",
        "  Computes the gradient of ln(det(X)) with respect to X.\n",
        "\n",
        "  Args:\n",
        "    X: A PyTorch tensor of shape (..., n, n) representing a batch of square matrices.\n",
        "\n",
        "  Returns:\n",
        "    A PyTorch tensor of the same shape as X representing the gradient.\n",
        "  \"\"\"\n",
        "  # Ensure X is a square matrix\n",
        "  if X.shape[-1] != X.shape[-2]:\n",
        "    raise ValueError(\"Input tensor X must be a square matrix.\")\n",
        "\n",
        "  try:\n",
        "    # Compute the inverse of X\n",
        "    X_inv = torch.linalg.inv(X)\n",
        "  except RuntimeError:\n",
        "    # Handle singular matrix cases\n",
        "    print(\"Encountered singular matrix, returning zeros for gradient.\")\n",
        "    return torch.zeros_like(X)\n",
        "\n",
        "  # Compute the gradient d(ln(det(X))) / dX = X^(-T)\n",
        "  gradient = X_inv.transpose(-1,-2)\n",
        "\n",
        "  return gradient\n",
        "\n",
        "X = torch.randn(3, 3)\n",
        "\n",
        "dX = torch.randn(3, 3)*0.0001\n",
        "\n",
        "X_new = X + dX\n",
        "\n",
        "gradient = ( torch.log(torch.linalg.det(X_new)) - torch.log(torch.linalg.det(X)) )/dX\n",
        "print(gradient)\n",
        "\n",
        "print(gradient_ln_det(X))"
      ],
      "metadata": {
        "id": "SAxm0EAcDzzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add a perbuation"
      ],
      "metadata": {
        "id": "p0-Jkju-kEae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "rows = 1024\n",
        "cols = 1024\n",
        "\n",
        "# Create a matrix with elements from a standard normal distribution N(0, 1)\n",
        "X = torch.randn(rows, cols)/torch.sqrt(torch.tensor(rows+cols))\n",
        "\n",
        "# Add a small perturbation\n",
        "dX = torch.ones(rows, cols) * 0.01\n",
        "\n",
        "u,s,v = torch.svd(X)\n",
        "\n",
        "Y = X + dX\n",
        "u1,s1,v1 = torch.svd(Y)\n",
        "\n",
        "print(s[0:10],s1[0:10])"
      ],
      "metadata": {
        "id": "Rv4Byjjwndex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Study $\\boldsymbol{X}$ and $\\boldsymbol{X} \\boldsymbol{X}^{\\top}$"
      ],
      "metadata": {
        "id": "40a_Dsc5syTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "X = torch.randn(64, 64)\n",
        "X_2 = torch.mm(X, X.T)\n",
        "u,s,v = torch.svd(X)\n",
        "u1,s1,v1 = torch.svd(X_2)\n",
        "print(s[0:10],s1[0:10])\n",
        "\n",
        "\n",
        "\n",
        "X = torch.randn(256, 256)\n",
        "X_2 = torch.mm(X, X.T)\n",
        "u,s,v = torch.svd(X)\n",
        "u1,s1,v1 = torch.svd(X_2)\n",
        "print(s[0:10],s1[0:10])\n",
        "\n",
        "\n",
        "X = torch.randn(1024, 1024)\n",
        "X_2 = torch.mm(X, X.T)\n",
        "u,s,v = torch.svd(X)\n",
        "u1,s1,v1 = torch.svd(X_2)\n",
        "print(s[0:10],s1[0:10])"
      ],
      "metadata": {
        "id": "fYskfAqlrIB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#\n",
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAvAAAAC0CAYAAADy8BgpAAAAAXNSR0IArs4c6QAAAJZlWElmTU0AKgAAAAgABAEaAAUAAAABAAAAPgEbAAUAAAABAAAARgEoAAMAAAABAAIAAIdpAAQAAAABAAAATgAAAAAAAACQAAAAAQAAAJAAAAABAASShgAHAAAAEgAAAISgAQADAAAAAQABAACgAgAEAAAAAQAAAvCgAwAEAAAAAQAAALQAAAAAQVNDSUkAAABTY3JlZW5zaG90R/Mn5wAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAAdZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IlhNUCBDb3JlIDYuMC4wIj4KICAgPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4KICAgICAgPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIKICAgICAgICAgICAgeG1sbnM6ZXhpZj0iaHR0cDovL25zLmFkb2JlLmNvbS9leGlmLzEuMC8iPgogICAgICAgICA8ZXhpZjpQaXhlbFlEaW1lbnNpb24+MTgwPC9leGlmOlBpeGVsWURpbWVuc2lvbj4KICAgICAgICAgPGV4aWY6UGl4ZWxYRGltZW5zaW9uPjc1MjwvZXhpZjpQaXhlbFhEaW1lbnNpb24+CiAgICAgICAgIDxleGlmOlVzZXJDb21tZW50PlNjcmVlbnNob3Q8L2V4aWY6VXNlckNvbW1lbnQ+CiAgICAgIDwvcmRmOkRlc2NyaXB0aW9uPgogICA8L3JkZjpSREY+CjwveDp4bXBtZXRhPgoC03SmAAAAHGlET1QAAAACAAAAAAAAAFoAAAAoAAAAWgAAAFoAACnQFHzFeAAAKZxJREFUeAHsnWuwHEUVxzskJpjEQBINlPLR735AeVOiAUtBEoFADFhllVUKCiivUBgNouKjUHmEKqsUH1QE1KCBJIhEEy0lIWD87ocQooDyhvAUEFjnN3qGs31nZmdmZ3fv5v676t6Znenp6fl19+nTp8/0TOskISiIgAiIgAiIgAiIgAiIgAiMBYFpUuDHopyUSREQAREQAREQAREQARFICUiBV0UQAREQAREQAREQAREQgTEiIAV+jApLWRUBERABERABERABERABKfCqAyIgAiIgAiIgAiIgAiIwRgSkwI9RYSmrIiACIiACIiACIiACIiAFXnVABERABERABERABERABMaIgBT4MSosZVUEREAEREAEREAEREAEpMCrDoiACIiACIiACIiACIjAGBGQAj9GhaWsioAIiIAIiIAIiIAIiIAUeNUBERABERABERABERABERgjAlLgx6iwlFUREAEREAEREAEREAERkAKvOiACIiACIiACIiACIiACY0RACvwYFZayKgIiIAIiIAIiIAIiIAJS4FUHREAEREAEREAEREAERGCMCEiBH6PCUlZFQAREQAREQAREQAREQAq86oAIiIAIiIAIiIAIiIAIjBEBKfBjVFjKqgiIgAiIgAiIgAiIgAhIgVcdEAEREAEREAEREAEREIExIiAFfowKS1kVAREQAREQAREQAREQASnwqgMiIAIiIAIiIAIiIAIiMEYEpMCPUWEpqyIgAiIgAiIgAiIgAiIgBV51QAREQAREQAREQAREQATGiIAU+DEqLGVVBERABERABERABERABKTAqw6IgAiIgAiIgAiIgAiIwBgRkAI/RoWlrIqACIiACIiACIiACIiAFPgxqwOvvfZaePXVV8O///3vNOfTp08Pb3vb2wLbquE///lPeOONN9K/119/PcyYMSPsv//+VS9XPBEQAREQAREQAREQgRESkAI/Qvh1bo3C/eyzz4ZvfOMb4b777gvbtm1LL7/44ovT7apVq8LcuXPDzJkzC5N9+eWXw+bNm8NVV10VOp1O2LFjRzjmmGPCUUcdFVDkv/rVr4Y5c+YUXq8TIiACIiACIiACIiACoycgBX4IZfDkk08GrN78oSBj7a6jKGN137RpU7jmmmvC3XffXZhjlPkvfelLYf78+RPioPyfdNJJYfv27ek5FHcGBYcddljYuXNnenzlypXhy1/+cpg3b96E63VABERABERABERABERgchCQAj/AcsDV5eGHHw7XXXddqiRj8T766KNTxfmCCy4IS5cuDbNmzeqZg0svvTR85zvfSeNddNFF4dhjj03TuOeee8L3vve9rutJH2X/wAMPDNOmTUvP8RurO1Z7zqPof+hDH0rvvd9++wUGCL/73e/CySefHC677LJUia8zwOjKgH6IgAiIgAiIgAiIgAgMlIAU+AHhxdq+bt268IlPfCK7wxFHHBHuvffe7Pctt9wSTj/99NQHPTsY7Tz//POp28yf//zngIX8lFNO6YrxwgsvpFb3NWvWZMePPPLIcOedd6ZK/N69e1PFHOUdxX316tXhgAMOyOL6ndtvvz1N/1e/+lW6RblXEAEREAEREAEREAERmGQEEl9ohQEQuOmmmzpJUWd/5513XidxY+k8+uijnXPOOSc7vmfPntK7Jxb3NG6ivBfGS15o7Vg8u2diae/s3r27w3Uc43yi7BemwYnEpSaNlww0OsnAoTSuToqACIiACIiACIiACIyGgCzwAxhQJUp5annHxYVw+OGHp77rSRGHRx55JPVlx62GcP7556duMG95y1vS3/4fK82ccMIJAYv617/+9dKVYnhBdfHixcHuSTq8nMpvLPf4xhdZ3v09yd873/nOgDUeFx8FERABERABERABERCByUVACnzL5fHUU0+lyrYp6CR/2223hY997GOp8o5y7AMvkW7cuDEcdNBB/nC6//TTT4eFCxeGSy65JPOBnxDJHcDd5iMf+Uj2oqqd4iVa0qkScMlhWUrcaE477bQqlyiOCIiACIiACIiACIjAEAlIgW8Z9q233hrOOOOMLFWs71u2bEmXeFy7dm345Cc/mZ2zHZT+BQsW2M9sy8oxvIy6fv36Cb7vWaRo59e//nVYtmxZ19E61nR89z/wgQ+kq9Pw4itrxCuIgAiIgAiIgAiIgAhMHgJS4FssixdffDF1Y2Gddgso1Keeemr6M/FJD+9+97vtVLq1VWPyln587rnnUreXDRs2hCVLlnRdV/SDpSFZ4eb666/PouBKc9ddd6WW9exgwQ4r53zwgx8MvHCLAq8XWQtA6bAIiIAIiIAIiIAIjIiAFPgWwd98880TVp35zW9+k1nXcWW58sor02Ul7bY/+MEPwqc+9alcSzcDApZ7RJmOl4u06+OtucDEx1Hi77jjjtw14n1cu14r0Xgq2hcBERABERABERCByUNACnxLZYHrCeuze+s77jN+2UhuhRLPmuscx7rNl1WL1lwnzeOOOy7N4R/+8Ieea8a/8sor6cuqKPu8HPvXv/41/dpqmkDyjzXkeRl29uzZdmjC9otf/GL49re/HR566KFwyCGHTDivAyIgAiIgAiIgAiIgAqMlIAW+Jf6PP/74hBdR8YeP/dG5HavR8FfFPQUL/kc/+tGeq8KgvK9atSpcffXVga+s8vEmAh9nYg14C6wFf8UVV6Q++XbMtvYMrFrzta99rXTVG7tGWxEQAREQAREQAREQgeESkALfEu/YfYZkH3vssbBo0aK+7sCLrCjTfMUVy/3cuXMnpIerzeWXX54q75z0PvMsC8lLtV6Jv/DCC7NlJadPnx7we8dHHp93lp2ss2rNhMzogAiIgAiIgAiIgAiIwEAJSIFvAS8vmx5//PFh586dWWr4rf/xj39sxYpN+ieeeGKaNr7sKOusDrN169ZU+cZlZvv27anlHes5Vvdp06al8bH0P/PMM+Fb3/pW+O53v5vljx3S4jyBAQIBCz0uNFp9JsWhfyIgAiIgAiIgAiIw6QhIgW+hSJ544okJlvZbbrklrFixooXU/5fE3r17wze/+c3C9eBRvFevXl34sSbWiMd1BhebvIDbDT7yJ510Upg5c2ZeFB0TAREQAREQAREQARGYBASkwLdQCOvWrQvLly/vSskvH9l1oo8ffJkVdxms6SwXiZWcr7Sy7CMvwuZ9zdXf7vXXX0+v54VYXrbF+s6HpLDW8xVXPuBklnt/nfZFQAREQAREQAREQAQmDwEp8H2WxUsvvZQq0H71GZIc5CouKN6vvfZaqmw3dXUhDXOfqfIybZ+YdLkIiIAIiIAIiIAIiEBLBKTA9wkS/3JWieHlTwtt+r9bmtqKgAiIgAiIgAiIgAiIAASkwPdZD9avXx9OO+20rlRYg33NmjVdx/RDBERABERABERABERABNogIAW+D4p8aIkvlp555pldqeATf/rpp3cd0w8REAEREAEREAEREAERaIOAFPg+KL7wwgvhhBNOmPC1VT6I9I53vKOPlHWpCIiACIiACIiACIiACOQTkAKfz6XS0aeffjosWbIkXYPdLmBtdT6KxIouCiIgAiIgAiIgAiIgAiLQNgEp8H0QZanIZcuWdaVw+OGHB5ZpnD17dtdx/RABERABERABERABERCBNghIgW9IEf/3W2+9NZx11lldKbT9AaeuxPVDBERABERABERABERgyhOQAt+wChT5v/NSa7wqTcNb6DIREAEREAEREAERmHIE+PAk37vhezV8q4YPVepDk93VQAp8N4/Kv/B/X7hw4YT4Tz31VFiwYMGE4zogAiIgAiIgAiIgAiJQTIAvzvMe4fbt28P06dOzdwwvvPDCVIn/8Ic/HGbOnFmcwBQ6IwW+YWHn+b8feeSRYcuWLfJ/b8hUl4mACIiACIiACExNAs8991y44oorwjXXXJMCQKfasWNHF4yLLroofOUrXwnz5s3rOj4Vf0iBb1jqP//5zyes/84LrDZqbJisLhMBERABERABERCBKUUAt2Ss6+hQF198cVi1alVqgZ8xY0ZYvXp1ptQDhfOXXnppWLRo0ZRiFD+sFPiYSIXfL774Yjj++OMnrP/+i1/8IixfvrxCCooiAiIgAiIgAiIgAiLwxhtvhJUrV4arr746fOELX0iV9djf/bbbbgunnnpqBuuSSy5JLfFz587Njk21HSnwDUr8mWeeCSeffHLmm2VJsCpNvKykndNWBERABERABERg+AReffXVgG8127vvvju17GKEmzNnzvAzoztOIIBRFOv7tm3bAm4zl112WfqNHR+RF1pR8q+99trsMO8izp8/P/s91XakwDco8fXr1+euNPPkk0/mvtja4Ba6RAREQAREQAREoA8Czz//fPjtb38b7rnnnrBz5850a8k98sgj4eCDD7af2o6QAIt/vP3tb89ygHvM5ZdfPmGAtXHjxrB06dIsHu8ieqt8dmKK7EiBb1DQuMqsWLFiwpW8gKEvsE7AogOOAIJq69at4bjjjhu5/x7WqA0bNqTLdLH0KW/8K4iACIjAuBN45ZVXwp49e8IPf/jDLt9p/1yPPvpoOOigg/yhnvssaYji/6c//SmceOKJ4YADDuh5jSL0JhCv6nfssceG3//+92HWrFldFz/77LPhwAMPzI7hC3/VVVely0xmB/+/Q9zNmzeHxYsX77uG1aRCKtQg8NJLL3WOOOKITlJHuv449vLLL9dISVGnGoFE8Gd1Z+3atSN//KQDy+pw8gGyTvIS0cjzpAyIgAiIQD8EEqt7J1mpJJNtcV/N76OPPrqTKI21b7Nu3bosXWSmQjsEEvemTuLTnrFlPzEwTUh87969WRzKkXjJRzUnxOPA5z73uTRu4pLT2b17d26ccT+I9U2hBoHE/71z1FFHdVUiKlKyAk0neRGjRkqKOtkJJD53HQTG448/nv4lI/pO8nGJRtl+7LHHMuX9vPPO6ySzNY3Safsi39HRISUzBG3fou/01K76RqgERGBSEcAQ9sQTT3SQqW22b9JK3kWb0D/HSvwFF1zQIQ91QvKRxizdm266SQa7OvAqxKUuJC+qdjAsFZVN4r6clQFlSpkUBfru888/P42PzvbPf/6z1bpWdN9hHpcCX5O2t1p6oZAsK1kzJUWfzARQZGn8frDG/s0331xb+UZ5Z4BnA70mlp9BsWJAQmdmdRklvsiiMag80JHzR9tCIMM4cVPrfP7zn0/L4Gc/+1njgdOg8qx0RUAE6hPAqko7P/fcc1OZw8w1BpK2AkobFleTZ3lb7l3XUOEHBcilIgWzredQOvkE/CCKsu1Vd5hVtr43Wd0mrXv5KY/nUSnwNcsNxSJPKNDAFfYNAgiF5KtvXeV82GGHZb+xoGMtqBISP8ysQ0GQ1O04qtyj3zhMX5ql4phjjumQ52EFBjN277x2xTHOMxuiIAIiMN4E/Eyktfdf/vKXrT2Ud3Gx9JEfKN233357Z9euXZVlt2XqgQceyAw5yPCqst+u17YdAgzOvEEN95kqbp/0uVYXklVsJmUf3JSQFPga5FAiipQNLIgK408gtuCgrP/973/vnH322ZkQQBhwrFdAOaXjMOFB5zJZA3m1fCIkh1Wfk+XDOu9973uze1se/LbNDn6y8le+RGAqEGCm2rdt9tsaoCNL/PtppIvLaz/hX//6V9cMZbLqST/J6dqGBHCN8u6eGJoYDFYN/lr6ZOrKvhCkwNcoRUZ7edNzvBCD4qcw3gRwJ8GFxHcwWKPjY5zHV69X8NOuWG4mu9Dwsw5MNw4jvwhmFHR8Spnd8jMdVg7DGkz0Kk+dFwER6I9AngGMPrUNt0LkhMkM5EkbwQ842hpotJGvqZYG/a2VLdsy3/c8Niww4o1puI3uC0EKfI1STNZ576pEVqEQQLghKIw3gfgFZaw5lLmftrMy7+UyxUuq5nvHNVUU/lHTY4DqLVgPPvjg0LOEv7sxZkvbqjJNOvSM6oYiIAK1CPBujZcvvp3jF99v8Mp23gomddPH9cLnt41BRt08KH4nnQ3GSEp9wfLOrEiTgGus1blk+cl9wpVGCnyNmuAtqlYR2DIyb7o6SY3bK+qACWAJjssVpZ7G7o+jmPeacfGdCfWjjQ5lwI+fJu+t8OwP098TFzU/6FHbGkaJ6x4iMBwCGLm8QuxlKtbRfgIvldrsOPdgxZF+Quwu22/++snLVL6WftbcXyjfflyi6IP9DNC+MKMiBb5G62B1DC90bB9lTWG8CSQf/pjQuaDQI8gpX6zw/CHIH3roodKHxVLjFdG6032liQ/4JGsoW71m+49//GPAd3wzee5tnbDlQf7vb/LRngiMMwHad95sJm2ddt+Pyx7vJJnMwBWPF0/7Cd5aS7p1/K37ua+ufZMAM6+mvGNE62U0e/PK4j1vjOMlWGbYxzlIga9YevhQFVkPxklBq/i4Uy4aAjvuXO6///6UA0o8woO/Kh/r8j7z1BkGB+MU8H+3zhBfwWG5sMSdJnmQ//s41RzlddgEcEuhfaKI4IaCIouBgbaE0sk+bcj/Das9xyywnppcydv2oyT71eH6ne2EKT70lkeMMW2uVR9z0e+JBBjMeeU9Xr2NfhiPCHSvOmVDut5INO4zK1LgJ9ad3CMob7GCZw183EdxuQ88xQ56pdvKtcl0HS4nvp6MowXZvwwGC74gO4wQuzAhaLHaKYiACLxJAIMCs3zMjjErTDsxH2GTXWVbvlA5ine2vAvqoYceminIlldcGpoEFHYzrsGi34Cy6HnKQNcv0XrXY/DCOk69wGKeNzPDIJTzTV5G9QYqBgnj3MdIga9Yt7BumKDxW5S1ca4AFR9/n4/mLTiUL1aXJuXqOynSGcfBnfcn5RkQknWsHE0qC0Lbux1ZGaCsKIiACHTSNsjgGkXXGwloK1X+uAZFFwW+Xyt1k/LwA/Tvf//7E/KMnGnygSRvXGNA02/wfQHMhvkeUL95H/freZeQARP1GeU9trzb823atCmNw5dZ64Z4pndYBqq6+awSXwp8FUpJHN+ovbBEII5CGFbMtqJVIIDQ8C+3UL78rvtiMvE3bNiQdUxYg5p0SBWyPPAoTC1aPcdKQSc5yMBgCV52T7bjOHsxSEZKe+oSwApp7+L4NsKglw8UYY3HKs+sof3RZlGA2KKE+r9hk8QAYAN0tn/7299yByEMUOoGU/jg0u9qX7hmvO9978vkEP27jAh1S6R5fOs/+f4KLlUYTn29ZZ96blZ06n7dYNZ7a0f09eNaxlLgK5R+noJnhd/GiL9CFhRlgATyVkdoso4wS0d6JZSOalyD+R9aPW9jmbcyFrFVhPtyTEEEpjoBFHA/oEbG8FE45M24BD+rh1LMgMQ/k8kZFLO6wRaXwFreb0A59O4zWqCiX6LVr9+4cWM2cLL60GvbxLDEYNIb7Khzo3ovpDqd/JhS4PO5dB2NX3zwlWoyf12z6yEm4Q8UZ1xMWPILywkrB7DGa96MBiNvrDMIa/zV2W86aqYzwdKCpYo8MNKPp6SZceGcxeEaXm4qC7t37+4SQG1ZkLFOk8eHH3447bhhlCe4yB8syQcDkD3JR6iauvBgufP1HB79BFxkYMl0JW0GVyPK0F7w9dPr3BclpaoLE2VDWvAhXeoRvOIZFAQ3eYARfCxeP881la6lvVEvaIvUBwZ1VcsITrRrZKmV+ajZ8TxWJ6kPtBvqRt4zUcd4br6NQP3l2YfR6ZM/szbSLlB68/I3apa97o9ibPLEjCO8YGvHbItyj9ytGnzf3IbBhIULLC9s+/nyKn0Mcol6Q53BIEE9qhqon8j5Ojyqpt1WPNo0+cv7Y4CZd9yO+TwQN+6DfTkU7TdtC7hr+TSb9pP+GUaxLwW+AnU6LT8q9wWPoqBQjwCKJoocHVPMFQGOMm/WJYQgyl1sESYe1hGEQZ1Ax4sPKAoiAp8tf75M2bdztuV+5557bu7gwu4fK6FN/PMsLbYoCChKfm128mZ556NHpgzRSWBVyONJvuoOduJpRhSHumnwDJQPA448axt5/cxnPpPWBTj7MuB3r/vx7CjtsTA2Rj/60Y+yF6Do6Kkvfhkx4lGu8DGO5FlhIgHaI/XLXi6zsqIuUvd6BRQXBpSkQV1o+jGWXvepeh6ZTl5ipYHnoT55uc6+V6Lt2bl+kLNEKCe+3SAD40Fp1ecddTzv5mIvhcbWbuNa5wNyKMh2He243xAvZkA9aRK4jrKL5THHqqRp9ZP4yHnrD5vkpe1rkKUYPxiI0S6QobSb+K/oOPH44raf1aWvtHKsuiX9uv2/sYjvRz8yjkEKfIVS88LHVy6Ef54ltEKSUzYKihKKlBds7NMYPVsEA6Ni4tpxGj7+iT4u5+sEFGJLr+4WpbJMYNhUrqVLB9U0UK98502aKBb+2TnG8yPsvYJhQtXyAbe6AgrLik+TQQSCu06wTszywZa8oIj48j/nnHO6/E6J18viT15iPijnpO/vh5JFPaKzseM8Fx2IZ9lG51+HzTjFRUnCJ9X4ocR7ztTLXoosHb5dz5ZyyZtpGwYXFAdvEKBd+7pA/qhLGBmI65V8nt3HpS71086LnhdF3dfvn/zkJ0VRx+K4V4xNgaUv8DLG6gfPXTX4dt1vX4zBwDOn3JtYeFnK09cvZCdfELXnoy31ssT75+K6yTDTjzGNGao8g4k9W50tZW+z2vSrGO5wo6n618T/3eqVnxEiz3XqnKUxGbZS4CuUglcifQWlEytT6CokPaWi0Fi9Ak2jYYoYhghzFC3P9z3veU/2GyFIPBreZz/72ew4nWkd5RRFA0s6CgT3Zxtbf0nT4nDe/sos6jybV2qaCn8qBNYW35HQGdCR0MGguJI3z8l4cH8ErM1wnH322Vk8Oo26Vhye29+njqKC1dLzIB2eg44LVxbygtLs0/f7WGuLAh2J50OHghUYPtSj2JfSK58oZja1HfNpczUCymBUf23KJJvZoWwoz5UrV6btMC47OuOiAIe4LtUdeBelXfc47d8UcuoQv5ltou5Qv5mds3rIeVNWyD9xqb8MRnzd5nfbwVuWuTf5G9fAQM0GPcha73q0a9eujLdxZ1tFcaZemeymPPoNlK0vV/KMrKgTKDeTTbQV+jfyacfsGf0MT5w+Axx7Lotf1vfE1w/iN4OjuA1b3ppuGdTBZhSB+mVygPwjv0ZlUOjn+aXA96CX1/iswlKhUUYUqhFg2tzY4VuY1ynlCQkaF5xRTLwyZmkhNKsG0rE/rFy+c7H0sKT7OFXSRrH2wp/9usKf+6CAmiUfK3Xel1Bj9xbLt63AQKfohRPnUZ7r5gfuljbbsk7HMyJefH+UnzjQtvLKm2vLOnCsUZavPCsM9SrOO/HxjyfAL+98vxY8ez6e/6c//Wl6D+4z7D+UUK8kWb6abGmnxtrPUvgy4DyKbpF7B+1zMigklK8p5ChUKFdx8M9rz40iZzxvvPHGjIedN5eQOK2mv+Ho20UVF6Wm9xrGdeTfWPFcvp5QBrFbFnGrMCVdkzOk22+gflh65IE66/NaJX2znFNnzK+afMb1v+z5kPnGiy15GmUdYEBh7cbny/Y9MztmWzhwnj/rH+mLKC/jU4Vr23HoCy0/5JX9ujPMbeepSXpS4HtQQ3D7graKybbXNH+PpKfUaQSQcUShKRp5x4oV15jl2H8u28oBJdemZJsAzStfU/TqpBcLfwRX0TOWpYsV2J7NFPI4fnwv4iMQbdCBz6SlYVssnnUV+Nj3vspMB2Vh5Wz3LuPpLY0Wn84ub3AHB9K3DqNoWhnu8Lf02PLbpq1jdw7Ok2ZbnWTRjJ3Pz6D36wxq4/plv7FIm+IRK0h23J6DQWcRPwY0Fo8trKnDwwzUCbOCUheKZEaeu6QpWzxf/Nw8T9v9APexOo48HPfgB0UxK9o5dcvXD/Y51kt++nTLZExVfvEMcFzne6WD4m1yh/bgQywT/WDYx2Pfz1LDgjQx7IwiIAPylHeOwYu+GWML8eJ8k/cbbrghPU+d5g9ZwDWjeh5jGPcRo5BJlpd+tlLge9CLG7UXNAgQhWoErHEjyIqsqzRqE4DG2QtRb8G383TKpphVy0l3LDpyS8u2TZQf0vF+3eQbhbpO4Dns+Xmuouvz6qRNsXJN3ixF3HFWyVeszPRKgxkSU5KMJQpPmbBG8HtuXFd0Hzp7m52AU5GrCPUr7jC9y0bs/sE9sZz1UhiqMCMOVkXyyXM0+atzLc8S//GsbViT/KpKsXtRzLdoMAUPGFh9YDsKhYQ2bfWsTG7HMws8p7VD2nhcvznfRF7ApSj4QTwKz7gHk2mUfd7z5Ml14iIbyoL3q8+b4Su7Nu8c7zv4ekpZ1wlmfUfmmdGJ62OXviIOxEXGxYNEnnMUAeNWXN/5nTdzZfmLjT4MQOsajiytQW5p055zv4bAQea1LG0p8GV0knOxEuMbeFlF7pHslDrt3UuKlDOAwNMLe1ib9YvzCBRvDUBR7Vdwx+Xb1BJLPnzd8AMP8l4l+DRQ0otCbOElz95/D0u554jPPOzqBm/h4tl6dSTc1zNg3wYWRffOG4zkdfJc7y2TZRa3uCMmH3wgxAL1zNcjXE56KQt27VTZokjYTE48CIvrBS/oFSmxDIp8R1mlHg2CsbUZ2kXRwI/72gDR6jF5twCTWKFBnrU18LP72IAHGdLGQMzSHcUW5c1kUZGVE9kUK37wp40WBcrQ0u1VpkVpxMfjgUSd2Q8UdssPirwPfqDBcxGvaAYK4wOcrP6xLRtw+vu0uY/rkLUZywv5RgaXBThYfLa4R01WPYn25fPary5RxmVQ56TA9yAbNz4rcCwv/Vh+e9x2nzqNEKfjozMs65BQAoyvbeP4CBCUN9xpiqbB68DLE1JNLAYonZZntk0UeJ6H1VFgVaYU5OXZPzPWBVgSD2W2ifJOeijf/pnIV1GIB1dcx7OUPQdpxRZPOi9vvfL34x50IrAtK6N4UEZbjQP1iHsXrfsdx59qv1FWGfgdeuihaT3yzx8r5PwuKjOOTwaFBGWQ+lg2U+CVQqv3sasD8ghXCY6j8PVSaDy3qvumWPCSNem3/Vc2gKmax6rxvPWZAWGRT3nsOgl/jhXllVkKm1Fh0NVGoF+xcmdbZmyK70e9wBDAKmnIXB+QP1XT9UYcu2YUCjD5ML6WjyKXTv+sDEBi+VA0uPfXjWI/HozH5TaKPNW9pxT4EmIoHzaqtkpsW4RskTAqSXLKnsKNoteAh07R+LKF/SADeYqFzac//elGt4ytvmXKbtENULwRgGUuJ3kv3aKoDyLUGZTESjPl14sBz2vKipU75VGm9MOmjA8c4kE3aSrUJ0B7pT768mCmIlbIy2ZD4jpEOY9CIUHBYgBYVnd4NquHti2ylNanWe2KvDZheWlru3bt2q4Zu2o5axbLD9DL6gkD6bznK7KKemXbz9I2y+X/rvJ5JS/x4K1X2tSt2OCUJxeLnon0bfbFWNAHlhkreuWpyXnau7kDWT6Q0wzqewXaUNynFs2o9kpr0OdjBT52Exz0/dtIXwp8CUUEfjx6tgpdZ3Recgud+j8BBkNxw2/LslIEGWEbl29doW1pt6HAW1pl2zwB6d1Dyq6tey5WvhDieQGBH7se0E56uc/k8e+3XWGxi+tR0zLNe9apfiweZNN+ymbCYkUAhaTIqjpqtrHyxLMNOyAH40Gt9TltbbEUD0MpRC5YW2RbpriSnzwrfJHM8fWqrQFhvwp8XFfyFGE4FM1WoSAbLyvrQfeBcZ75TXu2+9u2lyy3dChju8a242KBlwJvpbiPbPMqslXKUfil7SNYcx8jb7BUVWjkJljhYJ7FrWkjHpYCH7u1DFLJiO9V1JkwxR9bZWknWG/LQh3/97J0/Dk6c2ujth2Em4O/51TZR8mKZyRROIrCZFFIivIXH48HoUXKY3xd279NgWc2EIWo7b9e7bKt52H2wtog9QSFtizEg0O7Np4F8QN/6mPZrErZ/eJz3qrPvfs1JpDvWC6WzUJQLrFBqSx+nP+2fsflQJ7imYWie+W5wbY1wCq6Z9PjsQVeLjRNSU7S6/Kmv0yooPwptEcgfoEIzkwnDzLEL+Ph81f28mhZXmLLw6A6/2G6h8R8uHdewFLvvzRI2dFh95pyjS1edHZF1qm8++Ydi9Mc5AAn7/778jFkXqyQlBkyJotCUqVMUDLiwckolCfyau+40IbG2U3TK8S0y14hb/CNLIlfZvUD/zbdB2M3njovseY9W57+EA9G/HVxH8KzD1vPYJD14x//OBt4mSz3+Szbxz2La+yvjvJflu4gztlA2fIK/3EL08hw8gAKOQSSEXhYsWLFhDNJpQxbtmwJs2fPnnBOB5oRSAR8WL58eXZx0pmGHTt2ZL8HsZNYGsLHP/7xLGnK9a677grz5s3LjlXdSQRtWLJkSdi+fXt6SSIcwnXXXRemTZtWNYme8RL/9/D+978/3HvvvVlc6qjnlp1oYSdRzsKyZcuylIrulXR84ZBDDsnisZN0fuHaa6/tOuZ/IHaIc/3112eHE4UlbNu2LcyYMSM7VncnGWSEs846K7uMND2v7MQAd5LZpHDHHXeEmTNnDvAuxUkvXrw4zJo1K+y///7FkRqcietDosyHzZs3h7lz5+amlljjwqJFi7rOJbOaYcGCBV3HJsOPRHmckK/E6jgSGW+ckYGJEhje9a53TQZEtfKQzNaksuovf/lLoJ7QHubPn1+aRjLgD8mXksOaNWu64iUKfLjyyivDnDlz0uPJwCCcccYZ6X4ySxhOOeWUrvhNfySGiHDwwQdnl/eSYVnEgp1kcBHOPPPM7CzlSf+w3377Zcf8TtwH0h9t3bo1vPWtb/XRBrqfDLrD6tWr077LbpR8LC8sXbrUfhZuKb/EkBPuu+++LE7i6pQyaLMfzBLvc4fysLwmxruQuKKGhQsX9pnqkC8ftxHHsPKL9TCeYkmKJh1ZMnIbZ8vIsBjG98H3FXcGv+QhcbyvpDEuctcgPuwZLWPhidPifJWQ52tKufZ60bYo7Xi6lLrTywJdlBacsF7GMxDxPWC1adOmomTSl/awjvMXp1V4kTsRW7OLrGjxlCv5KrPMcgs/DW5l7qesscTjwpOXd1w5OB/zhVsilDPrD+mStyKrPvWO9LlPr+l9h6Xnbt4Hx+wZh7FltZWmrmBlDxfP/sC6LMT1Amucb19YPKlTw/SJp5zz7uetxZQReS0LtE/yPgirnW/n5GscA9Z0m63BXx1eVULsimjtha93ErzcJv2itl3lXnGc2P2D/qBpyPPpL5st4LmYcbHnZevvz3nY8NemrIqfD1dWKzfLS5XVZ0iH2QK7xra9+oH4/sP6Td/h+wpm36mz4xb0EmtBieUpGFYpvaJRcLkORwRQahBILLPFcmJ+KhH3mVh4oVQVBTpN0qE8EutAUbTS43TidNJWpmz7KdfYhx/hgBCvE4iPkEf5hwf58f6d/sUt8ougLQvm0wsrrziVXePPMYXs+RT5CMaKPteUvdjIPfw0uN0DZdqCKVSs9W95ZxCyZ8+etB6hpPJ8cLeQpzjfeeeddnrC1q9b36bw5jlYgpHyGcUfzDyXCQ/e4AAdHu3Xyoot9bQoFCkkNpBEvlobLlNSGaCTVr8BBZL8oggyEPEDHNKPn432V2QcgAXlC4Mbb7yx36xNuJ70ra2TD1iNW/Ayoax84+dCBlJOvp6xjxsNcsD3y7BpU5n1AyfuSZ1oWveob3H/UqbMxv0H94ehBT+4KBoM0VdQZ62N2bV1tjCI803/XCXEA3b6j7zBcpW0Bh0HVl6B55nblpmDfgbSlwJfQJkOPV4H1YSKF/4Fl+uwI4CVBGFr/NjSiZqgQVn359gvC+ZnR6NDsDUJsbDmnv2UK4LTK7xNBAKWjpiDV2pNIbc4CKAixdMrUCgDKAV1Q6zUPPDAA7lJ5L2/0OvF0diai6LrrWlWXxjsWSfNs9qz2/bBBx/M8mRKv52jDMqC1SM49spvWTp55+gMRvU3CL9ZOjxfv2HsFYyYQS+FhEG4lROW+DhwPYNZ6iAfg8Gf2A9m4/hlv6lXKIB2P7Z8OIh7EPLyyuD5/vvvz03WW4nLZgpzL654kHZv+S3jXDG5oUajvVr7pQ36dl0lI7t27cqe3RiwZfDlB/79GFzy8uEHB9wPudC0ziFPkGk+/2VGDa+g2zX+fSxTjuGalw79GQMf2ssNN9zQeGYIhdu3FXSgKuVHHMra8s62yOCTx37Yx+I2z3MWDYyGnbc69/svAAAA///reUpWAAAde0lEQVTtnefPFUUbhwcFG4ggYonG+OH9A9RYUQmW2BULGgQTExNj712xYu+KJVG/qFhRLFjAEkVBxfLJ+MWu2MWOCrbzzm/0Xufs2XOeU59zzu41yfPsnt2p1z07c+89szOuhMsk8OWXX5accxV/22yzTen777/PDMPFSgJ//fVXaebMmRUcH3roocTzvffeW3Z/6623Ln322WfJ/fjkq6++Km211VbBv+Jt1imeWL6S65IlS5qNLoQ79thjy+JspJ78+OOPpXHjxpWFV/4ef/zxELfui0ucZ6X3/vvvZ+ZZ183vxx9/nOmn1sU///wz4ax4lLeff/45M4i4WVp2VH6ruW+//bYk3uZXxy233LL0+++/hyC//fZbUlarB8uWLSvLj4V95JFHQpi///47xGHXdVQ9+eCDDzKz8cknnyRp3HPPPZl+uPgfgT/++KN03HHHlcls1qxZ/3lInb377rtlfiWP+Pm67777wn3J/bvvvisLLVmm2wSFVxjVy0bdgw8+WJEX1T/VM7lPP/204v7cuXMzk1F7Zs+5nse4TJkBmrwo3nG7OXv27CZj6lywH374ofTTTz9VJPDAAw8kPG+77baK+wNd+OWXX0onnHBCEoc907p25513Jtft2R8ovnrvp+u45Lt06dJ6g5f5E5d0e56u53GAWNYqr9IWB7nly5cnbVtWn6e2Nv1sKu0vvvgiTqLuc6vfxr1WW65I9Uwcf/zxiVwUTs9vM89q3Zls0aP6ZiufjnfddVeLMXYnuOtOsr2f6v33318mYBO2Hqxff/219wvQIzlUI6RO2vjZUXzlpBSKqV3XUb/VaKWdrlnjIj+NKMjpuNLylbI3UEOVjiP9e/HixWXl0EtCve7zzz8vC2scrIwPP/xwxX01mllODbcxFy9TjLP8VrumOh7LRXyyZKLwWR1utc5K8ra8xTKXcmZOL3e6p05ISoKcOMT5sbDGON0g677yLGUw7dS5HnPMMSEN+an2YpIOV/Tfd999d1kdlEKV5b755psKGUuWpuyqblkdyFJIJFO7b3LW8ZRTTinp5a9Rp845jkfnKovqs5SMLGVRZchycVy1XmCywjZ6Te1RnDe1Ab3i9Fyq/ZFcxcT6RB3jl/NqHAcqR7otTctPbUGzynWttPUyH6dVrR2rFYfu6eUwlp3i/PrrrzOD6QUyXd/F1hRg5cHylGWMESvJwfzYUenbS0BmwlUuqv+wOHSUMbOW03MQ+1efY/WhVrhu3kv3p3H/0818NZo2CnwVYuk3YqugqpxZSkGVaAp/WQ3Z0UcfXfaAS3lS5yTFMm05MM7phlMNURyPWWabBWwvApaeOvRmGrs4fXVWFp+OskTV69RonnTSSWXh1ajIuiFWUjQV54QJExI/Wcq5uB1++OHBz4knntj0S05aIRYf61CyyiSLdlx2dR5pp7xZOWK/OreXkdiSrs5U5ZeTwpCuK6pH6ihUx9LyVJzq5NMvZVLWjzjiiJBXdXr2ApDOK78rCaRfMiUPs2Kbb/HMkoXkYfXH2lYpLVn8syziVl8aeaYsT++8805Z3VS+rf2wl0WL3456FmOneii/piipjNVeaONwrZ6rTYlfXKUsNfNC3mo+0uHTBhDlcbPNNiuTfVb7lI6n2m8pu2oTTB7po+qOtQ3V4mjmerqOD6S81koj3V5ZGxeHkSKZ1SYqrDlrr1TmrJcWe57SjPTbXpotrnqO9nJm8SkvWX2jnn31MeZPR8k8K4/1pDuYfuJRIuW72kj2YOapmbRQ4DOoVRuul6DNcpwRjEtVCMhqYA/5LbfcEh4WXbMGThabt99+u0yBVYcgBU4drTqtuDGXDEwZqJJkzcuxQmz5akYxSCci5TBWwtWY1etU5+Iy3nrrraUPP/wwNJDWwKuxFKdtt9024SmGUizU0ccWH/GrZvGpJ09pxWYga6MaeJXXeOqoDkfD3PqL86ZyvPXWW2V+VUalefLJJ4frUpRs9MHyG08Luummm8L0GL3kWLoKIz6mZCkPilf1RR2x0jWWujdQmSxdjv8QyJKxXqqlhGiaylFHHZXIVDKxF0mrE+KtqTWmkErxyFLCZGW3MOljM9OdFJ+s94pr8803L91+++1hekGsfCjvcf71jKneaLRG01esjikOnQ/mqI2eg3T6et67pchLcYuf57SM7Hcr7Y9qXLVprIpfU2k64WRkiEcQspTuetNVXNbHGRPVKb2gKt7YICUl3UYFY79xn1BNyVScFiZ91AtJM06yi+uc2nI9r+onVffUz8V5U7r63euWd2Oh8hgr9Rc20mv3++WIAp8hqaz5aybsLItRRhRcigioszv77LOTB8ZY2lHWWynkarBjBdju21H31Pi1orwrW1LgYyVP8bdiabGiSgmPG2EpKrpWr5MVPlYwrdw6StmwTjtrfnDsVw1pM5aXOJ/pxllpD+Qk51rWIOVRnYIUEikfUr7jfMfnWYqaOsRzzjknM4zkaVZVHdPyjeMWS7HGNU5Az07cscdc7Vz31U5KztUUPSkstZRg1fGsZ0FKT6NOLwm16qXyq7yobGmFy8pkR92vle9G81avf/VJae7Ki7ULUrikPNmflGz9yRpq53ZsVfHXc2gvYcYlfdTz22o7nZ6THqfRDoNLFnvV2bgO6DzrJTMrbNa1rJHVuBw6V1sr+aoPMgNG2o+MINVGfCTPas9LK1NDpPzHLNJ5st/Kv/Leqryz+HXimp6RuP5Kma/GthPptzPOIYrMCwIXEfDKjxs7dmx05Z9TrxQ433C49ddfv+IeF2oT8A2Ue/LJJ93111+vl0a33XbbOf/QuGnTprm11lrLrbDCCiEC34A6/wGZe+2119yrr77qxNw3oOFv+vTpbtSoUbUTquOuV0bd2muvnfhUGnPmzHFrrrlmcq3ZE9/oldUP39m50aNH1x2d8jZv3jx38803uxVXXDGUf+ONN3a77bZbUnbxUB199tlnnbdEB3abbLJJ8H/qqaeG9FZbbbW600x7lFz8VJ3AX/d8A+2uuOIKt9JKK6W9Vvz2na5TmV955RV36aWXuqFDh7otttgiyE+ylvwsHt/xOMk7XQ5vLQ0MFTbt5P+pp55yM2bMKKtH5557bqhH5t9bXQPHN954w7388svON9jBv2+s3R577OFGjhxpXjk2SMB3gOFZfumll9zrr7/u/LSnEIPYnnbaaW7MmDFulVVWCdfsuR8yZIi79tprQ131VnB30UUXudVXX71myqrjSkttgbeQucMOO8x5RdxNnTq1Zrism6rTesYtz/Kz6aabujPOOCPkd9VVVw3BlN4TTzzhvDIS6pjyLX+6r3qpNiKrXmal2e5rKoOercsvvzy0o+n4VcfTTm2tyhC7I4880h1wwAGJjOJ79Zx7o0RgobYny/mXNnfhhRcOKN+ssOlr/qXK7bLLLm7RokXJLf8iExiYzJIbbTrxL0XuuuuuS2LzL2xuxIgRye9GTsTfG51CG6Q4JQv1dapTOj/99NPduuuum/R/1m7pudKf+iaVV/3VyiuvXDVptaV61p577rng55prrgnPjfQVybpZp/5o/vz5TvHfeOONybOu/kbt+FlnneXWWGONputSs/lqJZwYS+cw50cw3IEHHmg/++voKxguRSBr1QIv1WAR8o1oyjc/GyEgK5AsXRqOl4WlmpM/WZFl7Wr3nDpZbyRP+5NlS3lqh1OeYyuKLGSNOlkyxEfW+1oWfFmG5M+sh7V4NpIHTTkwNjpmWcMHik9z2WWpU94ky1pO5ZVVRP7kfyCnuOVf1jKF0e9qTv5kHVK8tVhWC8/16gRMbhqdkSz0u5oTe9UryaGWvKqFtyFvfYvRilP7rfbE6lu1uGTVlD/ltx2jc9XSaea61WlZR6uNcMTPb/pc4Vp9FtRHeuWyrJ1QOmrvWp06k2aiqYQajdHoi6aL1NNGpONo5LfkHTNrl/xVp9Req17J4l7Nsq/nQ+2a/prp+1THbfSq2rSbRnjIr/VJYiH59rMeFE85Eif12f3qsMD7JzXt/LCTO+igg9KXne883JQpUyquc6F3CchyIKuwrBdmOZNF+YYbbkgy3c43cN8QBIu1LMRyvrN0sobImt4vTjwmT54csisLkF/KsqFRhH4pJ/nsDwJ6fjViJyusnyqQ1M3+yH1nc+kVcac/tXNeyXILFy4Mv4cNGxasuhqt03nsdtppp2Q0L77e6LlGwzRaKKfRUq94uvPOOy+MaDQa10D+/UtLKI+N7Azkv5X7YqmRTOsj1IZr5LhfXDzC7BVut8466/RL1jueT//iUTa6rBG9/fffv69GEGJIKPAxDX+uRkhDgKaAxbf9R1hu0qRJ8SXOe5SA5KjGS1M/NATvrTdu4sSJQZEfP358Mj2kEwpq3IAKT6PTaLqJ1Ft93A477JDwQWHqpjRIWwT8fPowzUDnfk5xcq7fuO4TkMKr9lYvCv1kqKhFLj0VUtNTBpryVSu+wbwnPUVTQrx1OUxH1RQX3D8EHn30UbfPPvskOPxoQuZ06cRDr5/069BBp/Kt4cn4Awcvv2Q4rZ+HWjrFq1fjjYfJTIb6WFbTOuKhX33c1q6pJ8ZCw4vxB2f6yKdfXLz6jJ6Ddg0f90v5yWfvEbBnWdNoNK0AB4FOE9BUl3h6UjNTITudx6z4NdXFppupb8P9R0BTloyNdAL10f08FUglwwKfesPSRzO77757+OgkvqUPhJ555hk3fPjw+DLnPUhAVuQdd9wxfEgZZ88vdxd+7rvvvsnlTg0xpq3w+giv1y0h6eHFVj+ASiBzAoEmCeiDsz333DOMCLVzqluT2SFYgQikrfB+PnrP9//x9Me+ty63ua5pJFwf2Mtp5L1dC1e0OZuNRfff+wlnIhCvWe5JJtZ3vbm121IL8c4Q0AepsZXd5Kil7fQBl/3u5Bu4LCFxWjrvdRd/vK2Pe9r9MVqvl5/89RYBWdttJEujQdTH3pJP3nOjD32t/qnPkEW+l536PbMwK98aRcD9Q0C6WyzLvIxOsA58qoZrxQ1T8OKj1lnF9QcBfbmfXk9eq1doExmTqRQCrYrRSacGNR6G7WUFRKsi2MoFYvTee+91Eg1xQ6AmAXW48T4BzayEVDMBbkKgDgLqI+K+RFMwe9GprzEFVX3bRx991IvZ7FqetBmb9f3q57TGfR4cCnxKinozM0HHR80NxvUPgXgXSO2+GO96J7kO1kY+svpbw6rNa6Qo95pLjxZ04ruAXisz+eltAlqW0tpffUPC3Pfelleec6eN4cyy7Ven6UnLdvyyq29GcP8RiI1T2mW3XUtr/pdC985Q4CP28Qcg1nnoKKH36pt3lH1OIwJSAKSIyhoRy1LKgJTqwXSLFy9OpvRIme/0OsaNli2eOqP86UNuHAS6SUAKu1549eIt6yIOAt0iIL1AFm3rR9SH+JV3upWdzHQ1UuA36CpppFn5xf1DID0Nyq/Qkys0fMTqn0pz1T5+9Epg2OGsUzu/Wfoc20tA8tRaxVonWbtx7rzzzmFHvVZ2KW0mh0rfW3Hc//73vxBcy5ReeeWVybr0zcTZrjB+i25nH/V65T3soNiO3W7blT/iKS4BPTfatdJ27i0uCUrebQL+JTLs0Gu7APfiB9Va81/PS3rn3W6z61b6WpRBOy3brrpalEFrvtuu793KVzvTRYGPaOohzVJe7rrrrrB9Nw9GBIvThgiocVWjbx2At3qHxqShSNrsWS83WuFDLzfarETbn/f6SjltRkB0EIAABOoioPXutcb6wQcfHPxrf4K11167rrB4GnwC6mMPOOCAkLD6XinvedmnwGiiwBsJf4ytkdHloHhpYwQcBFol4D+mCaM52ulvo402ajW6lsJrWbTTTz89bMBy2WWX9e1udC1BIDAEIACBOgnIECPFcMGCBW7atGnsclont254k5xefPHF0Mett956ubK8G08UeCPhj7aDWXQpnPKmnSbCbwhAAAIQgAAEIACBbhFAgY/IaxrBjBkzoivOaf773Llz3ciRI8uu8wMCEIAABCAAAQhAAALdIIAC/y91v+6wGz9+fNjxLxaE/+LcXXzxxT2/A1ucZ84hAAEIQAACEIAABPJLAAX+X9n6pf3crrvuGj7oi8XtN3BykydPji9xDgEIQAACEIAABCAAga4RQIH/F/2SJUvc2LFjKwTB/PcKJFyAAAQgAAEIQAACEOgiART4f+HHSw6ZPMaNG+fmzZsX1g63axwhAAEIQAACEIAABCDQTQIo8P/S9zuYJeu7mkC0sc21116bu7VDrXwcIQABCEAAAhCAAAT6jwAKvJdZtQ9Ye3G3tf6rYuQYAhCAAAQgAAEIQKCdBFDgPc1qH7B+/PHHbsMNN2wnb+KCAAQgAAEIQAACEIBASwRQ4D2+rA9Ytf77s88+61ZbbbWWABMYAhCAAAQgAAEIQAAC7SSAAu9pZn3AOnPmTDd16tR2siYuCEAAAhCAAAQgAAEItEwABd4jzNqBddasWW7SpEktAyYCCEAAAhCAAAQgAAEItJNA4RX45cuXuwkTJpTtwLrNNtu4OXPmuNGjR7eTNXFBAAIQgAAEIAABCECgZQKFV+B//PFHN2rUqDKQW221lXvhhRfcyiuvXHadHxCAAAQgAAEIQAACEOg2gcIr8LNnz3b7779/mRzuueced9BBB5Vd4wcEIAABCEAAAhCAAAR6gUDhFfj77ruvQllfvHix22CDDXpBPgPmQWvYL1u2LKxl/8orr7htt902bDyl0YNhw4YNGF4eNI1Icfz+++9u/vz5bscddwyjD6zAUxc+PEEAAhCAAAQgAIFBJVBoBV4K6w477OAWLlyYQNfykU8//bQbMWJEcq1XTzT9Z/r06e61115zL730UpJNKfFbbrmlO+uss9zqq69eVZGX4q418K+44gp35ZVXJuF1ojhOOukkt9tuu7lVVlml7B4/IAABCEAAAhCAAAS6R6DQCvzSpUvdLrvs4l5++eVEAnfffbebMmVK8rtXT7777js3ceJEt2DBgppZlBIuJT9tTVfZzz//fHf11VeH8PpwV0r/0KFD3d9//x2YiMvJJ5/spk2bVvGdQM1EuQkBCEAAAhCAAAQg0DEChVbgH3nkEbfvvvuWwX3ooYfcfvvtV3at13789NNPbo899gjKuxT0M844I7GSa/OpdP5POeUUd9ppp7mxY8eGosjqLqVeVvdx48Y5jTqcc845bo011kiK+ttvvwXF/ZprrgmW+AsvvNANHz48uc8JBCAAAQhAAAIQgEB3CBRagU/Pf++H6TN//vlnUNhlOZdinp76omok6/zee+9dNjVIfi+44IJQy8477zx31VVXuYGWy5QlXoq/0vr++++xwnfnGSVVCEAAAhCAAAQgUEYgtwq8rMz6e/PNN91mm20W1nSP53Lr48/x48eXrf/eD6vPfPbZZ+EDW81Rf/7558OUlzKJ/vtjyZIlbp999ilT4mWtl1J+3XXXBeX98ccfH1Apt2U2FVYvCyussEJWclyDAAQgAAEIQAACEBgkArlU4KW4n3nmme6mm25KMGq3VX2saWu7y6K85pprJvdljX700UfdmDFjkmuNnPz6669u3rx5bsiQIY0Eq+lXq8HoI9TYPfzww2GKzA8//FA25SX2Y+eaahNPi7HrOqqsstLX4zQPftGiRe6ZZ55xq666aj1B8AMBCEAAAhCAAAQg0CkCpRy6WbNmlTyvsj8/Pab0ySefJKX102fK7h977LElvypNcr/RkzvuuKMsvnT6zfw++uijS95inmTlr7/+Kj344IMhHT9HPble68Qr6hX5Ov7440uKq1534oknhjj8C0G9QfAHAQhAAAIQgAAEINAhArm0wN9///1u8uTJFe88M2fOdFOnTg1rnm+//fbJ9BlZ32XZto88KwLWcUFTcmSFb6fTaEE87UfTX84+++ywS+xzzz1XsbJMVtpaKlPLSdpqM/Kj8j755JNu5MiRWUEqrsmvPpr94osv3LrrrltxnwsQgAAEIAABCEAAAoNHIJcK/Oeff+7WX3/9MopSWh977LEwbSa9+6oU+wMPPLDqeullEXX5h01n0WozsXJfK1vffvttmC4TL5epOe2XXHJJMqWoVnhNt9F8+nqm7dSKh3sQgAAEIAABCEAAAq0TyKUCn57fLqV3iy22cJMmTQo7lk6YMCGxvmvlGVmYR40a1TrNQYjBlr6sd1UYjQxomUktB5l2UuIvvfRSt9JKK6VvJb/9yE/Y1Ekfr86dO5elJBMynEAAAhCAAAQgAIHuEMilAq+lFqWczpgxI0wXOfTQQ93BBx8cVmCRMnv99dcntPth3fcks/7EVoWptoRk7FfKt6zntta9n6fvDjnkkNiL02iErOvVPr6V1X306NGB50DKflnE/IAABCAAAQhAAAIQ6AiBXCrwIvX111+HZQ+13rmcdhndfPPN3Y033hh+a0qN5sqvt956fbU0oua0ax68yvXLL79UnQfvP1INK/HYOvFS1Pfaay83Z86cio2e/IexQclPLxGpzZy0wZPmz2saTrxqT4DIPwhAAAIQgAAEIACBwSfQoY9jeyJav6FRSavLeKplf1qF5csvvyxb4aUnMlxnJr766qtQHj/KUPIfzlaE8op9yU8bSsqs8+XLlwd/8p+1So/i8i89Jb8EZ2np0qUl/8FqSdfETsdWVuipyCAXIAABCEAAAhCAAASaJpBbC7y9CslirWkgL7zwQti4acUVV3QjRozo6/XMvbTD1BgbXdB0GjnNd9d67fGKM7p38cUXl81z1xQjWeQ1xWjBggWGKhw1MrFw4cLkmsKff/75zH1PiHACAQhAAAIQgAAEuksg9wp8d/F2LnUtKakPWSdOnFimcFuK+nB32rRpYanI9NQY86ONnrQ5k3ZmTSvy8uMt72769OlVp+lYPBwhAAEIQAACEIAABAaPAAr84LHuSEqap75s2TI3f/58p9GFoUOHhg93tWPqsGHDBkxTc+XjOBRAq/RoZZrhw4f31fcBAxYWDxCAAAQgAAEIQCAHBFDgcyBEigABCEAAAhCAAAQgUBwCKPDFkTUlhQAEIAABCEAAAhDIAQEU+BwIkSJAAAIQgAAEIAABCBSHAAp8cWRNSSEAAQhAAAIQgAAEckAABT4HQqQIEIAABCAAAQhAAALFIYACXxxZU1IIQAACEIAABCAAgRwQQIHPgRApAgQgAAEIQAACEIBAcQigwBdH1pQUAhCAAAQgAAEIQCAHBFDgcyBEigABCEAAAhCAAAQgUBwCKPDFkTUlhQAEIAABCEAAAhDIAQEU+BwIkSJAAAIQgAAEIAABCBSHAAp8cWRNSSEAAQhAAAIQgAAEckAABT4HQqQIEIAABCAAAQhAAALFIYACXxxZU1IIQAACEIAABCAAgRwQQIHPgRApAgQgAAEIQAACEIBAcQigwBdH1pQUAhCAAAQgAAEIQCAHBFDgcyBEigABCEAAAhCAAAQgUBwCKPDFkTUlhQAEIAABCEAAAhDIAQEU+BwIkSJAAAIQgAAEIAABCBSHAAp8cWRNSSEAAQhAAAIQgAAEckAABT4HQqQIEIAABCAAAQhAAALFIYACXxxZU1IIQAACEIAABCAAgRwQQIHPgRApAgQgAAEIQAACEIBAcQigwBdH1pQUAhCAAAQgAAEIQCAHBFDgcyBEigABCEAAAhCAAAQgUBwCKPDFkTUlhQAEIAABCEAAAhDIAQEU+BwIkSJAAAIQgAAEIAABCBSHAAp8cWRNSSEAAQhAAAIQgAAEckAABT4HQqQIEIAABCAAAQhAAALFIYACXxxZU1IIQAACEIAABCAAgRwQQIHPgRApAgQgAAEIQAACEIBAcQigwBdH1pQUAhCAAAQgAAEIQCAHBFDgcyBEigABCEAAAhCAAAQgUBwCKPDFkTUlhQAEIAABCEAAAhDIAQEU+BwIkSJAAAIQgAAEIAABCBSHAAp8cWRNSSEAAQhAAAIQgAAEckAABT4HQqQIEIAABCAAAQhAAALFIYACXxxZU1IIQAACEIAABCAAgRwQQIHPgRApAgQgAAEIQAACEIBAcQigwBdH1pQUAhCAAAQgAAEIQCAHBP4PJpTaekbevS4AAAAASUVORK5CYII=)"
      ],
      "metadata": {
        "id": "CPYqzZDtaqQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "\n",
        "def normal_pdf(x, mean, std):\n",
        "    \"\"\"\n",
        "    Calculates the probability density function (PDF) of a normal distribution.\n",
        "\n",
        "    Args:\n",
        "        x (torch.Tensor): The input tensor for which the PDF is calculated.\n",
        "        mean (float): The mean of the normal distribution.\n",
        "        std (float): The standard deviation of the normal distribution.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The PDF values for the input tensor.\n",
        "    \"\"\"\n",
        "    return (1. / (std * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mean) / std) ** 2)\n",
        "\n",
        "\n",
        "\n",
        "def simulate_integral(u, sigma, num_samples=100000):\n",
        "  \"\"\"\n",
        "  Simulates the integral of x * f(x) dx, where f(x) is a normal distribution.\n",
        "\n",
        "  Args:\n",
        "    u: The mean of the normal distribution.\n",
        "    sigma: The standard deviation of the normal distribution.\n",
        "    num_samples: The number of samples to use for the Monte Carlo simulation.\n",
        "\n",
        "  Returns:\n",
        "    The estimated value of the integral.\n",
        "  \"\"\"\n",
        "  # Generate random samples from the normal distribution.\n",
        "  x = torch.normal(mean=u, std=sigma, size=(num_samples,))\n",
        "\n",
        "  # Calculate the integrand for each sample\n",
        "  integrand = x\n",
        "\n",
        "  # Estimate the integral using Monte Carlo method.\n",
        "  integral_estimate = torch.mean(integrand)\n",
        "\n",
        "  return integral_estimate.item()\n",
        "\n",
        "# Example usage:\n",
        "u = 2.0\n",
        "sigma = 1.5\n",
        "integral_value = simulate_integral(u, sigma)\n",
        "\n",
        "print(f\"The simulated value of the integral is: {integral_value}\")\n",
        "\n",
        "sum = []\n",
        "dx = 0.001\n",
        "bins = np.arange(-1000, 1000, dx)\n",
        "for _, x in enumerate(bins):\n",
        "  sum.append(x*normal_pdf(x, 2.0, 1.5)*dx)\n",
        "\n",
        "\n",
        "print(np.sum(np.array(sum)))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-UEnjOIrrHod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Eigenvalue decomposition\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "IsSsQnWjhtTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "\n",
        "matrix = [[1,2],[3,4]]\n",
        "\n",
        "# Example matrix (replace with your actual matrix)\n",
        "matrix = torch.tensor(matrix, dtype=torch.float32)\n",
        "\n",
        "# Perform eigenvalue decomposition\n",
        "Lambda, V = torch.linalg.eig(matrix)\n",
        "\n",
        "V_inv = torch.linalg.inv(V)\n",
        "\n",
        "V_mm_V_inv = torch.mm(V, V_inv)\n",
        "print(V_mm_V_inv)\n",
        "\n",
        "print(\"Eigenvalues:\", Lambda)\n",
        "print(\"Eigenvectors:\", V)"
      ],
      "metadata": {
        "id": "j0Ze6PHfrHfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# write code to implement least-square error\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "cThJBQcSz45L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "gt = torch.randn(10,1)\n",
        "\n",
        "A = torch.randn(100, 10)\n",
        "\n",
        "b = torch.mm(A, gt) + torch.randn(100,1)*0.01\n",
        "\n",
        "estimated_values = torch.linalg.inv(torch.mm(A.T, A)).mm(A.T).mm(b)\n",
        "\n",
        "\n",
        "# Calculate the least-squares error\n",
        "error = gt - estimated_values\n",
        "squared_error = error ** 2\n",
        "least_squares_error = torch.sum(squared_error) / len(gt)\n",
        "\n",
        "\n",
        "print(\"Ground Truth Values:\", gt)\n",
        "print(\"Estimated Values:\", estimated_values)\n",
        "print(\"Least Squares Error:\", least_squares_error)"
      ],
      "metadata": {
        "id": "rgAjrfAjrHc7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}